{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **DATA PERSISTANCE LOADER**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Avro\n",
    "import avro.schema\n",
    "from avro.datafile import DataFileReader, DataFileWriter\n",
    "from avro.io import DatumReader, DatumWriter\n",
    "\n",
    "import subprocess\n",
    "import pandas as pd\n",
    "from io import StringIO\n",
    "from datetime import datetime\n",
    "import json\n",
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set path of project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'D:/Master/BDMA/Courses/Semester_2/Big_Data_Management/Project/dataimporta/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check how many files need to be uploaded into the persistence zone (the count considering all countries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HDFS command to find count of directories and files inside a directory\n",
    "count_files = subprocess.run('hadoop fs -count /temporal', capture_output=True, shell=True).stdout.decode()\n",
    "# From the answer, get the count of files (note that this is the count for ALL countries)\n",
    "count_files = re.findall('\\d*\\s*(\\d*)\\s*\\d*\\s/', count_files)\n",
    "count_files = count_files[0]\n",
    "print('# of files to upload: '+count_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Countries to upload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Countries to get data from\n",
    "countries = [\"peru\", \"chile\", \"brazil\"]\n",
    "# Countries that have different metadata for imports and exports\n",
    "countries_meta = [\"chile\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check which directories have data in the temporal landing zone and get those file names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary to store the file names\n",
    "filenames = {}\n",
    "for country in countries:\n",
    "    # For countries that have different metadata for imports and exports\n",
    "    if country in countries_meta:\n",
    "        filenames[country] = {\"imports\":[], \"exports\":[], \"metadata\":{\"imports\":[], \"exports\":[]}}\n",
    "    # For countries that have the same metadata for imports and exports\n",
    "    else:\n",
    "        filenames[country] = {\"imports\":[], \"exports\":[], \"metadata\":[]}\n",
    "\n",
    "# Iterate trough each country directory\n",
    "for country in countries:\n",
    "\n",
    "    # Get the content of the directories of the country\n",
    "    # imports\n",
    "    imports = subprocess.run('hadoop fs -ls /temporal/'+country+'/imp', capture_output=True, shell=True).stdout.decode()\n",
    "    # exports\n",
    "    exports= subprocess.run('hadoop fs -ls /temporal/'+country+'/exp', capture_output=True, shell=True).stdout.decode()\n",
    "    # metadata\n",
    "    if country in countries_meta:\n",
    "        metadata_imports = subprocess.run('hadoop fs -ls /temporal/'+country+'/metadata/imp', capture_output=True, shell=True).stdout.decode()\n",
    "        metadata_exports = subprocess.run('hadoop fs -ls /temporal/'+country+'/metadata/exp', capture_output=True, shell=True).stdout.decode()\n",
    "    else:\n",
    "        metadata = subprocess.run('hadoop fs -ls /temporal/'+country+'/metadata', capture_output=True, shell=True).stdout.decode()\n",
    "    \n",
    "    # Get the names of the files (if any) existing in those directories\n",
    "     # imports\n",
    "    imports_files = re.findall('/.*/(.*)\\r', imports)\n",
    "     # exports\n",
    "    exports_files = re.findall('/.*/(.*)\\r', exports)\n",
    "    # metadata\n",
    "    if country in countries_meta:\n",
    "        metadata_import_files =re.findall('/.*/(.*)\\r', metadata_imports)\n",
    "        metadata_export_files =re.findall('/.*/(.*)\\r', metadata_exports)\n",
    "    else:\n",
    "        metadata_files =re.findall('/.*/(.*)\\r', metadata)\n",
    "\n",
    "\n",
    "    # Save those names in the variable \"file\"\n",
    "    #   For imports\n",
    "    for import_file in imports_files:\n",
    "        filenames[country]['imports'].append(import_file)\n",
    "    #   For exports\n",
    "    for export_file in exports_files:\n",
    "        filenames[country]['exports'].append(export_file)\n",
    "    #   For metadata\n",
    "    if country in countries_meta:\n",
    "        # imports\n",
    "        for metadata_import_file in metadata_import_files:\n",
    "            filenames[country]['metadata']['imports'].append(metadata_import_file)\n",
    "        # exports\n",
    "        for metadata_export_file in metadata_export_files:\n",
    "            filenames[country]['metadata']['exports'].append(metadata_export_file)\n",
    "    else:\n",
    "        for metadata_file in metadata_files:\n",
    "            filenames[country]['metadata'].append(metadata_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the files to be uploaded, we need to know to which year do they correspond"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary with the file names and the corresponding years\n",
    "filenames_year = {}\n",
    "for country in countries:\n",
    "    # For countries that have different metadata for imports and exports\n",
    "    if country in countries_meta:\n",
    "        filenames_year[country] = {\"imports\":{}, \"exports\":{}, \"metadata\":{\"imports\":{}, \"exports\":{}}}\n",
    "    # For countries that have the same metadata for imports and exports\n",
    "    else:\n",
    "        filenames_year[country] = {\"imports\":{}, \"exports\":{}, \"metadata\":{}}\n",
    "\n",
    "    # Get years of files for Peru\n",
    "    if country == \"peru\":\n",
    "        # Imports\n",
    "        for filename in filenames[country][\"imports\"]:\n",
    "            filenames_year[country][\"imports\"][filename] = '20'+ re.search('(.{2})(?=.csv)', filename).group(1)\n",
    "        # Exports\n",
    "        for filename in filenames[country][\"exports\"]:\n",
    "            filenames_year[country][\"exports\"][filename] = '20'+ re.search('(.{2})(?=.csv)', filename).group(1)\n",
    "        # Metadata\n",
    "        for filename in filenames[country][\"metadata\"]:\n",
    "            filenames_year[country][\"metadata\"][filename] = datetime.now().date().strftime(\"%Y\")\n",
    "\n",
    "    # Get years of files for Chile\n",
    "    if country == \"chile\":\n",
    "        # Imports\n",
    "        for filename in filenames[country][\"imports\"]:\n",
    "            filenames_year[country][\"imports\"][filename] = re.search('(\\d*)(?=.txt)', filename).group(1)\n",
    "        # Exports\n",
    "        for filename in filenames[country][\"exports\"]:\n",
    "            filenames_year[country][\"exports\"][filename] = re.search('(\\d*)(?=.txt)', filename).group(1)\n",
    "        # Metadata\n",
    "        # imports\n",
    "        for filename in filenames[country][\"metadata\"][\"imports\"]:\n",
    "            filenames_year[country][\"metadata\"][\"imports\"][filename] = datetime.now().date().strftime(\"%Y\")\n",
    "        # exports\n",
    "        for filename in filenames[country][\"metadata\"][\"exports\"]:\n",
    "            filenames_year[country][\"metadata\"][\"exports\"][filename] = datetime.now().date().strftime(\"%Y\")\n",
    "    \n",
    "    # Get years of files for Brazil\n",
    "    if country == \"brazil\":\n",
    "        # Imports\n",
    "        for filename in filenames[country][\"imports\"]:\n",
    "            filenames_year[country][\"imports\"][filename] = re.search('_(\\d{4})_', filename).group(1)\n",
    "        # Exports\n",
    "        for filename in filenames[country][\"exports\"]:\n",
    "            filenames_year[country][\"exports\"][filename] = re.search('_(\\d{4})_', filename).group(1)\n",
    "         # Metadata\n",
    "        for filename in filenames[country][\"metadata\"]:\n",
    "            filenames_year[country][\"metadata\"][filename] = datetime.now().date().strftime(\"%Y\")\n",
    "            \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the data years by country:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "years = {}\n",
    "# Get all the years existing in filenames_year\n",
    "for country in countries:\n",
    "    years[country] = []\n",
    "    # imports\n",
    "    for file, year in filenames_year[country][\"imports\"].items():\n",
    "        years[country].append(year)\n",
    "    # exports\n",
    "    for file, year in filenames_year[country][\"exports\"].items():\n",
    "        years[country].append(year)\n",
    "    # metadata\n",
    "    if country in countries_meta:\n",
    "        # imports\n",
    "        for file, year in filenames_year[country][\"metadata\"][\"imports\"].items():\n",
    "            years[country].append(year)\n",
    "        # exports\n",
    "        for file, year in filenames_year[country][\"metadata\"][\"exports\"].items():\n",
    "            years[country].append(year)\n",
    "    else:\n",
    "        for file, year in filenames_year[country][\"metadata\"].items():\n",
    "            years[country].append(year)\n",
    "\n",
    "# Remove duplicated years from dict\n",
    "for country in years:   \n",
    "    years[country] = list(set(years[country]))\n",
    "years\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if the persistent zone already has the corresponding \"years\" directories to upload the files into, if not, create them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for country in countries:\n",
    "    for year in years[country]:\n",
    "\n",
    "        # imports\n",
    "        years_in_persistent = subprocess.run('hadoop fs -ls /persistent/'+country+'/imp/'+year , capture_output=True, shell=True).stdout.decode()\n",
    "        years_in_persistent = re.findall('/.*/(.*)\\r', years_in_persistent)\n",
    "        # if the year directory does not exist in the persistent folder, create it\n",
    "        if year not in years_in_persistent:\n",
    "            subprocess.run('hadoop fs -mkdir /persistent/'+country+'/imp/'+year , capture_output=True, shell=True)\n",
    "\n",
    "        # exports\n",
    "        years_in_persistent = subprocess.run('hadoop fs -ls /persistent/'+country+'/exp/'+year , capture_output=True, shell=True).stdout.decode()\n",
    "        years_in_persistent = re.findall('/.*/(.*)\\r', years_in_persistent)\n",
    "        # if the year directory does not exist in the persistent folder, create it\n",
    "        if year not in years_in_persistent:\n",
    "            subprocess.run('hadoop fs -mkdir /persistent/'+country+'/exp/'+year , capture_output=True, shell=True)\n",
    "\n",
    "        # metadata\n",
    "        if country in countries_meta:\n",
    "\n",
    "            # imports\n",
    "            years_in_persistent = subprocess.run('hadoop fs -ls /persistent/'+country+'/metadata/imp/'+year , capture_output=True, shell=True).stdout.decode()\n",
    "            years_in_persistent = re.findall('/.*/(.*)\\r', years_in_persistent)\n",
    "            # if the year directory does not exist in the persistent folder, create it\n",
    "            if year not in years_in_persistent:\n",
    "                subprocess.run('hadoop fs -mkdir /persistent/'+country+'/metadata/imp/'+year , capture_output=True, shell=True)\n",
    "\n",
    "            # exports\n",
    "            years_in_persistent = subprocess.run('hadoop fs -ls /persistent/'+country+'/metadata/exp/'+year , capture_output=True, shell=True).stdout.decode()\n",
    "            years_in_persistent = re.findall('/.*/(.*)\\r', years_in_persistent)\n",
    "            # if the year directory does not exist in the persistent folder, create it\n",
    "            if year not in years_in_persistent:\n",
    "                subprocess.run('hadoop fs -mkdir /persistent/'+country+'/metadata/exp/'+year , capture_output=True, shell=True)\n",
    "\n",
    "        else:\n",
    "            \n",
    "            years_in_persistent = subprocess.run('hadoop fs -ls /persistent/'+country+'/metadata/'+year , capture_output=True, shell=True).stdout.decode()\n",
    "            years_in_persistent = re.findall('/.*/(.*)\\r', years_in_persistent)\n",
    "            # if the year directory does not exist in the persistent folder, create it\n",
    "            if year not in years_in_persistent:\n",
    "                subprocess.run('hadoop fs -mkdir /persistent/'+country+'/metadata/'+year , capture_output=True, shell=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's start the upload. Since we will save the files in AVRO format, they must be first converted and only then uploaded to the corresponding directory. Each country will have a different upload pipeline, given the differences of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for country in countries:\n",
    "\n",
    "    # initial file name inside a directory:\n",
    "    initial_name = \"version0.avro\"\n",
    "\n",
    "    if country == 'peru':\n",
    "\n",
    "        # paths to folders:\n",
    "        folders = {\"imports\":\"imp\", \"exports\":\"exp\", \"metadata\":\"metadata\"}\n",
    "        \n",
    "        for folder, folderpath in folders.items():\n",
    "            # imports or exports\n",
    "            if folder == 'imports' or folder == 'exports':\n",
    "\n",
    "                for filename, year in filenames_year['peru'][folder].items():\n",
    "\n",
    "                    # Print status\n",
    "                    print(\"Working on: \"+country+\" | \"+folder+\" | \"+filename+'...')\n",
    "\n",
    "                    # Check if there is any persistent file to append the temporal file's data into (e.g. the most recent version). If there is no persistent file, create one (version0.avro)\n",
    "                    all_versions = subprocess.run('hadoop fs -ls /persistent/'+country+'/'+folderpath+'/'+year, capture_output=True, shell=True).stdout.decode()\n",
    "                    \n",
    "                    # If no persistent file, create one\n",
    "                    if all_versions == '':\n",
    "                        # convert the temporal file to AVRO\n",
    "                        # - first retrieve the data and convert it into a dataframe\n",
    "                        data = subprocess.run('hadoop fs -cat /temporal/'+country+'/'+folderpath+'/'+filename, capture_output=True, shell=True, encoding=\"latin-1\").stdout\n",
    "                        data = pd.read_csv(StringIO(data))\n",
    "                        # - then create the AVRO schema\n",
    "                        # -- get the column names and create the fields argument for the schema\n",
    "                        fields = []\n",
    "                        for col in list(data.columns):\n",
    "                            fields.append({\"name\":col, \"type\": \"string\"})\n",
    "                        # -- complete the schema with the desired information\n",
    "                        schema = {\n",
    "                            \"doc\": datetime.now().strftime(\"%m/%d/%Y, %H:%M:%S\"),\n",
    "                            \"name\": \"trade_item\",\n",
    "                            \"namespace\": country+\"_\"+folder,\n",
    "                            \"type\": \"record\",\n",
    "                            \"fields\": fields\n",
    "                        }\n",
    "                        schema = json.dumps(schema)\n",
    "                        schema = avro.schema.parse(schema)\n",
    "                        # - mutate the records to AVRO format\n",
    "                        records = data.to_dict('records')\n",
    "                        for dicts in records:\n",
    "                            # ensure all the data are strings\n",
    "                            for keys in dicts:\n",
    "                                dicts[keys] = str(dicts[keys])\n",
    "                        # - create the AVRO file with the data in the local system\n",
    "                        writer = DataFileWriter(open(path+initial_name, \"wb\"), DatumWriter(), schema)\n",
    "                        # - append each record into the file\n",
    "                        for record in records:\n",
    "                            writer.append(record)\n",
    "                        writer.close()\n",
    "                    \n",
    "                        # after creating the AVRO file, upload it into HDFS\n",
    "                        load_hdfs = subprocess.run(['hadoop', 'fs', '-put', path+initial_name, '/persistent/'+country+'/'+folderpath+'/'+year], capture_output=True, shell=True)\n",
    "                        # print status\n",
    "                        if load_hdfs.returncode == 0 :\n",
    "                            print('First version created: version0.avro file was created in /persistent/'+country+'/'+folderpath+'/'+year+'/')\n",
    "                        else: \n",
    "                            print('An error occured, upload not performed')\n",
    "                        # delete the file in local\n",
    "                        os.remove(path+initial_name)\n",
    "\n",
    "                    # if there is a persistent file (e.g. a previous version)\n",
    "                    if all_versions != '':\n",
    "                        # get a list with all the versions\n",
    "                        all_versions = re.findall('/.*/(.*)\\r', all_versions)\n",
    "                        # get the most recent version\n",
    "                        most_recent_version = initial_name\n",
    "                        for version in all_versions:\n",
    "                            if re.search('(\\d*)(?=.avro)', version).group(1) > re.search('(\\d*)(?=.avro)', most_recent_version).group(1):\n",
    "                                most_recent_version = version\n",
    "                        # retrieve that version's file from hadoop\n",
    "                        old_avro = subprocess.run(['hadoop', 'fs', '-get', '/persistent/'+country+'/'+folderpath+'/'+year+'/'+most_recent_version, path], capture_output=True, shell=True)\n",
    "                        # get old records (this step can be omited if we only append the new records into the file directly)\n",
    "                        avro_records = []\n",
    "                        reader = DataFileReader(open(path+most_recent_version, \"rb\"), DatumReader())\n",
    "                        for record in reader:\n",
    "                            avro_records.append(record)\n",
    "                        # get old schema\n",
    "                        schema = json.loads(reader.schema)\n",
    "                        reader.close()\n",
    "                        # modify schema\n",
    "                        schema[\"doc\"] = datetime.now().strftime(\"%m/%d/%Y, %H:%M:%S\")\n",
    "                        schema = json.dumps(schema)\n",
    "                        schema = avro.schema.parse(schema)\n",
    "                        # delete the file in local\n",
    "                        os.remove(path+most_recent_version)\n",
    "                        # convert the temporal file to AVRO\n",
    "                        # - first retrieve the data and convert it into a dataframe\n",
    "                        data = subprocess.run('hadoop fs -cat /temporal/'+country+'/'+folderpath+'/'+filename, capture_output=True, shell=True, encoding=\"latin-1\").stdout\n",
    "                        data = pd.read_csv(StringIO(data))\n",
    "                        # - mutate the records to AVRO format\n",
    "                        new_records = data.to_dict('records')\n",
    "                        for dicts in new_records:\n",
    "                            # ensure all the data are strings\n",
    "                            for keys in dicts:\n",
    "                                dicts[keys] = str(dicts[keys])\n",
    "                        # append the new records into the old ones\n",
    "                        for record in new_records:\n",
    "                            avro_records.append(record)\n",
    "                        # define name for the AVRO file to upload\n",
    "                        final_name = \"version\"+str(int(re.search('(\\d*)(?=.avro)', most_recent_version).group(1))+1)+'.avro'\n",
    "                        # create the AVRO file with the data in the local system\n",
    "                        writer = DataFileWriter(open(path+final_name, \"wb\"), DatumWriter(), schema)\n",
    "                        # append each record into the file\n",
    "                        for record in avro_records:\n",
    "                            writer.append(record)\n",
    "                        writer.close()\n",
    "                        # after creating the AVRO file, upload it into HDFS\n",
    "                        load_hdfs = subprocess.run(['hadoop', 'fs', '-put', path+final_name, '/persistent/'+country+'/'+folderpath+'/'+year], capture_output=True, shell=True)\n",
    "                        # print status\n",
    "                        if load_hdfs.returncode == 0 :\n",
    "                            print('Update performed: '+final_name+' file was created in /persistent/'+country+'/'+folderpath+'/'+year+'/')\n",
    "                            # HERE TE CODE TO DELETE THE FILE IN THE TEMPORAL LANDING ZONE AFTER IT IS PASSED TO THE PERSISTENT\n",
    "                        else: \n",
    "                            print('An error occured, update not performed')\n",
    "                        # delete the file in local\n",
    "                        os.remove(path+final_name)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "fd040cd6d2f6c03fa7abba4128c8b33c22dfb22c58d45ceefc9f5658edabb3e8"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('bdm_project')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
