{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **DATA PERSISTANCE LOADER**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Avro\n",
    "import avro.schema\n",
    "from avro.datafile import DataFileReader, DataFileWriter\n",
    "from avro.io import DatumReader, DatumWriter\n",
    "\n",
    "import subprocess\n",
    "import pandas as pd\n",
    "from io import StringIO\n",
    "from datetime import datetime\n",
    "import json\n",
    "import re\n",
    "import os\n",
    "import logging\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set path of project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/home/bdm/DataImporta/P1/production/VM/'\n",
    "hadoop = '~/BDM_Software/hadoop/bin/hadoop '"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up log file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger()\n",
    "if (logger.hasHandlers()):\n",
    "    logger.handlers.clear()\n",
    "handler = logging.FileHandler('logfile.log')\n",
    "logger.addHandler(handler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check how many files need to be uploaded into the persistence zone (the count considering all countries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of files to upload: 66\n"
     ]
    }
   ],
   "source": [
    "# HDFS command to find count of directories and files inside a directory\n",
    "count_files = subprocess.run(hadoop + 'fs -count /user/bdm/temporal', capture_output=True, shell=True).stdout.decode()\n",
    "# From the answer, get the count of files (note that this is the count for ALL countries)\n",
    "count_files = re.findall('\\d*\\s*(\\d*)\\s*\\d*\\s/', count_files)\n",
    "count_files = count_files[0]\n",
    "print('# of files to upload: '+count_files)\n",
    "logger.error(datetime.now().strftime(\"%m/%d/%Y, %H:%M:%S\"))\n",
    "logger.error('# of files to upload: '+count_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Countries to upload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Countries to get data from\n",
    "countries = [\"peru\", \"chile\", \"brazil\"]\n",
    "# Countries that have different metadata for imports and exports\n",
    "countries_meta = [\"chile\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check which directories have data in the temporal landing zone and get those file names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove .DS_Store files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/bdm/DataImporta/P1/production/VM/data_persistance_loader_VM.ipynb Cell 14'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bpietro.ferrazzi/home/bdm/DataImporta/P1/production/VM/data_persistance_loader_VM.ipynb#ch0000014vscode-remote?line=0'>1</a>\u001b[0m \u001b[39mfor\u001b[39;00m country \u001b[39min\u001b[39;00m countries:\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bpietro.ferrazzi/home/bdm/DataImporta/P1/production/VM/data_persistance_loader_VM.ipynb#ch0000014vscode-remote?line=1'>2</a>\u001b[0m     \u001b[39mfor\u001b[39;00m \u001b[39mdir\u001b[39m \u001b[39min\u001b[39;00m [\u001b[39m'\u001b[39m\u001b[39mimp\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mexp\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mmetadata\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mmetadata/imp\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mmetadata/exp\u001b[39m\u001b[39m'\u001b[39m]:\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bpietro.ferrazzi/home/bdm/DataImporta/P1/production/VM/data_persistance_loader_VM.ipynb#ch0000014vscode-remote?line=2'>3</a>\u001b[0m         s \u001b[39m=\u001b[39m subprocess\u001b[39m.\u001b[39;49mrun(hadoop \u001b[39m+\u001b[39;49m \u001b[39m'\u001b[39;49m\u001b[39mfs -rm /user/bdm/temporal/\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m+\u001b[39;49mcountry\u001b[39m+\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m/\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m+\u001b[39;49m\u001b[39mdir\u001b[39;49m\u001b[39m+\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m/.DS_Store\u001b[39;49m\u001b[39m'\u001b[39;49m, capture_output\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, shell\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "File \u001b[0;32m~/miniconda3/envs/bdm/lib/python3.9/subprocess.py:507\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(input, capture_output, timeout, check, *popenargs, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///home/bdm/miniconda3/envs/bdm/lib/python3.9/subprocess.py?line=504'>505</a>\u001b[0m \u001b[39mwith\u001b[39;00m Popen(\u001b[39m*\u001b[39mpopenargs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs) \u001b[39mas\u001b[39;00m process:\n\u001b[1;32m    <a href='file:///home/bdm/miniconda3/envs/bdm/lib/python3.9/subprocess.py?line=505'>506</a>\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> <a href='file:///home/bdm/miniconda3/envs/bdm/lib/python3.9/subprocess.py?line=506'>507</a>\u001b[0m         stdout, stderr \u001b[39m=\u001b[39m process\u001b[39m.\u001b[39;49mcommunicate(\u001b[39minput\u001b[39;49m, timeout\u001b[39m=\u001b[39;49mtimeout)\n\u001b[1;32m    <a href='file:///home/bdm/miniconda3/envs/bdm/lib/python3.9/subprocess.py?line=507'>508</a>\u001b[0m     \u001b[39mexcept\u001b[39;00m TimeoutExpired \u001b[39mas\u001b[39;00m exc:\n\u001b[1;32m    <a href='file:///home/bdm/miniconda3/envs/bdm/lib/python3.9/subprocess.py?line=508'>509</a>\u001b[0m         process\u001b[39m.\u001b[39mkill()\n",
      "File \u001b[0;32m~/miniconda3/envs/bdm/lib/python3.9/subprocess.py:1134\u001b[0m, in \u001b[0;36mPopen.communicate\u001b[0;34m(self, input, timeout)\u001b[0m\n\u001b[1;32m   <a href='file:///home/bdm/miniconda3/envs/bdm/lib/python3.9/subprocess.py?line=1130'>1131</a>\u001b[0m     endtime \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/bdm/miniconda3/envs/bdm/lib/python3.9/subprocess.py?line=1132'>1133</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> <a href='file:///home/bdm/miniconda3/envs/bdm/lib/python3.9/subprocess.py?line=1133'>1134</a>\u001b[0m     stdout, stderr \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_communicate(\u001b[39minput\u001b[39;49m, endtime, timeout)\n\u001b[1;32m   <a href='file:///home/bdm/miniconda3/envs/bdm/lib/python3.9/subprocess.py?line=1134'>1135</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyboardInterrupt\u001b[39;00m:\n\u001b[1;32m   <a href='file:///home/bdm/miniconda3/envs/bdm/lib/python3.9/subprocess.py?line=1135'>1136</a>\u001b[0m     \u001b[39m# https://bugs.python.org/issue25942\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/bdm/miniconda3/envs/bdm/lib/python3.9/subprocess.py?line=1136'>1137</a>\u001b[0m     \u001b[39m# See the detailed comment in .wait().\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/bdm/miniconda3/envs/bdm/lib/python3.9/subprocess.py?line=1137'>1138</a>\u001b[0m     \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/bdm/lib/python3.9/subprocess.py:1979\u001b[0m, in \u001b[0;36mPopen._communicate\u001b[0;34m(self, input, endtime, orig_timeout)\u001b[0m\n\u001b[1;32m   <a href='file:///home/bdm/miniconda3/envs/bdm/lib/python3.9/subprocess.py?line=1971'>1972</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_timeout(endtime, orig_timeout,\n\u001b[1;32m   <a href='file:///home/bdm/miniconda3/envs/bdm/lib/python3.9/subprocess.py?line=1972'>1973</a>\u001b[0m                         stdout, stderr,\n\u001b[1;32m   <a href='file:///home/bdm/miniconda3/envs/bdm/lib/python3.9/subprocess.py?line=1973'>1974</a>\u001b[0m                         skip_check_and_raise\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m   <a href='file:///home/bdm/miniconda3/envs/bdm/lib/python3.9/subprocess.py?line=1974'>1975</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(  \u001b[39m# Impossible :)\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/bdm/miniconda3/envs/bdm/lib/python3.9/subprocess.py?line=1975'>1976</a>\u001b[0m         \u001b[39m'\u001b[39m\u001b[39m_check_timeout(..., skip_check_and_raise=True) \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m   <a href='file:///home/bdm/miniconda3/envs/bdm/lib/python3.9/subprocess.py?line=1976'>1977</a>\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mfailed to raise TimeoutExpired.\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m-> <a href='file:///home/bdm/miniconda3/envs/bdm/lib/python3.9/subprocess.py?line=1978'>1979</a>\u001b[0m ready \u001b[39m=\u001b[39m selector\u001b[39m.\u001b[39;49mselect(timeout)\n\u001b[1;32m   <a href='file:///home/bdm/miniconda3/envs/bdm/lib/python3.9/subprocess.py?line=1979'>1980</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_timeout(endtime, orig_timeout, stdout, stderr)\n\u001b[1;32m   <a href='file:///home/bdm/miniconda3/envs/bdm/lib/python3.9/subprocess.py?line=1981'>1982</a>\u001b[0m \u001b[39m# XXX Rewrite these to use non-blocking I/O on the file\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/bdm/miniconda3/envs/bdm/lib/python3.9/subprocess.py?line=1982'>1983</a>\u001b[0m \u001b[39m# objects; they are no longer using C stdio!\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/bdm/lib/python3.9/selectors.py:416\u001b[0m, in \u001b[0;36m_PollLikeSelector.select\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    <a href='file:///home/bdm/miniconda3/envs/bdm/lib/python3.9/selectors.py?line=413'>414</a>\u001b[0m ready \u001b[39m=\u001b[39m []\n\u001b[1;32m    <a href='file:///home/bdm/miniconda3/envs/bdm/lib/python3.9/selectors.py?line=414'>415</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> <a href='file:///home/bdm/miniconda3/envs/bdm/lib/python3.9/selectors.py?line=415'>416</a>\u001b[0m     fd_event_list \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_selector\u001b[39m.\u001b[39;49mpoll(timeout)\n\u001b[1;32m    <a href='file:///home/bdm/miniconda3/envs/bdm/lib/python3.9/selectors.py?line=416'>417</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mInterruptedError\u001b[39;00m:\n\u001b[1;32m    <a href='file:///home/bdm/miniconda3/envs/bdm/lib/python3.9/selectors.py?line=417'>418</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m ready\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for country in countries:\n",
    "    for dir in ['imp', 'exp', 'metadata','metadata/imp', 'metadata/exp']:\n",
    "        s = subprocess.run(hadoop + 'fs -rm /user/bdm/temporal/'+country+'/'+dir+'/.DS_Store', capture_output=True, shell=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary to store the file names\n",
    "filenames = {}\n",
    "for country in countries:\n",
    "    # For countries that have different metadata for imports and exports\n",
    "    if country in countries_meta:\n",
    "        filenames[country] = {\"imports\":[], \"exports\":[], \"metadata\":{\"imports\":[], \"exports\":[]}}\n",
    "    # For countries that have the same metadata for imports and exports\n",
    "    else:\n",
    "        filenames[country] = {\"imports\":[], \"exports\":[], \"metadata\":[]}\n",
    "\n",
    "# Iterate trough each country directory\n",
    "for country in countries:\n",
    "\n",
    "    # Get the content of the directories of the country\n",
    "    # imports\n",
    "    imports = subprocess.run(hadoop + 'fs -ls  /user/bdm/temporal/'+country+'/imp', capture_output=True, shell=True).stdout.decode()\n",
    "    # exports\n",
    "    exports= subprocess.run(hadoop + 'fs -ls  /user/bdm/temporal/'+country+'/exp', capture_output=True, shell=True).stdout.decode()\n",
    "    # metadata\n",
    "    if country in countries_meta:\n",
    "        metadata_imports = subprocess.run(hadoop + ' fs -ls  /user/bdm/temporal/'+country+'/metadata/imp', capture_output=True, shell=True).stdout.decode()\n",
    "        metadata_exports = subprocess.run(hadoop + ' fs -ls  /user/bdm/temporal/'+country+'/metadata/exp', capture_output=True, shell=True).stdout.decode()\n",
    "    else:\n",
    "        metadata = subprocess.run(hadoop + 'fs -ls  /user/bdm/temporal/'+country+'/metadata', capture_output=True, shell=True).stdout.decode()\n",
    "    \n",
    "    # Get the names of the files (if any) existing in those directories\n",
    "     # imports\n",
    "    #imports_files = re.findall('/.*/.*/.*/.*/.*/(.*)', imports)\n",
    "    imports_files = re.findall('/user/bdm/temporal/'+country+'/imp/(.*)', imports)\n",
    "\n",
    "     # exports\n",
    "    exports_files = re.findall('/user/bdm/temporal/'+country+'/exp/(.*)', exports)\n",
    "\n",
    "    # metadata\n",
    "    if country in countries_meta:\n",
    "        metadata_import_files =re.findall('/user/bdm/temporal/'+country+'/metadata/imp/(.*)', metadata_imports)\n",
    "        metadata_export_files =re.findall('/user/bdm/temporal/'+country+'/metadata/exp/(.*)', metadata_exports)\n",
    "    else:\n",
    "        metadata_files =re.findall('/user/bdm/temporal/'+country+'/metadata/(.*)', metadata)\n",
    "\n",
    "\n",
    "    # Save those names in the variable \"file\"\n",
    "    #   For imports\n",
    "    for import_file in imports_files:\n",
    "        filenames[country]['imports'].append(import_file)\n",
    "    #   For exports\n",
    "    for export_file in exports_files:\n",
    "        filenames[country]['exports'].append(export_file)\n",
    "    #   For metadata\n",
    "    if country in countries_meta:\n",
    "        # imports\n",
    "        for metadata_import_file in metadata_import_files:\n",
    "            filenames[country]['metadata']['imports'].append(metadata_import_file)\n",
    "        # exports\n",
    "        for metadata_export_file in metadata_export_files:\n",
    "            filenames[country]['metadata']['exports'].append(metadata_export_file)\n",
    "    else:\n",
    "        for metadata_file in metadata_files:\n",
    "            filenames[country]['metadata'].append(metadata_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the files to be uploaded, we need to know to which year do they correspond"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename=filenames['peru'][\"imports\"][2]\n",
    "filenames_year = {}\n",
    "for country in countries:\n",
    "    # For countries that have different metadata for imports and exports\n",
    "    if country in countries_meta:\n",
    "        filenames_year[country] = {\"imports\":{}, \"exports\":{}, \"metadata\":{\"imports\":{}, \"exports\":{}}}\n",
    "    # For countries that have the same metadata for imports and exports\n",
    "    else:\n",
    "        filenames_year[country] = {\"imports\":{}, \"exports\":{}, \"metadata\":{}}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary with the file names and the corresponding years\n",
    "filenames_year = {}\n",
    "for country in countries:\n",
    "    # For countries that have different metadata for imports and exports\n",
    "    if country in countries_meta:\n",
    "        filenames_year[country] = {\"imports\":{}, \"exports\":{}, \"metadata\":{\"imports\":{}, \"exports\":{}}}\n",
    "    # For countries that have the same metadata for imports and exports\n",
    "    else:\n",
    "        filenames_year[country] = {\"imports\":{}, \"exports\":{}, \"metadata\":{}}\n",
    "\n",
    "    # Get years of files for Peru\n",
    "    if country == \"peru\":\n",
    "        # Imports\n",
    "        for filename in filenames[country][\"imports\"]:\n",
    "            filenames_year[country][\"imports\"][filename] = '20'+ re.search('(.{2})(?=.csv)', filename).group(1)\n",
    "        # Exports\n",
    "        for filename in filenames[country][\"exports\"]:\n",
    "            filenames_year[country][\"exports\"][filename] = '20'+ re.search('(.{2})(?=.csv)', filename).group(1)\n",
    "        # Metadata\n",
    "        for filename in filenames[country][\"metadata\"]:\n",
    "            filenames_year[country][\"metadata\"][filename] = datetime.now().date().strftime(\"%Y\")\n",
    "\n",
    "    # Get years of files for Chile\n",
    "    if country == \"chile\":\n",
    "        # Imports\n",
    "        for filename in filenames[country][\"imports\"]:\n",
    "            filenames_year[country][\"imports\"][filename] = re.search('(\\d*)(?=.txt)', filename).group(1)\n",
    "        # Exports\n",
    "        for filename in filenames[country][\"exports\"]:\n",
    "            filenames_year[country][\"exports\"][filename] = re.search('(\\d*)(?=.txt)', filename).group(1)\n",
    "        # Metadata\n",
    "        # imports\n",
    "        for filename in filenames[country][\"metadata\"][\"imports\"]:\n",
    "            filenames_year[country][\"metadata\"][\"imports\"][filename] = datetime.now().date().strftime(\"%Y\")\n",
    "        # exports\n",
    "        for filename in filenames[country][\"metadata\"][\"exports\"]:\n",
    "            filenames_year[country][\"metadata\"][\"exports\"][filename] = datetime.now().date().strftime(\"%Y\")\n",
    "    \n",
    "    # Get years of files for Brazil\n",
    "    if country == \"brazil\":\n",
    "        # Imports\n",
    "        for filename in filenames[country][\"imports\"]:\n",
    "            filenames_year[country][\"imports\"][filename] = re.search('_(\\d{4})_', filename).group(1)\n",
    "        # Exports\n",
    "        for filename in filenames[country][\"exports\"]:\n",
    "            filenames_year[country][\"exports\"][filename] = re.search('_(\\d{4})_', filename).group(1)\n",
    "         # Metadata\n",
    "        for filename in filenames[country][\"metadata\"]:\n",
    "            filenames_year[country][\"metadata\"][filename] = datetime.now().date().strftime(\"%Y\")\n",
    "            \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the data years by country:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'peru': ['2022'], 'chile': ['2022'], 'brazil': ['2022']}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "years = {}\n",
    "# Get all the years existing in filenames_year\n",
    "for country in countries:\n",
    "    years[country] = []\n",
    "    # imports\n",
    "    for file, year in filenames_year[country][\"imports\"].items():\n",
    "        years[country].append(year)\n",
    "    # exports\n",
    "    for file, year in filenames_year[country][\"exports\"].items():\n",
    "        years[country].append(year)\n",
    "    # metadata\n",
    "    if country in countries_meta:\n",
    "        # imports\n",
    "        for file, year in filenames_year[country][\"metadata\"][\"imports\"].items():\n",
    "            years[country].append(year)\n",
    "        # exports\n",
    "        for file, year in filenames_year[country][\"metadata\"][\"exports\"].items():\n",
    "            years[country].append(year)\n",
    "    else:\n",
    "        for file, year in filenames_year[country][\"metadata\"].items():\n",
    "            years[country].append(year)\n",
    "\n",
    "# Remove duplicated years from dict\n",
    "for country in years:   \n",
    "    years[country] = list(set(years[country]))\n",
    "years\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if the persistent zone already has the corresponding \"years\" directories to upload the files into, if not, create them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "for country in countries:\n",
    "    for year in years[country]:\n",
    "\n",
    "        # imports\n",
    "        years_in_persistent = subprocess.run(hadoop + ' fs -ls /user/bdm/persistent/'+country+'/imp/'+year , capture_output=True, shell=True).stdout.decode()\n",
    "        years_in_persistent = re.findall('/.*/(.*)\\r', years_in_persistent)\n",
    "        # if the year directory does not exist in the persistent folder, create it\n",
    "        if year not in years_in_persistent:\n",
    "            subprocess.run(hadoop + ' fs -mkdir /user/bdm/persistent/'+country+'/imp/'+year , capture_output=True, shell=True)\n",
    "\n",
    "        # exports\n",
    "        years_in_persistent = subprocess.run(hadoop + ' fs -ls /user/bdm/persistent/'+country+'/exp/'+year , capture_output=True, shell=True).stdout.decode()\n",
    "        years_in_persistent = re.findall('/.*/(.*)\\r', years_in_persistent)\n",
    "        # if the year directory does not exist in the persistent folder, create it\n",
    "        if year not in years_in_persistent:\n",
    "            subprocess.run(hadoop + ' fs -mkdir /user/bdm/persistent/'+country+'/exp/'+year , capture_output=True, shell=True)\n",
    "\n",
    "        # metadata\n",
    "        if country in countries_meta:\n",
    "\n",
    "            # imports\n",
    "            years_in_persistent = subprocess.run(hadoop + ' fs -ls /user/bdm/persistent/'+country+'/metadata/imp/'+year , capture_output=True, shell=True).stdout.decode()\n",
    "            years_in_persistent = re.findall('/.*/(.*)\\r', years_in_persistent)\n",
    "            # if the year directory does not exist in the persistent folder, create it\n",
    "            if year not in years_in_persistent:\n",
    "                subprocess.run(hadoop + ' fs -mkdir /user/bdm/persistent/'+country+'/metadata/imp/'+year , capture_output=True, shell=True)\n",
    "\n",
    "            # exports\n",
    "            years_in_persistent = subprocess.run(hadoop + ' fs -ls /user/bdm/persistent/'+country+'/metadata/exp/'+year , capture_output=True, shell=True).stdout.decode()\n",
    "            years_in_persistent = re.findall('/.*/(.*)\\r', years_in_persistent)\n",
    "            # if the year directory does not exist in the persistent folder, create it\n",
    "            if year not in years_in_persistent:\n",
    "                subprocess.run(hadoop + ' fs -mkdir /user/bdm/persistent/'+country+'/metadata/exp/'+year , capture_output=True, shell=True)\n",
    "\n",
    "        else:\n",
    "            \n",
    "            years_in_persistent = subprocess.run(hadoop + ' fs -ls /user/bdm/persistent/'+country+'/metadata/'+year , capture_output=True, shell=True).stdout.decode()\n",
    "            years_in_persistent = re.findall('/.*/(.*)\\r', years_in_persistent)\n",
    "            # if the year directory does not exist in the persistent folder, create it\n",
    "            if year not in years_in_persistent:\n",
    "                subprocess.run(hadoop + ' fs -mkdir /user/bdm/persistent/'+country+'/metadata/'+year , capture_output=True, shell=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's start the upload. Since we will save the files in AVRO format, they must be first converted and only then uploaded to the corresponding directory. Each country will have a different upload pipeline, given the differences of the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Peru loading pipeline*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------\n",
    "# PERU (from temporal to persitent in AVRO)\n",
    "# ------------------------------------------\n",
    "def load_peru(filenames_year, path):\n",
    "    \n",
    "    country = \"peru\"\n",
    "\n",
    "    # initial file name inside a directory of metadata or imports in the persistent zone\n",
    "    initial_name = \"version0.avro\"\n",
    "\n",
    "    # paths to folders\n",
    "    folders = {\"imports\":\"imp\", \"exports\":\"exp\", \"metadata\":\"metadata\"}\n",
    "    \n",
    "    # Lists of times\n",
    "    times_imp_exp =[]\n",
    "    times_metadata =[]\n",
    "\n",
    "    # iterate trough every folder\n",
    "    for folder, folderpath in folders.items():\n",
    "\n",
    "            # ---------------------\n",
    "            # IMPORTS & EXPORTS\n",
    "            # ---------------------\n",
    "            if folder == 'imports' or folder == 'exports':\n",
    "\n",
    "                # Iterate trough every import/export file\n",
    "                for filename, year in filenames_year[country][folder].items():\n",
    "                    \n",
    "                    # Starting time of the conversion and uploading of this file\n",
    "                    start_time = time.time()\n",
    "\n",
    "                    # Print status\n",
    "                    print(\"Working on: \"+country+\" | \"+folder+\" | \"+filename+'...')\n",
    "                    logger.error(datetime.now().strftime(\"%m/%d/%Y, %H:%M:%S\"))\n",
    "                    logger.error(\"Working on: \"+country+\" | \"+folder+\" | \"+filename+'...')\n",
    "\n",
    "                    # Check if there is any persistent file to append the temporal file's data into (e.g. the most recent version). If there is no persistent file, create one (version0.avro)\n",
    "                    all_versions = subprocess.run(hadoop + ' fs -ls /user/bdm/persistent/'+country+'/'+folderpath+'/'+year, capture_output=True, shell=True).stdout.decode()\n",
    "\n",
    "                    # If no persistent file, create one\n",
    "                    if all_versions == '':\n",
    "                        # convert the temporal file to AVRO\n",
    "                        # - first retrieve the data and convert it into a dataframe\n",
    "                        data = subprocess.run(hadoop + ' fs -cat  /user/bdm/temporal/'+country+'/'+folderpath+'/'+filename, capture_output=True, shell=True, encoding=\"latin-1\").stdout\n",
    "                        data = pd.read_csv(StringIO(data))\n",
    "                        # - then create the AVRO schema\n",
    "                        # -- get the column names and create the fields argument for the schema\n",
    "                        fields = []\n",
    "                        for col in list(data.columns):\n",
    "                            fields.append({\"name\":col, \"type\": \"string\"})\n",
    "                        # -- complete the schema with the desired information\n",
    "                        schema = {\n",
    "                            \"doc\": datetime.now().strftime(\"%H%M%S%m%d%Y\"),\n",
    "                            \"name\": \"trade_item\",\n",
    "                            \"namespace\": country+\"_\"+folder,\n",
    "                            \"type\": \"record\",\n",
    "                            \"fields\": fields\n",
    "                        }\n",
    "                        schema = json.dumps(schema)\n",
    "                        schema = avro.schema.parse(schema)\n",
    "                        # - mutate the records to AVRO format\n",
    "                        records = data.to_dict('records')\n",
    "                        for dicts in records:\n",
    "                            # ensure all the data are strings\n",
    "                            for keys in dicts:\n",
    "                                dicts[keys] = str(dicts[keys])\n",
    "                        # - create the AVRO file with the data in the local system\n",
    "                        writer = DataFileWriter(open(path+initial_name, \"wb\"), DatumWriter(), schema)\n",
    "                        # - append each record into the file\n",
    "                        for record in records:\n",
    "                            writer.append(record)\n",
    "                        writer.close()\n",
    "                    \n",
    "                        # after creating the AVRO file, upload it into HDFS\n",
    "                        load_hdfs = subprocess.run(hadoop + 'fs -put '+ path + initial_name + ' /user/bdm/persistent/'+country+'/'+folderpath+'/'+year, capture_output=True, shell=True)\n",
    "                        # print status\n",
    "                        if load_hdfs.returncode == 0 :\n",
    "                            print('First version created: version0.avro file was created in /user/bdm/persistent/'+country+'/'+folderpath+'/'+year+'/')\n",
    "                            logger.error(datetime.now().strftime(\"%m/%d/%Y, %H:%M:%S\"))\n",
    "                            logger.error('First version created: version0.avro file was created in /user/bdm/persistent/'+country+'/'+folderpath+'/'+year+'/')\n",
    "                            \n",
    "                        else: \n",
    "                            print(\"An error occured, the file could not be uploaded\")\n",
    "                            print(load_hdfs)\n",
    "                            logger.error(datetime.now().strftime(\"%m/%d/%Y, %H:%M:%S\"))\n",
    "                            logger.error(\"An error occured, the file could not be uploaded\")\n",
    "                            logger.error(load_hdfs)\n",
    "                        # delete the file in local\n",
    "                        os.remove(path+initial_name)\n",
    "\n",
    "                        # add the time used for this step\n",
    "                        times_imp_exp.append(['First occurance', time.time() - start_time]) \n",
    "\n",
    "\n",
    "                    # if there is a persistent file (e.g. a previous version)\n",
    "                    if all_versions != '':\n",
    "                        # get a list with all the versions\n",
    "                        all_versions = re.findall('/.*/(.*)\\r', all_versions)\n",
    "                        # get the most recent version\n",
    "                        most_recent_version = initial_name\n",
    "                        for version in all_versions:\n",
    "                            if re.search('(\\d*)(?=.avro)', version).group(1) > re.search('(\\d*)(?=.avro)', most_recent_version).group(1):\n",
    "                                most_recent_version = version\n",
    "                        # retrieve that version's file from hadoop\n",
    "                        old_avro = subprocess.run([hadoop + '', 'fs', '-get',  '/user/bdm/persistent/'+country+'/'+folderpath+'/'+year+'/'+most_recent_version, path], capture_output=True, shell=True)\n",
    "                        # get old records (this step can be omited if we only append the new records into the file directly)\n",
    "                        avro_records = []\n",
    "                        reader = DataFileReader(open(path+most_recent_version, \"rb\"), DatumReader())\n",
    "                        for record in reader:\n",
    "                            avro_records.append(record)\n",
    "                        # get old schema\n",
    "                        schema = json.loads(reader.schema)\n",
    "                        reader.close()\n",
    "                        # modify schema\n",
    "                        schema[\"doc\"] = datetime.now().strftime(\"%H%M%S%m%d%Y\")\n",
    "                        schema = json.dumps(schema)\n",
    "                        schema = avro.schema.parse(schema)\n",
    "                        # delete the file in local\n",
    "                        os.remove(path+most_recent_version)\n",
    "                        # convert the temporal file to AVRO\n",
    "                        # - first retrieve the data and convert it into a dataframe\n",
    "                        data = subprocess.run(hadoop + ' fs -cat  /user/bdm/temporal/'+country+'/'+folderpath+'/'+filename, capture_output=True, shell=True, encoding=\"latin-1\").stdout\n",
    "                        data = pd.read_csv(StringIO(data))\n",
    "                        # - mutate the records to AVRO format\n",
    "                        new_records = data.to_dict('records')\n",
    "                        for dicts in new_records:\n",
    "                            # ensure all the data are strings\n",
    "                            for keys in dicts:\n",
    "                                dicts[keys] = str(dicts[keys])\n",
    "                        # append the new records into the old ones\n",
    "                        for record in new_records:\n",
    "                            avro_records.append(record)\n",
    "                        # define name for the AVRO file to upload\n",
    "                        final_name = \"version\"+str(int(re.search('(\\d*)(?=.avro)', most_recent_version).group(1))+1)+'.avro'\n",
    "                        # create the AVRO file with the data in the local system\n",
    "                        writer = DataFileWriter(open(path+final_name, \"wb\"), DatumWriter(), schema)\n",
    "                        # append each record into the file\n",
    "                        for record in avro_records:\n",
    "                            writer.append(record)\n",
    "                        writer.close()\n",
    "                        # after creating the AVRO file, upload it into HDFS\n",
    "                        load_hdfs = subprocess.run([hadoop + '', 'fs', '-put', path+final_name,  '/user/bdm/persistent/'+country+'/'+folderpath+'/'+year], capture_output=True, shell=True)\n",
    "                        # print status\n",
    "                        if load_hdfs.returncode == 0 :\n",
    "                            print('Update performed: '+final_name+' file was created in /user/bdm/persistent/'+country+'/'+folderpath+'/'+year+'/')\n",
    "                            logger.error(datetime.now().strftime(\"%m/%d/%Y, %H:%M:%S\"))\n",
    "                            logger.error('Update performed: '+final_name+' file was created in /user/bdm/persistent/'+country+'/'+folderpath+'/'+year+'/')\n",
    "                            # HERE TE CODE TO DELETE THE FILE IN THE TEMPORAL LANDING ZONE AFTER IT IS PASSED TO THE PERSISTENT\n",
    "                        else: \n",
    "                            print('An error occured, update not performed')\n",
    "                            print(load_hdfs)\n",
    "                            logger.error(datetime.now().strftime(\"%m/%d/%Y, %H:%M:%S\"))\n",
    "                            logger.error('An error occured, update not performed')\n",
    "                            logger.error(load_hdfs)\n",
    "                        # delete the file in local\n",
    "                        os.remove(path+final_name)\n",
    "\n",
    "                        # add the time used for this step\n",
    "                        times_imp_exp.append([final_name, time.time() - start_time]) \n",
    "\n",
    "            \n",
    "            # ---------------------\n",
    "            # METADATA\n",
    "            # ---------------------\n",
    "            elif folder == \"metadata\":\n",
    "                \n",
    "                # Starting time of the conversion and uploading of this file\n",
    "                start_time = time.time()\n",
    "\n",
    "                # If there is medatata to be uploaded, create a directory in the metadata directory in the temporal zone to store the files. The name is %H%M%S%m%d%Y (time)\n",
    "                if filenames_year[country][folder].items():\n",
    "                    metadata_directory = datetime.now().strftime(\"%H%M%S%m%d%Y\")\n",
    "                    # get the year of the any of the files (all have the same year, we will take the first one)\n",
    "                    year = list(filenames_year[country][folder].items())[0][1]\n",
    "                    # create the folder\n",
    "                    create_directory = subprocess.run(hadoop + ' fs -mkdir /user/bdm/persistent/'+country+'/'+folderpath+'/'+year+'/'+metadata_directory, capture_output=True, shell=True, encoding=\"latin-1\")\n",
    "                    if create_directory.returncode == 0 :\n",
    "                        # print success status\n",
    "                        print('A directory named '+metadata_directory+' has been created in /user/bdm/persistent/'+country+'/'+folderpath+'/'+year+'/')\n",
    "                        logger.error(datetime.now().strftime(\"%m/%d/%Y, %H:%M:%S\"))\n",
    "                        logger.error('A directory named '+metadata_directory+' has been created in /user/bdm/persistent/'+country+'/'+folderpath+'/'+year+'/')\n",
    "                        \n",
    "                        # Iterate trough every metadata file\n",
    "                        for filename, year in filenames_year[country][folder].items():\n",
    "                            # print working on\n",
    "                            print(\"Working on: \"+country+\" | \"+folder+\" | \"+filename+'...')\n",
    "                            logger.error(datetime.now().strftime(\"%m/%d/%Y, %H:%M:%S\"))\n",
    "                            logger.error(\"Working on: \"+country+\" | \"+folder+\" | \"+filename+'...')\n",
    "                            # CONVERT FILE TO AVRO\n",
    "                            # get temporal file (for Peru, the files are in txt)\n",
    "                            temporal_file = subprocess.run(hadoop + ' fs -cat  /user/bdm/temporal/'+country+'/'+folderpath+'/'+filename, capture_output=True, shell=True, encoding=\"latin-1\").stdout\n",
    "                            # convert it to dataframe (the headers use another separator, and thus, we canÂ´t just directly import them, that's why we skip them)\n",
    "                            data = pd.read_table(StringIO(temporal_file), skiprows = 1, header=None)\n",
    "                            # here we process the headers in a special way\n",
    "                            headers = pd.read_table(StringIO(temporal_file), nrows=1, header=None)\n",
    "                            headers['concatenated'] = headers.apply(lambda x: ' '.join(x.dropna().values.tolist()), axis=1)\n",
    "                            headers = headers['concatenated'].to_frame()\n",
    "                            headers = headers['concatenated'].values[0].split()\n",
    "                            headers_dict = {}\n",
    "                            for i in range(len(headers)):\n",
    "                                headers_dict[i] = headers[i]\n",
    "                            # finally we add the headers to the dataframe\n",
    "                            data.rename(columns=headers_dict, inplace=True)\n",
    "                            # then create the AVRO schema\n",
    "                            # - get the column names and create the fields argument for the schema\n",
    "                            fields = []\n",
    "                            for col in list(data.columns):\n",
    "                                fields.append({\"name\":col, \"type\": \"string\"})\n",
    "                            # - complete the schema with the desired information\n",
    "                            filename_without_extension = filename[:filename.index(\".\")]\n",
    "                            schema = {\n",
    "                                \"doc\": datetime.now().strftime(\"%H%M%S%m%d%Y\"),\n",
    "                                \"name\": filename_without_extension,\n",
    "                                \"namespace\": country+\"_\"+folder,\n",
    "                                \"type\": \"record\",\n",
    "                                \"fields\": fields\n",
    "                            }\n",
    "                            schema = json.dumps(schema)\n",
    "                            schema = avro.schema.parse(schema)\n",
    "                            # mutate the records to AVRO format\n",
    "                            records = data.to_dict('records')\n",
    "                            for dicts in records:\n",
    "                                # ensure all the data are strings\n",
    "                                for keys in dicts:\n",
    "                                    dicts[keys] = str(dicts[keys])\n",
    "                            # create the AVRO file with the data in the local system\n",
    "                            writer = DataFileWriter(open(path+filename_without_extension+'.avro', \"wb\"), DatumWriter(), schema)\n",
    "                            # append each record into the file\n",
    "                            for record in records:\n",
    "                                writer.append(record)\n",
    "                            writer.close()\n",
    "                        \n",
    "                            # after creating the AVRO file, upload it into HDFS\n",
    "                            load_hdfs = subprocess.run([hadoop + '', 'fs', '-put', path+filename_without_extension+'.avro',  '/user/bdm/persistent/'+country+'/'+folderpath+'/'+year+'/'+metadata_directory], capture_output=True, shell=True)\n",
    "                            # print status\n",
    "                            if load_hdfs.returncode == 0 :\n",
    "                                print('File ' + filename_without_extension+'.avro was created in /user/bdm/persistent/'+country+'/'+folderpath+'/'+year+'/'+metadata_directory)\n",
    "                                logger.error(datetime.now().strftime(\"%m/%d/%Y, %H:%M:%S\"))\n",
    "                                logger.error('File ' + filename_without_extension+'.avro was created in /user/bdm/persistent/'+country+'/'+folderpath+'/'+year+'/'+metadata_directory)\n",
    "                                # HERE THE LINE OF CODE TO DELETE THE FILE IN THE TEMPORAL LANDING ZONE AFTER IT IS PASSED TO THE PERSISTENT\n",
    "                            else: \n",
    "                                print('An error occured, the file could not be created')\n",
    "                                logger.error(datetime.now().strftime(\"%m/%d/%Y, %H:%M:%S\"))\n",
    "                                logger.error('An error occured, the file could not be created')\n",
    "                                logger.error(load_hdfs)\n",
    "                            # delete the file in local\n",
    "                            os.remove(path+filename_without_extension+'.avro')\n",
    "\n",
    "                    else: \n",
    "                        # if there was an error creating the directory\n",
    "                        print('An error occured, the directory could not be created')\n",
    "                        logger.error(datetime.now().strftime(\"%m/%d/%Y, %H:%M:%S\"))\n",
    "                        logger.error('An error occured, the directory could not be created')\n",
    "                        logger.error(load_hdfs)  \n",
    "                \n",
    "                        # add the time used for this step\n",
    "                        times_metadata.append(time.time() - start_time) \n",
    "                        \n",
    "    return [times_imp_exp, times_metadata]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on: peru | imports | ma07130222.csv...\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'DataFileReader' object has no attribute 'schema'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/home/bdm/DataImporta/P1/production/VM/data_persistance_loader_VM.ipynb Cell 26'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bpietro.ferrazzi/home/bdm/DataImporta/P1/production/VM/data_persistance_loader_VM.ipynb#ch0000033vscode-remote?line=0'>1</a>\u001b[0m load_peru(filenames_year, path)\n",
      "\u001b[1;32m/home/bdm/DataImporta/P1/production/VM/data_persistance_loader_VM.ipynb Cell 25'\u001b[0m in \u001b[0;36mload_peru\u001b[0;34m(filenames_year, path)\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bpietro.ferrazzi/home/bdm/DataImporta/P1/production/VM/data_persistance_loader_VM.ipynb#ch0000025vscode-remote?line=109'>110</a>\u001b[0m     avro_records\u001b[39m.\u001b[39mappend(record)\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bpietro.ferrazzi/home/bdm/DataImporta/P1/production/VM/data_persistance_loader_VM.ipynb#ch0000025vscode-remote?line=110'>111</a>\u001b[0m \u001b[39m# get old schema\u001b[39;00m\n\u001b[0;32m--> <a href='vscode-notebook-cell://ssh-remote%2Bpietro.ferrazzi/home/bdm/DataImporta/P1/production/VM/data_persistance_loader_VM.ipynb#ch0000025vscode-remote?line=111'>112</a>\u001b[0m schema \u001b[39m=\u001b[39m json\u001b[39m.\u001b[39mloads(reader\u001b[39m.\u001b[39;49mschema)\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bpietro.ferrazzi/home/bdm/DataImporta/P1/production/VM/data_persistance_loader_VM.ipynb#ch0000025vscode-remote?line=112'>113</a>\u001b[0m reader\u001b[39m.\u001b[39mclose()\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bpietro.ferrazzi/home/bdm/DataImporta/P1/production/VM/data_persistance_loader_VM.ipynb#ch0000025vscode-remote?line=113'>114</a>\u001b[0m \u001b[39m# modify schema\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFileReader' object has no attribute 'schema'"
     ]
    }
   ],
   "source": [
    "load_peru(filenames_year, path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Chile loading pipeline*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------\n",
    "# CHILE (from temporal to persitent in AVRO)\n",
    "# ------------------------------------------\n",
    "def load_chile(filenames_year, path):\n",
    "\n",
    "    country = \"chile\"\n",
    "    \n",
    "    # initial file name inside a directory of metadata or imports in the persistent zone\n",
    "    initial_name = \"version0.avro\"\n",
    "\n",
    "    # paths to folders\n",
    "    folders = {\"imports\":\"imp\", \"exports\":\"exp\", \"metadata\":\"metadata\"}\n",
    "\n",
    "    # Lists of times\n",
    "    times_imp_exp =[]\n",
    "    times_metadata =[]\n",
    "\n",
    "    # iterate trough every folder\n",
    "    for folder, folderpath in folders.items():\n",
    "\n",
    "        # ---------------------\n",
    "        # IMPORTS & EXPORTS\n",
    "        # ---------------------\n",
    "        if folder == 'imports' or folder == 'exports':\n",
    "\n",
    "            # Iterate trough every import/export file\n",
    "            for filename, year in filenames_year[country][folder].items():\n",
    "                \n",
    "                # Starting time of the conversion and uploading of this file\n",
    "                start_time = time.time()\n",
    "                \n",
    "                # Print status\n",
    "                print(\"Working on: \"+country+\" | \"+folder+\" | \"+filename+'...')\n",
    "                logger.error(datetime.now().strftime(\"%m/%d/%Y, %H:%M:%S\"))\n",
    "                logger.error(\"Working on: \"+country+\" | \"+folder+\" | \"+filename+'...')\n",
    "\n",
    "                # Check if there is any persistent file to append the temporal file's data into (e.g. the most recent version). If there is no persistent file, create one (version0.avro)\n",
    "                all_versions = subprocess.run(hadoop + ' fs -ls /user/bdm/persistent/'+country+'/'+folderpath+'/'+year, capture_output=True, shell=True).stdout.decode()\n",
    "                \n",
    "                # If no persistent file, create one\n",
    "                if all_versions == '':\n",
    "                    # convert the temporal file to AVRO\n",
    "                    # - first retrieve the data and convert it into a dataframe\n",
    "                    data = subprocess.run([hadoop + '','fs','-cat', '/user/bdm/temporal/'+country+'/'+folderpath+'/'+filename], capture_output=True, shell=True, encoding=\"latin-1\").stdout\n",
    "                    data = pd.read_csv(StringIO(data), delimiter=\";\", header=None)\n",
    "                    # convert the default assigned headers (0,1,2,3...) to string\n",
    "                    headers_string = {}\n",
    "                    for col in list(data.columns):\n",
    "                        headers_string[col] = str(col)\n",
    "                    data.rename(columns=headers_string, inplace=True)\n",
    "                    # - then create the AVRO schema\n",
    "                    # -- get the column names and create the fields argument for the schema\n",
    "                    fields = []\n",
    "                    for col in list(data.columns):\n",
    "                        fields.append({\"name\":col, \"type\": \"string\"})\n",
    "                    # -- complete the schema with the desired information\n",
    "                    schema = {\n",
    "                        \"doc\": datetime.now().strftime(\"%H%M%S%m%d%Y\"),\n",
    "                        \"name\": \"trade_item\",\n",
    "                        \"namespace\": country+\"_\"+folder,\n",
    "                        \"type\": \"record\",\n",
    "                        \"fields\": fields\n",
    "                    }\n",
    "                    schema = json.dumps(schema)\n",
    "                    schema = avro.schema.parse(schema)\n",
    "                    # - mutate the records to AVRO format\n",
    "                    records = data.to_dict('records')\n",
    "                    for dicts in records:\n",
    "                        # ensure all the data are strings\n",
    "                        for keys in dicts:\n",
    "                            dicts[keys] = str(dicts[keys])\n",
    "                    # - create the AVRO file with the data in the local system\n",
    "                    writer = DataFileWriter(open(path+initial_name, \"wb\"), DatumWriter(), schema)\n",
    "                    # - append each record into the file\n",
    "                    for record in records:\n",
    "                        writer.append(record)\n",
    "                    writer.close()\n",
    "                \n",
    "                    # after creating the AVRO file, upload it into HDFS\n",
    "                    load_hdfs = subprocess.run([hadoop + '', 'fs', '-put', path+initial_name,  '/user/bdm/persistent/'+country+'/'+folderpath+'/'+year], capture_output=True, shell=True)\n",
    "                    # print status\n",
    "                    if load_hdfs.returncode == 0 :\n",
    "                        print('First version created: version0.avro file was created in /user/bdm/persistent/'+country+'/'+folderpath+'/'+year+'/')\n",
    "                        logger.error(datetime.now().strftime(\"%m/%d/%Y, %H:%M:%S\"))\n",
    "                        logger.error('First version created: version0.avro file was created in /user/bdm/persistent/'+country+'/'+folderpath+'/'+year+'/')\n",
    "                    else: \n",
    "                        print('An error occured, upload not performed')\n",
    "                        logger.error(datetime.now().strftime(\"%m/%d/%Y, %H:%M:%S\"))\n",
    "                        logger.error('An error occured, upload not performed')\n",
    "                        logger.error(load_hdfs)\n",
    "                    # delete the file in local\n",
    "                    os.remove(path+initial_name)\n",
    "\n",
    "                    # add the time used for this step\n",
    "                    times_imp_exp.append(['First occurance', time.time() - start_time]) \n",
    "\n",
    "                # if there is a persistent file (e.g. a previous version)\n",
    "                if all_versions != '':\n",
    "                    # get a list with all the versions\n",
    "                    all_versions = re.findall('/.*/(.*)\\r', all_versions)\n",
    "                    # get the most recent version\n",
    "                    most_recent_version = initial_name\n",
    "                    for version in all_versions:\n",
    "                        if re.search('(\\d*)(?=.avro)', version).group(1) > re.search('(\\d*)(?=.avro)', most_recent_version).group(1):\n",
    "                            most_recent_version = version\n",
    "                    # retrieve that version's file from hadoop\n",
    "                    old_avro = subprocess.run([hadoop + '', 'fs', '-get',  '/user/bdm/persistent/'+country+'/'+folderpath+'/'+year+'/'+most_recent_version, path], capture_output=True, shell=True)\n",
    "                    # get old records (this step can be omited if we only append the new records into the file directly)\n",
    "                    avro_records = []\n",
    "                    reader = DataFileReader(open(path+most_recent_version, \"rb\"), DatumReader())\n",
    "                    for record in reader:\n",
    "                        avro_records.append(record)\n",
    "                    # get old schema\n",
    "                    schema = json.loads(reader.schema)\n",
    "                    reader.close()\n",
    "                    # modify schema\n",
    "                    schema[\"doc\"] = datetime.now().strftime(\"%H%M%S%m%d%Y\")\n",
    "                    schema = json.dumps(schema)\n",
    "                    schema = avro.schema.parse(schema)\n",
    "                    # delete the file in local\n",
    "                    os.remove(path+most_recent_version)\n",
    "                    # convert the temporal file to AVRO\n",
    "                    # - first retrieve the data and convert it into a dataframe\n",
    "                    data = subprocess.run([hadoop + '','fs','-cat', '/user/bdm/temporal/'+country+'/'+folderpath+'/'+filename], capture_output=True, shell=True, encoding=\"latin-1\").stdout\n",
    "                    data = pd.read_csv(StringIO(data), delimiter=\";\", header=None)\n",
    "                    # convert the default assigned headers (0,1,2,3...) to string\n",
    "                    headers_string = {}\n",
    "                    for col in list(data.columns):\n",
    "                        headers_string[col] = str(col)\n",
    "                    data.rename(columns=headers_string, inplace=True)\n",
    "                    # - mutate the records to AVRO format\n",
    "                    new_records = data.to_dict('records')\n",
    "                    for dicts in new_records:\n",
    "                        # ensure all the data are strings\n",
    "                        for keys in dicts:\n",
    "                            dicts[keys] = str(dicts[keys])\n",
    "                    # append the new records into the old ones\n",
    "                    for record in new_records:\n",
    "                        avro_records.append(record)\n",
    "                    # define name for the AVRO file to upload\n",
    "                    final_name = \"version\"+str(int(re.search('(\\d*)(?=.avro)', most_recent_version).group(1))+1)+'.avro'\n",
    "                    # create the AVRO file with the data in the local system\n",
    "                    writer = DataFileWriter(open(path+final_name, \"wb\"), DatumWriter(), schema)\n",
    "                    # append each record into the file\n",
    "                    for record in avro_records:\n",
    "                        writer.append(record)\n",
    "                    writer.close()\n",
    "                    # after creating the AVRO file, upload it into HDFS\n",
    "                    load_hdfs = subprocess.run([hadoop + '', 'fs', '-put', path+final_name,  '/user/bdm/persistent/'+country+'/'+folderpath+'/'+year], capture_output=True, shell=True)\n",
    "                    # print status\n",
    "                    if load_hdfs.returncode == 0 :\n",
    "                        print('Update performed: '+final_name+' file was created in /user/bdm/persistent/'+country+'/'+folderpath+'/'+year+'/')\n",
    "                        logger.error(datetime.now().strftime(\"%m/%d/%Y, %H:%M:%S\"))\n",
    "                        logger.error('Update performed: '+final_name+' file was created in /user/bdm/persistent/'+country+'/'+folderpath+'/'+year+'/')\n",
    "                        # HERE THE LINE OF CODE TO DELETE THE FILE IN THE TEMPORAL LANDING ZONE AFTER IT IS PASSED TO THE PERSISTENT\n",
    "                    else: \n",
    "                        print('An error occured, update not performed')\n",
    "                        logger.error(datetime.now().strftime(\"%m/%d/%Y, %H:%M:%S\"))\n",
    "                        logger.error('An error occured, update not performed')\n",
    "                        logger.error(load_hdfs)\n",
    "                    # delete the file in local\n",
    "                    os.remove(path+final_name)\n",
    "\n",
    "                    # add the time used for this step\n",
    "                    times_imp_exp.append([final_name, time.time() - start_time]) \n",
    "        \n",
    "        # ---------------------\n",
    "        # METADATA\n",
    "        # ---------------------\n",
    "        if folder == 'metadata':\n",
    "\n",
    "            # Starting time of the conversion and uploading of this file\n",
    "            start_time = time.time()\n",
    "            \n",
    "            folders_metadata = {\"imports\":\"imp\", \"exports\":\"exp\"}\n",
    "\n",
    "            # name of the directory to store the metadata\n",
    "            metadata_directory_name = datetime.now().strftime(\"%H%M%S%m%d%Y\")\n",
    "\n",
    "            # iterate trough every metadata folder\n",
    "            for folder_metadata, folder_metadata_path in folders_metadata.items():\n",
    "                \n",
    "                # if there is data in the metadata imports/exports to be uploaded, create the directory in the persistent to store it\n",
    "                if list(filenames_year[country][folder][folder_metadata].items()):\n",
    "                    # get year of first file in metadata imports/exports in temporal\n",
    "                    year = list(filenames_year[country][folder][folder_metadata].items())[0][1]\n",
    "                    metadata_directory = subprocess.run([hadoop + '', 'fs', '-mkdir',  '/user/bdm/persistent/'+country+'/'+folderpath+'/'+folder_metadata_path+'/'+year+'/'+metadata_directory_name], capture_output=True, shell=True)\n",
    "                    if metadata_directory.returncode == 0 :\n",
    "                        print(\"Directory named \"+metadata_directory_name+' created in '+ '/user/bdm/persistent/'+country+'/'+folderpath+'/'+folder_metadata_path+'/'+year+'/')\n",
    "                        logger.error(datetime.now().strftime(\"%m/%d/%Y, %H:%M:%S\"))\n",
    "                        logger.error(\"Directory named \"+metadata_directory_name+' created in '+ '/user/bdm/persistent/'+country+'/'+folderpath+'/'+folder_metadata_path+'/'+year+'/')\n",
    "                    else: \n",
    "                        print('An error occured, folder could not be created')\n",
    "                        logger.error(datetime.now().strftime(\"%m/%d/%Y, %H:%M:%S\"))\n",
    "                        logger.error('An error occured, folder could not be created')\n",
    "                        logger.error(load_hdfs)\n",
    "\n",
    "                # Iterate trough every metadata file\n",
    "                for filename, year in filenames_year[country][folder][folder_metadata].items():\n",
    "\n",
    "                    # Print status\n",
    "                    print(\"Working on: \"+country+\" | \"+folder+\" | \"+folder_metadata+\" | \"+filename+'...')\n",
    "                    logger.error(datetime.now().strftime(\"%m/%d/%Y, %H:%M:%S\"))\n",
    "                    logger.error(\"Working on: \"+country+\" | \"+folder+\" | \"+folder_metadata+\" | \"+filename+'...')\n",
    "\n",
    "                    # send data to persistent\n",
    "                    load_hdfs = subprocess.run([hadoop + '', 'fs', '-cp',  '/user/bdm/temporal/'+country+'/'+folderpath+'/'+folder_metadata_path+'/'+filename,  '/user/bdm/persistent/'+country+'/'+folderpath+'/'+folder_metadata_path+'/'+year+'/'+metadata_directory_name], capture_output=True, shell=True)\n",
    "                    if load_hdfs.returncode == 0 :\n",
    "                        print('File '+filename+' uploaded into '+ '/user/bdm/persistent/'+country+'/'+folderpath+'/'+folder_metadata_path+'/'+year+'/')\n",
    "                        logger.error(datetime.now().strftime(\"%m/%d/%Y, %H:%M:%S\"))\n",
    "                        logger.error('File '+filename+' uploaded into '+ '/user/bdm/persistent/'+country+'/'+folderpath+'/'+folder_metadata_path+'/'+year+'/')\n",
    "                        # HERE THE LINE OF CODE TO DELETE THE FILE IN THE TEMPORAL LANDING ZONE AFTER IT IS PASSED TO THE PERSISTENT\n",
    "\n",
    "                    else: \n",
    "                        print('An error occured, upload not performed')\n",
    "                        logger.error(datetime.now().strftime(\"%m/%d/%Y, %H:%M:%S\"))\n",
    "                        logger.error('An error occured, upload not performed')\n",
    "                        logger.error(load_hdfs)\n",
    "\n",
    "            # add the time used for this step\n",
    "            times_metadata.append( time.time() - start_time) \n",
    "                        \n",
    "    return [times_imp_exp, times_metadata]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Brazil loading pipeline*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------\n",
    "# BRAZIL (from temporal to persitent in AVRO)\n",
    "# ------------------------------------------\n",
    "def load_brazil(filenames_year, path):\n",
    "    \n",
    "    country = \"brazil\"\n",
    "\n",
    "    # initial file name inside a directory of metadata or imports in the persistent zone\n",
    "    initial_name = \"version0.avro\"\n",
    "\n",
    "    # paths to folders\n",
    "    folders = {\"imports\":\"imp\", \"exports\":\"exp\", \"metadata\":\"metadata\"}\n",
    "    \n",
    "    # Lists of times\n",
    "    times_imp_exp =[]\n",
    "    times_metadata =[]\n",
    "\n",
    "    # iterate trough every folder\n",
    "    for folder, folderpath in folders.items():\n",
    "\n",
    "            # ---------------------\n",
    "            # IMPORTS & EXPORTS\n",
    "            # ---------------------\n",
    "            if folder == 'imports' or folder == 'exports':\n",
    "                \n",
    "                # Starting time of the conversion and uploading of this file\n",
    "                start_time = time.time()\n",
    "                \n",
    "                # Iterate trough every import/export file\n",
    "                for filename, year in filenames_year[country][folder].items():\n",
    "\n",
    "                    # Print status\n",
    "                    print(\"Working on: \"+country+\" | \"+folder+\" | \"+filename+'...')\n",
    "                    logger.error(datetime.now().strftime(\"%m/%d/%Y, %H:%M:%S\"))\n",
    "                    logger.error(\"Working on: \"+country+\" | \"+folder+\" | \"+filename+'...')\n",
    "\n",
    "                    # Check if there is any persistent file to append the temporal file's data into (e.g. the most recent version). If there is no persistent file, create one (version0.avro)\n",
    "                    all_versions = subprocess.run(hadoop + ' fs -ls /user/bdm/persistent/'+country+'/'+folderpath+'/'+year, capture_output=True, shell=True).stdout.decode()\n",
    "                    \n",
    "                    # If no persistent file, create one\n",
    "                    if all_versions == '':\n",
    "                        # convert the temporal file to AVRO\n",
    "                        # - first retrieve the data and convert it into a dataframe\n",
    "                        data = subprocess.run(hadoop + ' fs -cat /temporal/'+country+'/'+folderpath+'/'+filename, capture_output=True, shell=True, encoding=\"latin-1\").stdout\n",
    "                        data = pd.read_csv(StringIO(data))\n",
    "                        # - then create the AVRO schema\n",
    "                        # -- get the column names and create the fields argument for the schema\n",
    "                        fields = []\n",
    "                        for col in list(data.columns):\n",
    "                            fields.append({\"name\":col, \"type\": \"string\"})\n",
    "                        # -- complete the schema with the desired information\n",
    "                        schema = {\n",
    "                            \"doc\": datetime.now().strftime(\"%H%M%S%m%d%Y\"),\n",
    "                            \"name\": \"trade_item\",\n",
    "                            \"namespace\": country+\"_\"+folder,\n",
    "                            \"type\": \"record\",\n",
    "                            \"fields\": fields\n",
    "                        }\n",
    "                        schema = json.dumps(schema)\n",
    "                        schema = avro.schema.parse(schema)\n",
    "                        # - mutate the records to AVRO format\n",
    "                        records = data.to_dict('records')\n",
    "                        for dicts in records:\n",
    "                            # ensure all the data are strings\n",
    "                            for keys in dicts:\n",
    "                                dicts[keys] = str(dicts[keys])\n",
    "                        # - create the AVRO file with the data in the local system\n",
    "                        writer = DataFileWriter(open(path+initial_name, \"wb\"), DatumWriter(), schema)\n",
    "                        # - append each record into the file\n",
    "                        for record in records:\n",
    "                            writer.append(record)\n",
    "                        writer.close()\n",
    "                    \n",
    "                        # after creating the AVRO file, upload it into HDFS\n",
    "                        load_hdfs = subprocess.run([hadoop + '', 'fs', '-put', path+initial_name,  '/user/bdm/persistent/'+country+'/'+folderpath+'/'+year], capture_output=True, shell=True)\n",
    "                        # print status\n",
    "                        if load_hdfs.returncode == 0 :\n",
    "                            print('First version created: version0.avro file was created in /user/bdm/persistent/'+country+'/'+folderpath+'/'+year+'/')\n",
    "                            logger.error(datetime.now().strftime(\"%m/%d/%Y, %H:%M:%S\"))\n",
    "                            logger.error('First version created: version0.avro file was created in /user/bdm/persistent/'+country+'/'+folderpath+'/'+year+'/')\n",
    "                        else: \n",
    "                            print('An error occured, upload not performed')\n",
    "                            logger.error(datetime.now().strftime(\"%m/%d/%Y, %H:%M:%S\"))\n",
    "                            logger.error('An error occured, upload not performed')\n",
    "                            logger.error(load_hdfs)\n",
    "\n",
    "                        # delete the file in local\n",
    "                        os.remove(path+initial_name)\n",
    "\n",
    "                        # add the time used for this step\n",
    "                        times_imp_exp.append(['First occurance', time.time() - start_time]) \n",
    "\n",
    "                    # if there is a persistent file (e.g. a previous version)\n",
    "                    if all_versions != '':\n",
    "                        # get a list with all the versions\n",
    "                        all_versions = re.findall('/.*/(.*)\\r', all_versions)\n",
    "                        # get the most recent version\n",
    "                        most_recent_version = initial_name\n",
    "                        for version in all_versions:\n",
    "                            if re.search('(\\d*)(?=.avro)', version).group(1) > re.search('(\\d*)(?=.avro)', most_recent_version).group(1):\n",
    "                                most_recent_version = version\n",
    "                        # retrieve that version's file from hadoop\n",
    "                        old_avro = subprocess.run([hadoop + '', 'fs', '-get',  '/user/bdm/persistent/'+country+'/'+folderpath+'/'+year+'/'+most_recent_version, path], capture_output=True, shell=True)\n",
    "                        # get old records (this step can be omited if we only append the new records into the file directly)\n",
    "                        avro_records = []\n",
    "                        reader = DataFileReader(open(path+most_recent_version, \"rb\"), DatumReader())\n",
    "                        for record in reader:\n",
    "                            avro_records.append(record)\n",
    "                        # get old schema\n",
    "                        schema = json.loads(reader.schema)\n",
    "                        reader.close()\n",
    "                        # modify schema\n",
    "                        schema[\"doc\"] = datetime.now().strftime(\"%H%M%S%m%d%Y\")\n",
    "                        schema = json.dumps(schema)\n",
    "                        schema = avro.schema.parse(schema)\n",
    "                        # delete the file in local\n",
    "                        os.remove(path+most_recent_version)\n",
    "                        # convert the temporal file to AVRO\n",
    "                        # - first retrieve the data and convert it into a dataframe\n",
    "                        data = subprocess.run(hadoop + ' fs -cat /temporal/'+country+'/'+folderpath+'/'+filename, capture_output=True, shell=True, encoding=\"latin-1\").stdout\n",
    "                        data = pd.read_csv(StringIO(data))\n",
    "                        # - mutate the records to AVRO format\n",
    "                        new_records = data.to_dict('records')\n",
    "                        for dicts in new_records:\n",
    "                            # ensure all the data are strings\n",
    "                            for keys in dicts:\n",
    "                                dicts[keys] = str(dicts[keys])\n",
    "                        # append the new records into the old ones\n",
    "                        for record in new_records:\n",
    "                            avro_records.append(record)\n",
    "                        # define name for the AVRO file to upload\n",
    "                        final_name = \"version\"+str(int(re.search('(\\d*)(?=.avro)', most_recent_version).group(1))+1)+'.avro'\n",
    "                        # create the AVRO file with the data in the local system\n",
    "                        writer = DataFileWriter(open(path+final_name, \"wb\"), DatumWriter(), schema)\n",
    "                        # append each record into the file\n",
    "                        for record in avro_records:\n",
    "                            writer.append(record)\n",
    "                        writer.close()\n",
    "                        # after creating the AVRO file, upload it into HDFS\n",
    "                        load_hdfs = subprocess.run([hadoop + '', 'fs', '-put', path+final_name,  '/user/bdm/persistent/'+country+'/'+folderpath+'/'+year], capture_output=True, shell=True)\n",
    "                        # print status\n",
    "                        if load_hdfs.returncode == 0 :\n",
    "                            print('Update performed: '+final_name+' file was created in /user/bdm/persistent/'+country+'/'+folderpath+'/'+year+'/')\n",
    "                            logger.error(datetime.now().strftime(\"%m/%d/%Y, %H:%M:%S\"))\n",
    "                            logger.error('Update performed: '+final_name+' file was created in /user/bdm/persistent/'+country+'/'+folderpath+'/'+year+'/')\n",
    "                            # HERE TE CODE TO DELETE THE FILE IN THE TEMPORAL LANDING ZONE AFTER IT IS PASSED TO THE PERSISTENT\n",
    "                        else: \n",
    "                            print('An error occured, update not performed')\n",
    "                            logger.error(datetime.now().strftime(\"%m/%d/%Y, %H:%M:%S\"))\n",
    "                            logger.error('An error occured, update not performed')\n",
    "                            logger.error(load_hdfs)\n",
    "                        # delete the file in local\n",
    "                        os.remove(path+final_name)\n",
    "\n",
    "                        # add the time used for this step\n",
    "                        times_imp_exp.append([final_name, time.time() - start_time]) \n",
    "            \n",
    "            # ---------------------\n",
    "            # METADATA\n",
    "            # ---------------------\n",
    "            elif folder == \"metadata\":\n",
    "\n",
    "                # Starting time of the conversion and uploading of this file\n",
    "                start_time = time.time()\n",
    "\n",
    "                # If there is medatata to be uploaded, create a directory in the metadata directory in the temporal zone to store the files. The name is %H%M%S%m%d%Y (time)\n",
    "                if filenames_year[country][folder].items():\n",
    "                    metadata_directory = datetime.now().strftime(\"%H%M%S%m%d%Y\")\n",
    "                    # get the year of the any of the files (all have the same year, we will take the first one)\n",
    "                    year = list(filenames_year[country][folder].items())[0][1]\n",
    "                    # create the folder\n",
    "                    create_directory = subprocess.run(hadoop + ' fs -mkdir /user/bdm/persistent/'+country+'/'+folderpath+'/'+year+'/'+metadata_directory, capture_output=True, shell=True, encoding=\"latin-1\")\n",
    "                    if create_directory.returncode == 0 :\n",
    "                        # print success status\n",
    "                        print('A directory named '+metadata_directory+' has been created in /user/bdm/persistent/'+country+'/'+folderpath+'/'+year+'/')\n",
    "                        logger.error(datetime.now().strftime(\"%m/%d/%Y, %H:%M:%S\"))\n",
    "                        logger.error('A directory named '+metadata_directory+' has been created in /user/bdm/persistent/'+country+'/'+folderpath+'/'+year+'/')\n",
    "                        \n",
    "                        # Iterate trough every metadata file\n",
    "                        for filename, year in filenames_year[country][folder].items():\n",
    "                            # print working on\n",
    "                            print(\"Working on: \"+country+\" | \"+folder+\" | \"+filename+'...')\n",
    "                            logger.error(datetime.now().strftime(\"%m/%d/%Y, %H:%M:%S\"))\n",
    "                            logger.error(\"Working on: \"+country+\" | \"+folder+\" | \"+filename+'...') \n",
    "                            # CONVERT FILE TO AVRO\n",
    "                            # get temporal file\n",
    "                            data = subprocess.run([hadoop + '','fs','-cat',' /user/bdm/temporal/'+country+'/'+folderpath+'/'+filename], capture_output=True, shell=True, encoding=\"latin-1\").stdout\n",
    "                            data = pd.read_csv(StringIO(data), delimiter=\";\")\n",
    "                            # then create the AVRO schema\n",
    "                            # - get the column names and create the fields argument for the schema\n",
    "                            fields = []\n",
    "                            for col in list(data.columns):\n",
    "                                fields.append({\"name\":col, \"type\": \"string\"})\n",
    "                            # - complete the schema with the desired information\n",
    "                            filename_without_extension = filename[:filename.index(\".\")].replace(\" \", \"\")\n",
    "                            schema = {\n",
    "                                \"doc\": datetime.now().strftime(\"%H%M%S%m%d%Y\"),\n",
    "                                \"name\": filename_without_extension,\n",
    "                                \"namespace\": country+\"_\"+folder,\n",
    "                                \"type\": \"record\",\n",
    "                                \"fields\": fields\n",
    "                            }\n",
    "                            schema = json.dumps(schema)\n",
    "                            schema = avro.schema.parse(schema)\n",
    "                            # mutate the records to AVRO format\n",
    "                            records = data.to_dict('records')\n",
    "                            for dicts in records:\n",
    "                                # ensure all the data are strings\n",
    "                                for keys in dicts:\n",
    "                                    dicts[keys] = str(dicts[keys])\n",
    "                            # create the AVRO file with the data in the local system\n",
    "                            writer = DataFileWriter(open(path+filename_without_extension+'.avro', \"wb\"), DatumWriter(), schema)\n",
    "                            # append each record into the file\n",
    "                            for record in records:\n",
    "                                writer.append(record)\n",
    "                            writer.close()\n",
    "                        \n",
    "                            # after creating the AVRO file, upload it into HDFS\n",
    "                            load_hdfs = subprocess.run([hadoop + '', 'fs', '-put', path+filename_without_extension+'.avro',  '/user/bdm/persistent/'+country+'/'+folderpath+'/'+year+'/'+metadata_directory], capture_output=True, shell=True)\n",
    "                            # print status\n",
    "                            if load_hdfs.returncode == 0 :\n",
    "                                print('File ' + filename_without_extension+'.avro was created in /user/bdm/persistent/'+country+'/'+folderpath+'/'+year+'/'+metadata_directory)\n",
    "                                logger.error(datetime.now().strftime(\"%m/%d/%Y, %H:%M:%S\"))\n",
    "                                logger.error('File ' + filename_without_extension+'.avro was created in /user/bdm/persistent/'+country+'/'+folderpath+'/'+year+'/'+metadata_directory) \n",
    "                                # HERE THE LINE OF CODE TO DELETE THE FILE IN THE TEMPORAL LANDING ZONE AFTER IT IS PASSED TO THE PERSISTENT\n",
    "                            else: \n",
    "                                print('An error occured, the file could not be created')\n",
    "                                logger.error(datetime.now().strftime(\"%m/%d/%Y, %H:%M:%S\"))\n",
    "                                logger.error('An error occured, the file could not be created') \n",
    "                                logger.error(load_hdfs)\n",
    "                            # delete the file in local\n",
    "                            os.remove(path+filename_without_extension+'.avro')\n",
    "\n",
    "                    else: \n",
    "                        # if there was an error creating the directory\n",
    "                        print('An error occured, the directory could not be created')\n",
    "                        logger.error(datetime.now().strftime(\"%m/%d/%Y, %H:%M:%S\"))\n",
    "                        logger.error('An error occured, the directory could not be created') \n",
    "                        logger.error(load_hdfs)  \n",
    "    \n",
    "            # add the time used for this step\n",
    "            times_metadata.append( time.time() - start_time) \n",
    "                        \n",
    "    return [times_imp_exp, times_metadata]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Call loading functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on: peru | imports | ma07130222.csv...\n",
      "An error occured, the file could not be uploaded\n",
      "CompletedProcess(args='~/BDM_Software/hadoop/bin/hadoop fs -put /home/bdm/DataImporta/P1/production/VM//version0.avro/user/bdm/persistent/peru/imp/2022', returncode=1, stdout=b'', stderr=b\"put: `/home/bdm/DataImporta/P1/production/VM//version0.avro/user/bdm/persistent/peru/imp/2022': No such file or directory\\n\")\n",
      "Working on: peru | imports | ma07130322.csv...\n"
     ]
    }
   ],
   "source": [
    "for country in countries:\n",
    "    # PERU\n",
    "    if country == 'peru':\n",
    "        load_peru(filenames_year, path)\n",
    "    # CHILE\n",
    "    if country == 'chile':\n",
    "        load_chile(filenames_year, path)\n",
    "    # BRAZIL\n",
    "    if country == 'brazil':\n",
    "        load_brazil(filenames_year, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'peru': {'imports': {}, 'exports': {}, 'metadata': {}},\n",
       " 'chile': {'imports': {},\n",
       "  'exports': {},\n",
       "  'metadata': {'imports': {}, 'exports': {}}},\n",
       " 'brazil': {'imports': {}, 'exports': {}, 'metadata': {}}}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filenames_year"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "bbe16ac06b6d4bacd07cf5564ffa86eb58e7e2ab169d83d01490a955ddfe1246"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('bdm')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
