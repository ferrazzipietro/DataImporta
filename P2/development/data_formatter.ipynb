{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "afa202d8",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pyspark'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m/Users/pietro/Desktop/BDM/Project/DataImporta/P2/development/data_formatter.ipynb Cell 1'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/pietro/Desktop/BDM/Project/DataImporta/P2/development/data_formatter.ipynb#ch0000000?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpyspark\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msql\u001b[39;00m \u001b[39mimport\u001b[39;00m SparkSession\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/pietro/Desktop/BDM/Project/DataImporta/P2/development/data_formatter.ipynb#ch0000000?line=1'>2</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpyspark\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msql\u001b[39;00m \u001b[39mimport\u001b[39;00m Row\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/pietro/Desktop/BDM/Project/DataImporta/P2/development/data_formatter.ipynb#ch0000000?line=2'>3</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mre\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pyspark'"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Row\n",
    "import re\n",
    "\n",
    "spark = SparkSession.builder.config(\"spark.jars\", \"/Users/pietro-avro_2.12-3.1.3\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b8d9e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "hdfs_path = 'hdfs://localhost:9000/persistent/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a6d5bdd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "# How it works\n",
    "###\n",
    "# 1) metadata is opened\n",
    "# 2) metadata is formatted as (code, name to be kept in the data)\n",
    "# 3) data is formatted as (code for the join, all row)\n",
    "# the variables that only have to have different names with the same values are kept as they are\n",
    "# the metadata is treated as coming from txt files, not avros.\n",
    "# Avro are always opened as set of Row objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1e1fd1f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FUNCTIONS\n",
    "\n",
    "def delete(row, col_name):\n",
    "    d = row.asDict()\n",
    "    d.pop(col_name)\n",
    "    return Row(**d)\n",
    "\n",
    "# function add one key to a row\n",
    "def insert(row, new_col_name, value):\n",
    "    d = row.asDict()\n",
    "    d[new_col_name] = str(value).rstrip()\n",
    "    return Row(**d)\n",
    "    \n",
    "def lenght(val):\n",
    "    if isinstance(val, int):\n",
    "        return 1\n",
    "    else:\n",
    "        return len(val)\n",
    "\n",
    "# update one field in row\n",
    "def updateRow(row, field_to_update, value):\n",
    "    d = row.asDict()\n",
    "    d[field_to_update] = value \n",
    "    return Row(**d)\n",
    "\n",
    "def changeNames(row, old_names, new_names):\n",
    "    for i in range(0, len(old_names)):\n",
    "        row = insert(row, new_names[i], row[old_names[i]])\n",
    "        row = delete(row, old_names[i])\n",
    "    return row\n",
    "\n",
    "def keepOnly(row, fields_to_be_kept):\n",
    "    new_row = Row(var=True)\n",
    "    for field in fields_to_be_kept:\n",
    "        new_row = insert(new_row, field, row[field])\n",
    "    new_row = delete(new_row, \"var\")\n",
    "    return new_row  \n",
    "\n",
    "def merging_data_meta (rdd, meta_file_name, key_pos_in_meta, name_pos_in_meta, key_name_in_data, keep_code, new_col_name, key_as_int, path_to_metadata, country):\n",
    "    \n",
    "    # METADATA\n",
    "    if country == \"peru\":\n",
    "        # if the first element starts with \"C\" or \"t\", it has to be dropped (it is the header)\n",
    "        meta_split = sc.textFile(path_to_metadata + meta_file_name).filter(lambda l: not l.startswith(tuple([\"C\",\"t\"]))).map(lambda l: tuple(l.split(\"\\t\")))\n",
    "        #print(meta_split.take(4))\n",
    "        #print(meta_split.take(2)) \n",
    "    \n",
    "    elif country==\"brasil\":\n",
    "        if meta_file_name == \"NCM.csv\":\n",
    "            delim = \";\"\n",
    "            meta_split = sc.textFile(path_to_metadata + meta_file_name).filter(lambda l: not l.startswith(\"C\")).map(lambda l: tuple(tuple(l.split(\"\\n\"))[0].split(delim)))\n",
    "        else:\n",
    "            delim=\"\\\",\\\"\"\n",
    "            meta_split = sc.textFile(path_to_metadata + meta_file_name).filter(lambda l: not l.startswith(\"\\\"C\")).map(lambda l: tuple(tuple(l.split(\"\\n\"))[0][1:-1].split(delim)))\n",
    "    else:\n",
    "        print('--- ERROR: wrong country name --')\n",
    "    # prepare metadata for the join\n",
    "    if lenght(key_pos_in_meta) == 2: # the key is the combination of 2 columns\n",
    "        meta_ready_for_join0 = meta_split.map(lambda l: ( l[key_pos_in_meta[0]].strip() + l[key_pos_in_meta[1]].strip(), l[name_pos_in_meta].rstrip())).distinct()\n",
    "    elif (key_as_int):\n",
    "        meta_ready_for_join0 = meta_split.map(lambda l: ( int(l[key_pos_in_meta].strip()), l[name_pos_in_meta].rstrip())).distinct()\n",
    "    else:\n",
    "        meta_ready_for_join0 = meta_split.map(lambda l: ( l[key_pos_in_meta].strip(), l[name_pos_in_meta].rstrip())).distinct() \n",
    "    \n",
    "    # if there are repetiotions of the key with typos on the attr. name that should instead be always the same\n",
    "    meta_ready_for_join = meta_ready_for_join0.groupByKey().mapValues(list).map(lambda l: (l[0],l[1][0]))\n",
    "\n",
    "    # DATA\n",
    "    # prepare data for the join:\n",
    "    if (key_as_int):\n",
    "        # if there are nan, we set them to 9999999999\n",
    "        data_ready_for_join = rdd.map(lambda l: updateRow(l, key_name_in_data, \"9999999999\") if l[key_name_in_data]=='nan' else l).map(lambda l: (int( l[key_name_in_data].split(\".\")[0]) , l) )\n",
    "        # print(data_ready_for_join.take(5))\n",
    "    else:\n",
    "        data_ready_for_join = rdd.map(lambda l: ( l[key_name_in_data], l))\n",
    "   \n",
    "    # JOIN\n",
    "    join = data_ready_for_join.leftOuterJoin(meta_ready_for_join)\n",
    "    \n",
    "    if (not keep_code): # remove the column with the code\n",
    "        merged = join.map(lambda l:  (delete(l[1][0], key_name_in_data), l[1][1]) )\n",
    "        #print(merged.take(1))\n",
    "\n",
    "    else:\n",
    "        merged = join.map(lambda l:  (l[1][0], l[1][1]) )\n",
    "    \n",
    "    complete = merged.map(lambda l:  insert(l[0], new_col_name, l[1]) )\n",
    "    \n",
    "    return complete\n",
    "\n",
    "\n",
    "\n",
    "def create_composed_columns(rdd, source_col_names, dest_col_name, string = False, operation = \"+\"):\n",
    "    \n",
    "    # \"+\" (addition for numbers or concatenating strings)\n",
    "    # \"/\" (division for numberes)\n",
    "    \n",
    "    if operation == \"+\":\n",
    "        if string:\n",
    "            return rdd.map(lambda l: insert(l, dest_col_name, ' '.join(l[s].strip() for s in source_col_names if l[s])))\n",
    "        return rdd.map(lambda l: insert(l, dest_col_name, int(float(l[source_col_names[0]].replace(\",\",\".\"))) + int(float(l[source_col_names[1]].replace(\",\",\".\"))) ))\n",
    "    if operation == \"/\":\n",
    "        return rdd.map(lambda l: insert(l, dest_col_name, int(float(l[source_col_names[0]].strip())) / (int(float(l[source_col_names[1]].strip())) + 0.000001)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2f22b836",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(CODI_ADUAN='172', ANO_PRESE='22', NUME_CORRE='2418', FECH_INGSI='20220210', TIPO_DOCUM='4', LIBR_TRIBU='20602049451', DNOMBRE='\"IMPORTACIONES MUSSA EMPRESA INDIVIDUAL', CODI_AGENT='7521', FECH_LLEGA='0', VIA_TRANSP='7', EMPR_TRANS='nan', CODI_ALMA='9998', CADU_MANIF='172', FECH_MANIF='2022', NUME_MANIF='0', FECH_RECEP='20220210', FECH_CANCE='20220210', TIPO_CANCE='33', BANC_CANCE='172.0', CODI_ENFIN='nan', DK='T', PAIS_ORIGE='CL', PAIS_ADQUI='CL', PUER_EMBAR='CLARI', FECH_EMBAR='20220210', NUME_SERIE='2', PART_NANDI='808100000', DESC_COMER='MANZANAS FRESCAS,SIN MARCA,SIN MODELO,', DESC_MATCO='VARIEDAD RED DELICIOUS CON 20 KN', DESC_USOAP='PARA CONSUMO', DESC_FOPRE='KILOGRAMOS CAJAS', DESC_OTROS='nan', FOB_DOLPOL='2352.0', FLE_DOLAR='42.14', SEG_DOLAR='58.8', PESO_NETO='3920.0', PESO_BRUTO='4214.0', UNID_FIQTY='3920.0', UNID_FIDES='KG', QUNICOM='196.0', TUNICOM='CAJ', SEST_MERCA='10', ADV_DOLAR='0.0', IGV_DOLAR='0.0', ISC_DOLAR='0.0', IPM_DOLAR='0.0', DES_DOLAR='0.0', IPA_DOLAR='0.0', SAD_DOLAR='0.0', DER_ADUM='0.0', COMM='0.0', FMOD='20220210', CANT_BULTO='196.0', CLASE='CAJ', TRAT_PREFE='0', TIPO_TRAT='1', CODI_LIBER='0', IMPR_RELIQ='nan')]\n",
      "meta_file_name ['Partidas.txt', 'Paises.txt', 'Paises.txt', 'Puertos.txt', 'MedTransporte.txt', 'Agente.txt', 'Bancos.txt', 'RecintAduaner.txt', 'EstMercancia.txt']\n",
      "key_pos_in_meta [0, 0, 0, [0, 2], 0, 0, 0, 0, 0]\n",
      "name_pos_in_meta [1, 1, 1, 3, 1, 1, 1, 1, 1]\n",
      "key_name_in_data ['PART_NANDI', 'PAIS_ORIGE', 'PAIS_ADQUI', 'PUER_EMBAR', 'VIA_TRANSP', 'CODI_AGENT', 'BANC_CANCE', 'CODI_ALMA', 'SEST_MERCA']\n",
      "keep_code [True, False, False, False, True, False, False, False, False]\n",
      "new_col_name ['custom_description', 'country_of_origin', 'country_of_arrival', 'port_of_boarding', 'mean_of_transport', 'agente', 'bank', 'warehouse', 'state']\n",
      "key_as_int [True, False, False, False, True, True, True, False, False]\n",
      "[Row(CODI_ADUAN='172', ANO_PRESE='22', NUME_CORRE='2418', FECH_INGSI='20220210', TIPO_DOCUM='4', LIBR_TRIBU='20602049451', DNOMBRE='\"IMPORTACIONES MUSSA EMPRESA INDIVIDUAL', CODI_AGENT='7521', FECH_LLEGA='0', VIA_TRANSP='7', EMPR_TRANS='nan', CODI_ALMA='9998', CADU_MANIF='172', FECH_MANIF='2022', NUME_MANIF='0', FECH_RECEP='20220210', FECH_CANCE='20220210', TIPO_CANCE='33', BANC_CANCE='172.0', CODI_ENFIN='nan', DK='T', PAIS_ORIGE='CL', PAIS_ADQUI='CL', PUER_EMBAR='CLARI', FECH_EMBAR='20220210', NUME_SERIE='2', PART_NANDI='808100000', DESC_COMER='MANZANAS FRESCAS,SIN MARCA,SIN MODELO,', DESC_MATCO='VARIEDAD RED DELICIOUS CON 20 KN', DESC_USOAP='PARA CONSUMO', DESC_FOPRE='KILOGRAMOS CAJAS', DESC_OTROS='nan', FOB_DOLPOL='2352.0', FLE_DOLAR='42.14', SEG_DOLAR='58.8', PESO_NETO='3920.0', PESO_BRUTO='4214.0', UNID_FIQTY='3920.0', UNID_FIDES='KG', QUNICOM='196.0', TUNICOM='CAJ', SEST_MERCA='10', ADV_DOLAR='0.0', IGV_DOLAR='0.0', ISC_DOLAR='0.0', IPM_DOLAR='0.0', DES_DOLAR='0.0', IPA_DOLAR='0.0', SAD_DOLAR='0.0', DER_ADUM='0.0', COMM='0.0', FMOD='20220210', CANT_BULTO='196.0', CLASE='CAJ', TRAT_PREFE='0', TIPO_TRAT='1', CODI_LIBER='0', IMPR_RELIQ='nan')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/06/10 17:41:52 ERROR Executor: Exception in task 0.0 in stage 131.0 (TID 305)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/opt/homebrew/Cellar/apache-spark/3.2.1/libexec/python/lib/pyspark.zip/pyspark/worker.py\", line 619, in main\n",
      "    process()\n",
      "  File \"/opt/homebrew/Cellar/apache-spark/3.2.1/libexec/python/lib/pyspark.zip/pyspark/worker.py\", line 609, in process\n",
      "    out_iter = func(split_index, iterator)\n",
      "  File \"/opt/homebrew/Cellar/apache-spark/3.2.1/libexec/python/pyspark/rdd.py\", line 2918, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "  File \"/opt/homebrew/Cellar/apache-spark/3.2.1/libexec/python/pyspark/rdd.py\", line 2918, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "  File \"/opt/homebrew/Cellar/apache-spark/3.2.1/libexec/python/pyspark/rdd.py\", line 417, in func\n",
      "    return f(iterator)\n",
      "  File \"/opt/homebrew/Cellar/apache-spark/3.2.1/libexec/python/pyspark/rdd.py\", line 2146, in combineLocally\n",
      "    merger.mergeValues(iterator)\n",
      "  File \"/opt/homebrew/Cellar/apache-spark/3.2.1/libexec/python/lib/pyspark.zip/pyspark/shuffle.py\", line 240, in mergeValues\n",
      "    for k, v in iterator:\n",
      "  File \"/opt/homebrew/Cellar/apache-spark/3.2.1/libexec/python/lib/pyspark.zip/pyspark/util.py\", line 74, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/var/folders/s_/9g6p7mn936j1qzv2gp75jsz00000gn/T/ipykernel_8124/3841941305.py\", line 61, in <lambda>\n",
      "ValueError: invalid literal for int() with base 10: '101210000\\tCABALLOS VIVOS REPRODUCTORES DE RAZA PUR'\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:555)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:713)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:695)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:508)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1211)\n",
      "\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1217)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "22/06/10 17:41:52 WARN TaskSetManager: Lost task 0.0 in stage 131.0 (TID 305) (10-192-220-80client.eduroam.upc.edu executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/opt/homebrew/Cellar/apache-spark/3.2.1/libexec/python/lib/pyspark.zip/pyspark/worker.py\", line 619, in main\n",
      "    process()\n",
      "  File \"/opt/homebrew/Cellar/apache-spark/3.2.1/libexec/python/lib/pyspark.zip/pyspark/worker.py\", line 609, in process\n",
      "    out_iter = func(split_index, iterator)\n",
      "  File \"/opt/homebrew/Cellar/apache-spark/3.2.1/libexec/python/pyspark/rdd.py\", line 2918, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "  File \"/opt/homebrew/Cellar/apache-spark/3.2.1/libexec/python/pyspark/rdd.py\", line 2918, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "  File \"/opt/homebrew/Cellar/apache-spark/3.2.1/libexec/python/pyspark/rdd.py\", line 417, in func\n",
      "    return f(iterator)\n",
      "  File \"/opt/homebrew/Cellar/apache-spark/3.2.1/libexec/python/pyspark/rdd.py\", line 2146, in combineLocally\n",
      "    merger.mergeValues(iterator)\n",
      "  File \"/opt/homebrew/Cellar/apache-spark/3.2.1/libexec/python/lib/pyspark.zip/pyspark/shuffle.py\", line 240, in mergeValues\n",
      "    for k, v in iterator:\n",
      "  File \"/opt/homebrew/Cellar/apache-spark/3.2.1/libexec/python/lib/pyspark.zip/pyspark/util.py\", line 74, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/var/folders/s_/9g6p7mn936j1qzv2gp75jsz00000gn/T/ipykernel_8124/3841941305.py\", line 61, in <lambda>\n",
      "ValueError: invalid literal for int() with base 10: '101210000\\tCABALLOS VIVOS REPRODUCTORES DE RAZA PUR'\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:555)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:713)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:695)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:508)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1211)\n",
      "\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1217)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n",
      "22/06/10 17:41:52 ERROR TaskSetManager: Task 0 in stage 131.0 failed 1 times; aborting job\n",
      "22/06/10 17:41:52 WARN TaskSetManager: Lost task 1.0 in stage 131.0 (TID 306) (10-192-220-80client.eduroam.upc.edu executor driver): TaskKilled (Stage cancelled)\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 131.0 failed 1 times, most recent failure: Lost task 0.0 in stage 131.0 (TID 305) (10-192-220-80client.eduroam.upc.edu executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/opt/homebrew/Cellar/apache-spark/3.2.1/libexec/python/lib/pyspark.zip/pyspark/worker.py\", line 619, in main\n    process()\n  File \"/opt/homebrew/Cellar/apache-spark/3.2.1/libexec/python/lib/pyspark.zip/pyspark/worker.py\", line 609, in process\n    out_iter = func(split_index, iterator)\n  File \"/opt/homebrew/Cellar/apache-spark/3.2.1/libexec/python/pyspark/rdd.py\", line 2918, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/opt/homebrew/Cellar/apache-spark/3.2.1/libexec/python/pyspark/rdd.py\", line 2918, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/opt/homebrew/Cellar/apache-spark/3.2.1/libexec/python/pyspark/rdd.py\", line 417, in func\n    return f(iterator)\n  File \"/opt/homebrew/Cellar/apache-spark/3.2.1/libexec/python/pyspark/rdd.py\", line 2146, in combineLocally\n    merger.mergeValues(iterator)\n  File \"/opt/homebrew/Cellar/apache-spark/3.2.1/libexec/python/lib/pyspark.zip/pyspark/shuffle.py\", line 240, in mergeValues\n    for k, v in iterator:\n  File \"/opt/homebrew/Cellar/apache-spark/3.2.1/libexec/python/lib/pyspark.zip/pyspark/util.py\", line 74, in wrapper\n    return f(*args, **kwargs)\n  File \"/var/folders/s_/9g6p7mn936j1qzv2gp75jsz00000gn/T/ipykernel_8124/3841941305.py\", line 61, in <lambda>\nValueError: invalid literal for int() with base 10: '101210000\\tCABALLOS VIVOS REPRODUCTORES DE RAZA PUR'\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:555)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:713)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:695)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:508)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1211)\n\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1217)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2454)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2403)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2402)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2402)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1160)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1160)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1160)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2642)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2584)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2573)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:938)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2214)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2235)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2254)\n\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:166)\n\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\n\tat jdk.internal.reflect.GeneratedMethodAccessor92.invoke(Unknown Source)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/opt/homebrew/Cellar/apache-spark/3.2.1/libexec/python/lib/pyspark.zip/pyspark/worker.py\", line 619, in main\n    process()\n  File \"/opt/homebrew/Cellar/apache-spark/3.2.1/libexec/python/lib/pyspark.zip/pyspark/worker.py\", line 609, in process\n    out_iter = func(split_index, iterator)\n  File \"/opt/homebrew/Cellar/apache-spark/3.2.1/libexec/python/pyspark/rdd.py\", line 2918, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/opt/homebrew/Cellar/apache-spark/3.2.1/libexec/python/pyspark/rdd.py\", line 2918, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/opt/homebrew/Cellar/apache-spark/3.2.1/libexec/python/pyspark/rdd.py\", line 417, in func\n    return f(iterator)\n  File \"/opt/homebrew/Cellar/apache-spark/3.2.1/libexec/python/pyspark/rdd.py\", line 2146, in combineLocally\n    merger.mergeValues(iterator)\n  File \"/opt/homebrew/Cellar/apache-spark/3.2.1/libexec/python/lib/pyspark.zip/pyspark/shuffle.py\", line 240, in mergeValues\n    for k, v in iterator:\n  File \"/opt/homebrew/Cellar/apache-spark/3.2.1/libexec/python/lib/pyspark.zip/pyspark/util.py\", line 74, in wrapper\n    return f(*args, **kwargs)\n  File \"/var/folders/s_/9g6p7mn936j1qzv2gp75jsz00000gn/T/ipykernel_8124/3841941305.py\", line 61, in <lambda>\nValueError: invalid literal for int() with base 10: '101210000\\tCABALLOS VIVOS REPRODUCTORES DE RAZA PUR'\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:555)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:713)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:695)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:508)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1211)\n\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1217)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/s_/9g6p7mn936j1qzv2gp75jsz00000gn/T/ipykernel_8124/2733819333.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0mpath_to_metadata_peru\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/Users/pietro/Desktop/BDM/Project/Data.nosync/peru/metadata_copia/'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m \u001b[0mgeneral_main\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"peru\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"IMP\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmerging_peru\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcombining_peru\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchanging_peru\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath_to_avro_peru\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath_to_metadata_peru\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0;31m#main_peru()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/s_/9g6p7mn936j1qzv2gp75jsz00000gn/T/ipykernel_8124/213744749.py\u001b[0m in \u001b[0;36mgeneral_main\u001b[0;34m(country, data_type, merging, combining, changing, path_to_avro, path_to_metadata)\u001b[0m\n\u001b[1;32m     35\u001b[0m             \u001b[0mrdds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmerging_data_meta\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrdds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeta_file_name\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey_pos_in_meta\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname_pos_in_meta\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey_name_in_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeep_code\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_col_name\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey_as_int\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath_to_metadata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"brasil\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m             \u001b[0miteration_merging\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miteration_merging\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrdds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/homebrew/Cellar/apache-spark/3.2.1/libexec/python/pyspark/rdd.py\u001b[0m in \u001b[0;36mtake\u001b[0;34m(self, num)\u001b[0m\n\u001b[1;32m   1566\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1567\u001b[0m             \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartsScanned\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartsScanned\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnumPartsToTry\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotalParts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1568\u001b[0;31m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunJob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtakeUpToNumLeft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1569\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1570\u001b[0m             \u001b[0mitems\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/homebrew/Cellar/apache-spark/3.2.1/libexec/python/pyspark/context.py\u001b[0m in \u001b[0;36mrunJob\u001b[0;34m(self, rdd, partitionFunc, partitions, allowLocal)\u001b[0m\n\u001b[1;32m   1225\u001b[0m         \u001b[0;31m# SparkContext#runJob.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1226\u001b[0m         \u001b[0mmappedRDD\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartitionFunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1227\u001b[0;31m         \u001b[0msock_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunJob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmappedRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpartitions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1228\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msock_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmappedRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1229\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/homebrew/Cellar/apache-spark/3.2.1/libexec/python/lib/py4j-0.10.9.3-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1321\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1322\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/homebrew/Cellar/apache-spark/3.2.1/libexec/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/homebrew/Cellar/apache-spark/3.2.1/libexec/python/lib/py4j-0.10.9.3-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m                 raise Py4JJavaError(\n\u001b[0m\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 131.0 failed 1 times, most recent failure: Lost task 0.0 in stage 131.0 (TID 305) (10-192-220-80client.eduroam.upc.edu executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/opt/homebrew/Cellar/apache-spark/3.2.1/libexec/python/lib/pyspark.zip/pyspark/worker.py\", line 619, in main\n    process()\n  File \"/opt/homebrew/Cellar/apache-spark/3.2.1/libexec/python/lib/pyspark.zip/pyspark/worker.py\", line 609, in process\n    out_iter = func(split_index, iterator)\n  File \"/opt/homebrew/Cellar/apache-spark/3.2.1/libexec/python/pyspark/rdd.py\", line 2918, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/opt/homebrew/Cellar/apache-spark/3.2.1/libexec/python/pyspark/rdd.py\", line 2918, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/opt/homebrew/Cellar/apache-spark/3.2.1/libexec/python/pyspark/rdd.py\", line 417, in func\n    return f(iterator)\n  File \"/opt/homebrew/Cellar/apache-spark/3.2.1/libexec/python/pyspark/rdd.py\", line 2146, in combineLocally\n    merger.mergeValues(iterator)\n  File \"/opt/homebrew/Cellar/apache-spark/3.2.1/libexec/python/lib/pyspark.zip/pyspark/shuffle.py\", line 240, in mergeValues\n    for k, v in iterator:\n  File \"/opt/homebrew/Cellar/apache-spark/3.2.1/libexec/python/lib/pyspark.zip/pyspark/util.py\", line 74, in wrapper\n    return f(*args, **kwargs)\n  File \"/var/folders/s_/9g6p7mn936j1qzv2gp75jsz00000gn/T/ipykernel_8124/3841941305.py\", line 61, in <lambda>\nValueError: invalid literal for int() with base 10: '101210000\\tCABALLOS VIVOS REPRODUCTORES DE RAZA PUR'\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:555)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:713)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:695)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:508)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1211)\n\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1217)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2454)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2403)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2402)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2402)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1160)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1160)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1160)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2642)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2584)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2573)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:938)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2214)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2235)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2254)\n\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:166)\n\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\n\tat jdk.internal.reflect.GeneratedMethodAccessor92.invoke(Unknown Source)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/opt/homebrew/Cellar/apache-spark/3.2.1/libexec/python/lib/pyspark.zip/pyspark/worker.py\", line 619, in main\n    process()\n  File \"/opt/homebrew/Cellar/apache-spark/3.2.1/libexec/python/lib/pyspark.zip/pyspark/worker.py\", line 609, in process\n    out_iter = func(split_index, iterator)\n  File \"/opt/homebrew/Cellar/apache-spark/3.2.1/libexec/python/pyspark/rdd.py\", line 2918, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/opt/homebrew/Cellar/apache-spark/3.2.1/libexec/python/pyspark/rdd.py\", line 2918, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/opt/homebrew/Cellar/apache-spark/3.2.1/libexec/python/pyspark/rdd.py\", line 417, in func\n    return f(iterator)\n  File \"/opt/homebrew/Cellar/apache-spark/3.2.1/libexec/python/pyspark/rdd.py\", line 2146, in combineLocally\n    merger.mergeValues(iterator)\n  File \"/opt/homebrew/Cellar/apache-spark/3.2.1/libexec/python/lib/pyspark.zip/pyspark/shuffle.py\", line 240, in mergeValues\n    for k, v in iterator:\n  File \"/opt/homebrew/Cellar/apache-spark/3.2.1/libexec/python/lib/pyspark.zip/pyspark/util.py\", line 74, in wrapper\n    return f(*args, **kwargs)\n  File \"/var/folders/s_/9g6p7mn936j1qzv2gp75jsz00000gn/T/ipykernel_8124/3841941305.py\", line 61, in <lambda>\nValueError: invalid literal for int() with base 10: '101210000\\tCABALLOS VIVOS REPRODUCTORES DE RAZA PUR'\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:555)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:713)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:695)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:508)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1211)\n\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1217)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "def main_peru(path_to_avro=True, path_to_metadata=True):\n",
    "    \n",
    "    if path_to_avro:\n",
    "        path_to_avro = \"/Users/pietro/Desktop/BDM/Project/DataImporta/P2/development/test_data/version0.avro\"\n",
    "    if path_to_metadata:\n",
    "        path_to_metadata = '/Users/pietro/Desktop/BDM/Project/Data.nosync/peru/metadata_copia/'\n",
    "    df = spark.read.format('avro').load(path_to_avro)\n",
    "    rdd=df.rdd\n",
    "    \n",
    "    \n",
    "    # MERGING DATA AND METADATA\n",
    "    meta_file_name = [\"Partidas.txt\", \"Paises.txt\", \"Paises.txt\", \"Puertos.txt\", \"MedTransporte.txt\", \"Agente.txt\", \"Bancos.txt\", \"RecintAduaner.txt\", \"EstMercancia.txt\"]\n",
    "    key_pos_in_meta =  [0,0,0,[0,2],0,0,0,0,0]\n",
    "    name_pos_in_meta = [1,1,1,  3,  1,1,1,1,1] # position of the columns in meta that contains the name we want to keep \n",
    "    key_name_in_data = [\"PART_NANDI\", \"PAIS_ORIGE\", \"PAIS_ADQUI\", \"PUER_EMBAR\", \"VIA_TRANSP\", \"CODI_AGENT\", \"BANC_CANCE\", \"CODI_ALMA\", \"SEST_MERCA\"]\n",
    "    keep_code = [True, False, False, False, True, False, False, False, False] # true if we want to keep the code of the variable in the final data\n",
    "    new_col_name = [\"custom_description\", \"country_of_origin\", \"country_of_arrival\", \"port_of_boarding\", \"mean_of_transport\", \"agente\", \"bank\", \"warehouse\", \"state\"]\n",
    "    key_as_int = [True, False, False, False, True, True, True, False, False ]\n",
    "    \n",
    "\n",
    "        # [source_col_names, dest_col_name, string, operation]\n",
    "        combining_peru = [ [[\"FOB_DOLPOL\",  \"FLE_DOLAR\"], \"price_transport_net\", False, \"+\"],      \n",
    "                          [[\"price_transport_net\",  \"SEG_DOLAR\"], \"price_transport_net_insurance\", False, \"+\"], \n",
    "                          [[\"FOB_DOLPOL\", \"UNID_FIQTY\"], \"net_price_per_unit\", False, \"/\"],   \n",
    "                          [[\"DESC_COMER\", \"DESC_MATCO\", \"DESC_USOAP\", \"DESC_FOPRE\", \"DESC_OTROS\"], \"commercial_description\", True, \"+\"]]\n",
    "        #[old_names, new_names]\n",
    "        changing_peru = [[\"CODI_ADUAN\",\"custom\"],\n",
    "                        [\"FECH_RECEP\", \"date\"],\n",
    "                        [\"FOB_DOLPOL\", \"net_price\"]]\n",
    "        \n",
    "        \n",
    "        \n",
    "    rdds = []\n",
    "    rdds.append(rdd)\n",
    "    n_merging = len(key_pos_in_meta)\n",
    "    for i in range(0,n_merging):\n",
    "        rdds.append(merging_data_meta(rdds[i], meta_file_name[i], key_pos_in_meta[i], name_pos_in_meta[i], key_name_in_data[i], keep_code[i], new_col_name[i], key_as_int[i], path_to_metadata, \"peru\")) \n",
    "        #rdds.append(rdd)\n",
    "        \n",
    "    # COMBINING COLUMNS\n",
    "    source_col_names = [[\"FOB_DOLPOL\",  \"FLE_DOLAR\"],[\"price_transport_net\",  \"SEG_DOLAR\"], [\"FOB_DOLPOL\", \"UNID_FIQTY\"], [\"DESC_COMER\", \"DESC_MATCO\", \"DESC_USOAP\", \"DESC_FOPRE\", \"DESC_OTROS\"]]\n",
    "    dest_col_name = [    \"price_transport_net\",     \"price_transport_net_insurance\",     \"net_price_per_unit\",          \"commercial_description\"]\n",
    "    string = [False, False, False, True]\n",
    "    operation = [\"+\", \"+\", \"/\", \"+\"]\n",
    "    \n",
    "    print(\"inizio il secondo\")\n",
    "    \n",
    "    for i in range(0, len(dest_col_name)):\n",
    "        rdds.append(create_composed_columns(rdds[n_merging + i], source_col_names[i], dest_col_name[i], string[i], operation[i])) \n",
    "    \n",
    "    # CHANGING NAMES\n",
    "    n_combining = len(source_col_names)\n",
    "    \n",
    "    old_names = [\"CODI_ADUAN\", \"FECH_RECEP\", \"FOB_DOLPOL\"]\n",
    "    new_names = [\"custom\", \"date\", \"net_price\"]\n",
    "    rdds.append(rdds[n_combining + n_merging].map(lambda l: changeNames(l, old_names, new_names)))\n",
    "    \n",
    "    \n",
    "    # SELECT THE COLUMNS TO BE KEPT\n",
    "    to_be_kept = ['country_of_arrival', 'mean_of_transport', 'price_transport_net', 'price_transport_net_insurance', 'net_price_per_unit', 'commercial_description', 'custom', 'date', 'net_price']\n",
    "    rdds.append(rdds[n_combining + n_merging + 1].map(lambda l: keepOnly(l, to_be_kept)))\n",
    "\n",
    "    # ADDING COUNTRY NAME and \"IMP\" label\n",
    "    rdds.append(rdds[n_combining + n_merging + 2].map(lambda l: insert(l, \"country\", \"peru\")).map(lambda l: insert(l, \"type\", \"IMP\")))\n",
    "    \n",
    "    print(rdds[n_combining + n_merging + 3].take(1), \"\\n\\n\\n\")\n",
    "    return(rdds[n_combining + n_merging + 3])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4ffa5985",
   "metadata": {},
   "outputs": [],
   "source": [
    "def general_main(country, data_type, merging, combining, changing, path_to_avro, path_to_metadata):\n",
    "    \n",
    "    if country not in [\"peru\", \"brazil\", \"chile\"]:\n",
    "        print(\"ERROR in COUNTRY NAME. It must be 'peru', 'chile' or 'brazil'\")\n",
    "        return -1\n",
    "    if data_type not in [\"IMP\", \"EXP\"]:\n",
    "        print(\"ERROR in DATA_TYPE . It must be 'IMP' or 'EXP'\")\n",
    "        return -1\n",
    "    \n",
    "    df = spark.read.format('avro').load(path_to_avro)\n",
    "    rdd=df.rdd\n",
    "    \n",
    "    print(rdd.take(1))\n",
    "    iteration_merging = 0\n",
    "    rdds = []\n",
    "    rdds.append(rdd)\n",
    "    \n",
    "    # MERGING DATA AND META\n",
    "    if len(merging)>0:\n",
    "        meta_file_name = [row[0] for row in merging]\n",
    "        key_pos_in_meta = [row[1] for row in merging]\n",
    "        name_pos_in_meta = [row[2] for row in merging]\n",
    "        key_name_in_data = [row[3] for row in merging]\n",
    "        keep_code = [row[4] for row in merging]\n",
    "        new_col_name = [row[5] for row in merging]\n",
    "        key_as_int = [row[6] for row in merging]\n",
    "        #print(\"meta_file_name\", meta_file_name)\n",
    "        #print(\"key_pos_in_meta\", key_pos_in_meta)\n",
    "        #print(\"name_pos_in_meta\", name_pos_in_meta)\n",
    "        #print(\"key_name_in_data\", key_name_in_data)\n",
    "        #print(\"keep_code\", keep_code)\n",
    "        #print(\"new_col_name\", new_col_name)\n",
    "        #print(\"key_as_int\", key_as_int)\n",
    "\n",
    "        for i in range(0,len(key_pos_in_meta)):\n",
    "            rdds.append(merging_data_meta(rdds[i], meta_file_name[i], key_pos_in_meta[i], name_pos_in_meta[i], key_name_in_data[i], keep_code[i], new_col_name[i], key_as_int[i], path_to_metadata, \"brasil\")) \n",
    "            iteration_merging = iteration_merging + 1\n",
    "            #print(rdds[i].take(1))\n",
    "\n",
    "\n",
    "    # COMBINING COLUMNS\n",
    "    iteration_combining = 0\n",
    "    if len(combining)>0:\n",
    "        source_col_names = [row[0] for row in combining]\n",
    "        dest_col_name = [row[1] for row in combining]\n",
    "        string = [row[2] for row in combining]\n",
    "        operation = [row[3] for row in combining]\n",
    "\n",
    "        for i in range(0, len(dest_col_name)):\n",
    "            rdds.append(create_composed_columns(rdds[iteration_merging + i], source_col_names[i], dest_col_name[i], string[i], operation[i])) \n",
    "            iteration_combining = iteration_combining + 1\n",
    "\n",
    "    # CHANGING NAMES\n",
    "    n_combining = len(source_col_names)\n",
    "    \n",
    "    old_names = [row[0] for row in changing]\n",
    "    new_names = [row[1] for row in changing]\n",
    "    rdds.append(rdds[iteration_merging + iteration_combining].map(lambda l: changeNames(l, old_names, new_names)))\n",
    "    \n",
    "    # SELECT THE COLUMNS TO BE KEPT\n",
    "    to_be_kept = ['country_of_arrival', 'mean_of_transport', 'price_transport_net', 'price_transport_net_insurance', 'net_price_per_unit', 'commercial_description', 'custom', 'date', 'net_price']\n",
    "    rdds.append(rdds[iteration_merging + iteration_combining + 1].map(lambda l: keepOnly(l, to_be_kept)))\n",
    "    \n",
    "    # ADDING COUNTRY NAME and \"IMP\" label\n",
    "    rdds.append(rdds[iteration_merging + iteration_combining + 2].map(lambda l: insert(l, \"country\", country)).map(lambda l: insert(l, \"type\", data_type)))\n",
    "    print(rdds[iteration_merging + iteration_combining + 3].take(1))\n",
    "    return(rdds[iteration_merging + iteration_combining + 3])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "89abf753",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(CODI_ADUAN='172', ANO_PRESE='22', NUME_CORRE='2418', FECH_INGSI='20220210', TIPO_DOCUM='4', LIBR_TRIBU='20602049451', DNOMBRE='\"IMPORTACIONES MUSSA EMPRESA INDIVIDUAL', CODI_AGENT='7521', FECH_LLEGA='0', VIA_TRANSP='7', EMPR_TRANS='nan', CODI_ALMA='9998', CADU_MANIF='172', FECH_MANIF='2022', NUME_MANIF='0', FECH_RECEP='20220210', FECH_CANCE='20220210', TIPO_CANCE='33', BANC_CANCE='172.0', CODI_ENFIN='nan', DK='T', PAIS_ORIGE='CL', PAIS_ADQUI='CL', PUER_EMBAR='CLARI', FECH_EMBAR='20220210', NUME_SERIE='2', PART_NANDI='808100000', DESC_COMER='MANZANAS FRESCAS,SIN MARCA,SIN MODELO,', DESC_MATCO='VARIEDAD RED DELICIOUS CON 20 KN', DESC_USOAP='PARA CONSUMO', DESC_FOPRE='KILOGRAMOS CAJAS', DESC_OTROS='nan', FOB_DOLPOL='2352.0', FLE_DOLAR='42.14', SEG_DOLAR='58.8', PESO_NETO='3920.0', PESO_BRUTO='4214.0', UNID_FIQTY='3920.0', UNID_FIDES='KG', QUNICOM='196.0', TUNICOM='CAJ', SEST_MERCA='10', ADV_DOLAR='0.0', IGV_DOLAR='0.0', ISC_DOLAR='0.0', IPM_DOLAR='0.0', DES_DOLAR='0.0', IPA_DOLAR='0.0', SAD_DOLAR='0.0', DER_ADUM='0.0', COMM='0.0', FMOD='20220210', CANT_BULTO='196.0', CLASE='CAJ', TRAT_PREFE='0', TIPO_TRAT='1', CODI_LIBER='0', IMPR_RELIQ='nan')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/06/10 17:50:43 ERROR Executor: Exception in task 0.0 in stage 238.0 (TID 411)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/opt/homebrew/Cellar/apache-spark/3.2.1/libexec/python/lib/pyspark.zip/pyspark/worker.py\", line 619, in main\n",
      "    process()\n",
      "  File \"/opt/homebrew/Cellar/apache-spark/3.2.1/libexec/python/lib/pyspark.zip/pyspark/worker.py\", line 609, in process\n",
      "    out_iter = func(split_index, iterator)\n",
      "  File \"/opt/homebrew/Cellar/apache-spark/3.2.1/libexec/python/pyspark/rdd.py\", line 2918, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "  File \"/opt/homebrew/Cellar/apache-spark/3.2.1/libexec/python/pyspark/rdd.py\", line 2918, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "  File \"/opt/homebrew/Cellar/apache-spark/3.2.1/libexec/python/pyspark/rdd.py\", line 417, in func\n",
      "    return f(iterator)\n",
      "  File \"/opt/homebrew/Cellar/apache-spark/3.2.1/libexec/python/pyspark/rdd.py\", line 2146, in combineLocally\n",
      "    merger.mergeValues(iterator)\n",
      "  File \"/opt/homebrew/Cellar/apache-spark/3.2.1/libexec/python/lib/pyspark.zip/pyspark/shuffle.py\", line 240, in mergeValues\n",
      "    for k, v in iterator:\n",
      "  File \"/opt/homebrew/Cellar/apache-spark/3.2.1/libexec/python/lib/pyspark.zip/pyspark/util.py\", line 74, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/var/folders/s_/9g6p7mn936j1qzv2gp75jsz00000gn/T/ipykernel_8124/3841941305.py\", line 63, in <lambda>\n",
      "IndexError: tuple index out of range\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:555)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:713)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:695)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:508)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1211)\n",
      "\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1217)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "22/06/10 17:50:43 WARN TaskSetManager: Lost task 0.0 in stage 238.0 (TID 411) (10-192-220-80client.eduroam.upc.edu executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/opt/homebrew/Cellar/apache-spark/3.2.1/libexec/python/lib/pyspark.zip/pyspark/worker.py\", line 619, in main\n",
      "    process()\n",
      "  File \"/opt/homebrew/Cellar/apache-spark/3.2.1/libexec/python/lib/pyspark.zip/pyspark/worker.py\", line 609, in process\n",
      "    out_iter = func(split_index, iterator)\n",
      "  File \"/opt/homebrew/Cellar/apache-spark/3.2.1/libexec/python/pyspark/rdd.py\", line 2918, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "  File \"/opt/homebrew/Cellar/apache-spark/3.2.1/libexec/python/pyspark/rdd.py\", line 2918, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "  File \"/opt/homebrew/Cellar/apache-spark/3.2.1/libexec/python/pyspark/rdd.py\", line 417, in func\n",
      "    return f(iterator)\n",
      "  File \"/opt/homebrew/Cellar/apache-spark/3.2.1/libexec/python/pyspark/rdd.py\", line 2146, in combineLocally\n",
      "    merger.mergeValues(iterator)\n",
      "  File \"/opt/homebrew/Cellar/apache-spark/3.2.1/libexec/python/lib/pyspark.zip/pyspark/shuffle.py\", line 240, in mergeValues\n",
      "    for k, v in iterator:\n",
      "  File \"/opt/homebrew/Cellar/apache-spark/3.2.1/libexec/python/lib/pyspark.zip/pyspark/util.py\", line 74, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/var/folders/s_/9g6p7mn936j1qzv2gp75jsz00000gn/T/ipykernel_8124/3841941305.py\", line 63, in <lambda>\n",
      "IndexError: tuple index out of range\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:555)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:713)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:695)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:508)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1211)\n",
      "\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1217)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n",
      "22/06/10 17:50:43 ERROR TaskSetManager: Task 0 in stage 238.0 failed 1 times; aborting job\n",
      "22/06/10 17:50:43 ERROR Executor: Exception in task 1.0 in stage 232.0 (TID 416)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/opt/homebrew/Cellar/apache-spark/3.2.1/libexec/python/lib/pyspark.zip/pyspark/worker.py\", line 619, in main\n",
      "    process()\n",
      "  File \"/opt/homebrew/Cellar/apache-spark/3.2.1/libexec/python/lib/pyspark.zip/pyspark/worker.py\", line 609, in process\n",
      "    out_iter = func(split_index, iterator)\n",
      "  File \"/opt/homebrew/Cellar/apache-spark/3.2.1/libexec/python/pyspark/rdd.py\", line 2918, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "  File \"/opt/homebrew/Cellar/apache-spark/3.2.1/libexec/python/pyspark/rdd.py\", line 2918, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "  File \"/opt/homebrew/Cellar/apache-spark/3.2.1/libexec/python/pyspark/rdd.py\", line 417, in func\n",
      "    return f(iterator)\n",
      "  File \"/opt/homebrew/Cellar/apache-spark/3.2.1/libexec/python/pyspark/rdd.py\", line 2146, in combineLocally\n",
      "    merger.mergeValues(iterator)\n",
      "  File \"/opt/homebrew/Cellar/apache-spark/3.2.1/libexec/python/lib/pyspark.zip/pyspark/shuffle.py\", line 240, in mergeValues\n",
      "    for k, v in iterator:\n",
      "  File \"/opt/homebrew/Cellar/apache-spark/3.2.1/libexec/python/lib/pyspark.zip/pyspark/util.py\", line 74, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/var/folders/s_/9g6p7mn936j1qzv2gp75jsz00000gn/T/ipykernel_8124/3841941305.py\", line 61, in <lambda>\n",
      "ValueError: invalid literal for int() with base 10: '627   \\tAEROPERU EMP.DE TRANSP.AEREO.- M.T.C.                                 \\tLIMA METROPOLITANA                      \\tSUSPENDID'\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:555)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:713)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:695)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:508)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1211)\n",
      "\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1217)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "22/06/10 17:50:43 WARN TaskSetManager: Lost task 1.0 in stage 232.0 (TID 416) (10-192-220-80client.eduroam.upc.edu executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/opt/homebrew/Cellar/apache-spark/3.2.1/libexec/python/lib/pyspark.zip/pyspark/worker.py\", line 619, in main\n",
      "    process()\n",
      "  File \"/opt/homebrew/Cellar/apache-spark/3.2.1/libexec/python/lib/pyspark.zip/pyspark/worker.py\", line 609, in process\n",
      "    out_iter = func(split_index, iterator)\n",
      "  File \"/opt/homebrew/Cellar/apache-spark/3.2.1/libexec/python/pyspark/rdd.py\", line 2918, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "  File \"/opt/homebrew/Cellar/apache-spark/3.2.1/libexec/python/pyspark/rdd.py\", line 2918, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "  File \"/opt/homebrew/Cellar/apache-spark/3.2.1/libexec/python/pyspark/rdd.py\", line 417, in func\n",
      "    return f(iterator)\n",
      "  File \"/opt/homebrew/Cellar/apache-spark/3.2.1/libexec/python/pyspark/rdd.py\", line 2146, in combineLocally\n",
      "    merger.mergeValues(iterator)\n",
      "  File \"/opt/homebrew/Cellar/apache-spark/3.2.1/libexec/python/lib/pyspark.zip/pyspark/shuffle.py\", line 240, in mergeValues\n",
      "    for k, v in iterator:\n",
      "  File \"/opt/homebrew/Cellar/apache-spark/3.2.1/libexec/python/lib/pyspark.zip/pyspark/util.py\", line 74, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/var/folders/s_/9g6p7mn936j1qzv2gp75jsz00000gn/T/ipykernel_8124/3841941305.py\", line 61, in <lambda>\n",
      "ValueError: invalid literal for int() with base 10: '627   \\tAEROPERU EMP.DE TRANSP.AEREO.- M.T.C.                                 \\tLIMA METROPOLITANA                      \\tSUSPENDID'\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:555)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:713)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:695)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:508)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1211)\n",
      "\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1217)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n",
      "22/06/10 17:50:43 ERROR TaskSetManager: Task 1 in stage 232.0 failed 1 times; aborting job\n",
      "22/06/10 17:50:43 ERROR Executor: Exception in task 0.0 in stage 232.0 (TID 415)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/opt/homebrew/Cellar/apache-spark/3.2.1/libexec/python/lib/pyspark.zip/pyspark/worker.py\", line 619, in main\n",
      "    process()\n",
      "  File \"/opt/homebrew/Cellar/apache-spark/3.2.1/libexec/python/lib/pyspark.zip/pyspark/worker.py\", line 609, in process\n",
      "    out_iter = func(split_index, iterator)\n",
      "  File \"/opt/homebrew/Cellar/apache-spark/3.2.1/libexec/python/pyspark/rdd.py\", line 2918, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "  File \"/opt/homebrew/Cellar/apache-spark/3.2.1/libexec/python/pyspark/rdd.py\", line 2918, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "  File \"/opt/homebrew/Cellar/apache-spark/3.2.1/libexec/python/pyspark/rdd.py\", line 417, in func\n",
      "    return f(iterator)\n",
      "  File \"/opt/homebrew/Cellar/apache-spark/3.2.1/libexec/python/pyspark/rdd.py\", line 2146, in combineLocally\n",
      "    merger.mergeValues(iterator)\n",
      "  File \"/opt/homebrew/Cellar/apache-spark/3.2.1/libexec/python/lib/pyspark.zip/pyspark/shuffle.py\", line 240, in mergeValues\n",
      "    for k, v in iterator:\n",
      "  File \"/opt/homebrew/Cellar/apache-spark/3.2.1/libexec/python/lib/pyspark.zip/pyspark/util.py\", line 74, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/var/folders/s_/9g6p7mn936j1qzv2gp75jsz00000gn/T/ipykernel_8124/3841941305.py\", line 61, in <lambda>\n",
      "ValueError: invalid literal for int() with base 10: 'odigo  Agencia                                                                 Jurisdiccion                                Estad'\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:555)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:713)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:695)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:508)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1211)\n",
      "\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1217)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "22/06/10 17:50:43 WARN TaskSetManager: Lost task 0.0 in stage 232.0 (TID 415) (10-192-220-80client.eduroam.upc.edu executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/opt/homebrew/Cellar/apache-spark/3.2.1/libexec/python/lib/pyspark.zip/pyspark/worker.py\", line 619, in main\n",
      "    process()\n",
      "  File \"/opt/homebrew/Cellar/apache-spark/3.2.1/libexec/python/lib/pyspark.zip/pyspark/worker.py\", line 609, in process\n",
      "    out_iter = func(split_index, iterator)\n",
      "  File \"/opt/homebrew/Cellar/apache-spark/3.2.1/libexec/python/pyspark/rdd.py\", line 2918, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "  File \"/opt/homebrew/Cellar/apache-spark/3.2.1/libexec/python/pyspark/rdd.py\", line 2918, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "  File \"/opt/homebrew/Cellar/apache-spark/3.2.1/libexec/python/pyspark/rdd.py\", line 417, in func\n",
      "    return f(iterator)\n",
      "  File \"/opt/homebrew/Cellar/apache-spark/3.2.1/libexec/python/pyspark/rdd.py\", line 2146, in combineLocally\n",
      "    merger.mergeValues(iterator)\n",
      "  File \"/opt/homebrew/Cellar/apache-spark/3.2.1/libexec/python/lib/pyspark.zip/pyspark/shuffle.py\", line 240, in mergeValues\n",
      "    for k, v in iterator:\n",
      "  File \"/opt/homebrew/Cellar/apache-spark/3.2.1/libexec/python/lib/pyspark.zip/pyspark/util.py\", line 74, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/var/folders/s_/9g6p7mn936j1qzv2gp75jsz00000gn/T/ipykernel_8124/3841941305.py\", line 61, in <lambda>\n",
      "ValueError: invalid literal for int() with base 10: 'odigo  Agencia                                                                 Jurisdiccion                                Estad'\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:555)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:713)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:695)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:508)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1211)\n",
      "\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1217)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n",
      "22/06/10 17:50:43 WARN TaskSetManager: Lost task 1.0 in stage 235.0 (TID 414) (10-192-220-80client.eduroam.upc.edu executor driver): TaskKilled (Stage cancelled)\n",
      "22/06/10 17:50:43 WARN TaskSetManager: Lost task 0.0 in stage 235.0 (TID 413) (10-192-220-80client.eduroam.upc.edu executor driver): TaskKilled (Stage cancelled)\n",
      "22/06/10 17:50:43 WARN TaskSetManager: Lost task 1.0 in stage 238.0 (TID 412) (10-192-220-80client.eduroam.upc.edu executor driver): TaskKilled (Stage cancelled)\n",
      "22/06/10 17:50:43 WARN TaskSetManager: Lost task 0.0 in stage 218.0 (TID 421) (10-192-220-80client.eduroam.upc.edu executor driver): TaskKilled (Stage cancelled)\n",
      "22/06/10 17:50:43 WARN TaskSetManager: Lost task 0.0 in stage 220.0 (TID 420) (10-192-220-80client.eduroam.upc.edu executor driver): TaskKilled (Stage cancelled)\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 238.0 failed 1 times, most recent failure: Lost task 0.0 in stage 238.0 (TID 411) (10-192-220-80client.eduroam.upc.edu executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/opt/homebrew/Cellar/apache-spark/3.2.1/libexec/python/lib/pyspark.zip/pyspark/worker.py\", line 619, in main\n    process()\n  File \"/opt/homebrew/Cellar/apache-spark/3.2.1/libexec/python/lib/pyspark.zip/pyspark/worker.py\", line 609, in process\n    out_iter = func(split_index, iterator)\n  File \"/opt/homebrew/Cellar/apache-spark/3.2.1/libexec/python/pyspark/rdd.py\", line 2918, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/opt/homebrew/Cellar/apache-spark/3.2.1/libexec/python/pyspark/rdd.py\", line 2918, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/opt/homebrew/Cellar/apache-spark/3.2.1/libexec/python/pyspark/rdd.py\", line 417, in func\n    return f(iterator)\n  File \"/opt/homebrew/Cellar/apache-spark/3.2.1/libexec/python/pyspark/rdd.py\", line 2146, in combineLocally\n    merger.mergeValues(iterator)\n  File \"/opt/homebrew/Cellar/apache-spark/3.2.1/libexec/python/lib/pyspark.zip/pyspark/shuffle.py\", line 240, in mergeValues\n    for k, v in iterator:\n  File \"/opt/homebrew/Cellar/apache-spark/3.2.1/libexec/python/lib/pyspark.zip/pyspark/util.py\", line 74, in wrapper\n    return f(*args, **kwargs)\n  File \"/var/folders/s_/9g6p7mn936j1qzv2gp75jsz00000gn/T/ipykernel_8124/3841941305.py\", line 63, in <lambda>\nIndexError: tuple index out of range\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:555)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:713)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:695)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:508)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1211)\n\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1217)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2454)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2403)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2402)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2402)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1160)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1160)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1160)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2642)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2584)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2573)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:938)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2214)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2235)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2254)\n\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:166)\n\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\n\tat jdk.internal.reflect.GeneratedMethodAccessor92.invoke(Unknown Source)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/opt/homebrew/Cellar/apache-spark/3.2.1/libexec/python/lib/pyspark.zip/pyspark/worker.py\", line 619, in main\n    process()\n  File \"/opt/homebrew/Cellar/apache-spark/3.2.1/libexec/python/lib/pyspark.zip/pyspark/worker.py\", line 609, in process\n    out_iter = func(split_index, iterator)\n  File \"/opt/homebrew/Cellar/apache-spark/3.2.1/libexec/python/pyspark/rdd.py\", line 2918, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/opt/homebrew/Cellar/apache-spark/3.2.1/libexec/python/pyspark/rdd.py\", line 2918, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/opt/homebrew/Cellar/apache-spark/3.2.1/libexec/python/pyspark/rdd.py\", line 417, in func\n    return f(iterator)\n  File \"/opt/homebrew/Cellar/apache-spark/3.2.1/libexec/python/pyspark/rdd.py\", line 2146, in combineLocally\n    merger.mergeValues(iterator)\n  File \"/opt/homebrew/Cellar/apache-spark/3.2.1/libexec/python/lib/pyspark.zip/pyspark/shuffle.py\", line 240, in mergeValues\n    for k, v in iterator:\n  File \"/opt/homebrew/Cellar/apache-spark/3.2.1/libexec/python/lib/pyspark.zip/pyspark/util.py\", line 74, in wrapper\n    return f(*args, **kwargs)\n  File \"/var/folders/s_/9g6p7mn936j1qzv2gp75jsz00000gn/T/ipykernel_8124/3841941305.py\", line 63, in <lambda>\nIndexError: tuple index out of range\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:555)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:713)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:695)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:508)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1211)\n\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1217)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/s_/9g6p7mn936j1qzv2gp75jsz00000gn/T/ipykernel_8124/1133313226.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m \u001b[0muser_define_formatting\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"peru\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/s_/9g6p7mn936j1qzv2gp75jsz00000gn/T/ipykernel_8124/1133313226.py\u001b[0m in \u001b[0;36muser_define_formatting\u001b[0;34m(country)\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0mpath_to_metadata_peru\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/Users/pietro/Desktop/BDM/Project/Data.nosync/peru/metadata_copia/'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mgeneral_main\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"peru\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"IMP\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmerging_peru\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcombining_peru\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchanging_peru\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath_to_avro_peru\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath_to_metadata_peru\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/s_/9g6p7mn936j1qzv2gp75jsz00000gn/T/ipykernel_8124/3433784098.py\u001b[0m in \u001b[0;36mgeneral_main\u001b[0;34m(country, data_type, merging, combining, changing, path_to_avro, path_to_metadata)\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[0;31m# ADDING COUNTRY NAME and \"IMP\" label\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0mrdds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrdds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0miteration_merging\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0miteration_combining\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0minsert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"country\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcountry\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0minsert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"type\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrdds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0miteration_merging\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0miteration_combining\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m     \u001b[0;32mreturn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrdds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0miteration_merging\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0miteration_combining\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/homebrew/Cellar/apache-spark/3.2.1/libexec/python/pyspark/rdd.py\u001b[0m in \u001b[0;36mtake\u001b[0;34m(self, num)\u001b[0m\n\u001b[1;32m   1566\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1567\u001b[0m             \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartsScanned\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartsScanned\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnumPartsToTry\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotalParts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1568\u001b[0;31m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunJob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtakeUpToNumLeft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1569\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1570\u001b[0m             \u001b[0mitems\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/homebrew/Cellar/apache-spark/3.2.1/libexec/python/pyspark/context.py\u001b[0m in \u001b[0;36mrunJob\u001b[0;34m(self, rdd, partitionFunc, partitions, allowLocal)\u001b[0m\n\u001b[1;32m   1225\u001b[0m         \u001b[0;31m# SparkContext#runJob.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1226\u001b[0m         \u001b[0mmappedRDD\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartitionFunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1227\u001b[0;31m         \u001b[0msock_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunJob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmappedRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpartitions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1228\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msock_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmappedRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1229\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/homebrew/Cellar/apache-spark/3.2.1/libexec/python/lib/py4j-0.10.9.3-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1321\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1322\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/homebrew/Cellar/apache-spark/3.2.1/libexec/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/homebrew/Cellar/apache-spark/3.2.1/libexec/python/lib/py4j-0.10.9.3-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m                 raise Py4JJavaError(\n\u001b[0m\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 238.0 failed 1 times, most recent failure: Lost task 0.0 in stage 238.0 (TID 411) (10-192-220-80client.eduroam.upc.edu executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/opt/homebrew/Cellar/apache-spark/3.2.1/libexec/python/lib/pyspark.zip/pyspark/worker.py\", line 619, in main\n    process()\n  File \"/opt/homebrew/Cellar/apache-spark/3.2.1/libexec/python/lib/pyspark.zip/pyspark/worker.py\", line 609, in process\n    out_iter = func(split_index, iterator)\n  File \"/opt/homebrew/Cellar/apache-spark/3.2.1/libexec/python/pyspark/rdd.py\", line 2918, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/opt/homebrew/Cellar/apache-spark/3.2.1/libexec/python/pyspark/rdd.py\", line 2918, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/opt/homebrew/Cellar/apache-spark/3.2.1/libexec/python/pyspark/rdd.py\", line 417, in func\n    return f(iterator)\n  File \"/opt/homebrew/Cellar/apache-spark/3.2.1/libexec/python/pyspark/rdd.py\", line 2146, in combineLocally\n    merger.mergeValues(iterator)\n  File \"/opt/homebrew/Cellar/apache-spark/3.2.1/libexec/python/lib/pyspark.zip/pyspark/shuffle.py\", line 240, in mergeValues\n    for k, v in iterator:\n  File \"/opt/homebrew/Cellar/apache-spark/3.2.1/libexec/python/lib/pyspark.zip/pyspark/util.py\", line 74, in wrapper\n    return f(*args, **kwargs)\n  File \"/var/folders/s_/9g6p7mn936j1qzv2gp75jsz00000gn/T/ipykernel_8124/3841941305.py\", line 63, in <lambda>\nIndexError: tuple index out of range\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:555)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:713)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:695)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:508)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1211)\n\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1217)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2454)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2403)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2402)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2402)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1160)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1160)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1160)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2642)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2584)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2573)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:938)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2214)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2235)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2254)\n\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:166)\n\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\n\tat jdk.internal.reflect.GeneratedMethodAccessor92.invoke(Unknown Source)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/opt/homebrew/Cellar/apache-spark/3.2.1/libexec/python/lib/pyspark.zip/pyspark/worker.py\", line 619, in main\n    process()\n  File \"/opt/homebrew/Cellar/apache-spark/3.2.1/libexec/python/lib/pyspark.zip/pyspark/worker.py\", line 609, in process\n    out_iter = func(split_index, iterator)\n  File \"/opt/homebrew/Cellar/apache-spark/3.2.1/libexec/python/pyspark/rdd.py\", line 2918, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/opt/homebrew/Cellar/apache-spark/3.2.1/libexec/python/pyspark/rdd.py\", line 2918, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/opt/homebrew/Cellar/apache-spark/3.2.1/libexec/python/pyspark/rdd.py\", line 417, in func\n    return f(iterator)\n  File \"/opt/homebrew/Cellar/apache-spark/3.2.1/libexec/python/pyspark/rdd.py\", line 2146, in combineLocally\n    merger.mergeValues(iterator)\n  File \"/opt/homebrew/Cellar/apache-spark/3.2.1/libexec/python/lib/pyspark.zip/pyspark/shuffle.py\", line 240, in mergeValues\n    for k, v in iterator:\n  File \"/opt/homebrew/Cellar/apache-spark/3.2.1/libexec/python/lib/pyspark.zip/pyspark/util.py\", line 74, in wrapper\n    return f(*args, **kwargs)\n  File \"/var/folders/s_/9g6p7mn936j1qzv2gp75jsz00000gn/T/ipykernel_8124/3841941305.py\", line 63, in <lambda>\nIndexError: tuple index out of range\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:555)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:713)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:695)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:508)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1211)\n\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1217)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\t... 1 more\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/06/10 17:50:43 WARN TaskSetManager: Lost task 0.0 in stage 223.0 (TID 419) (10-192-220-80client.eduroam.upc.edu executor driver): TaskKilled (Stage cancelled)\n",
      "22/06/10 17:50:43 WARN TaskSetManager: Lost task 1.0 in stage 229.0 (TID 418) (10-192-220-80client.eduroam.upc.edu executor driver): TaskKilled (Stage cancelled)\n",
      "22/06/10 17:50:43 WARN TaskSetManager: Lost task 0.0 in stage 229.0 (TID 417) (10-192-220-80client.eduroam.upc.edu executor driver): TaskKilled (Stage cancelled)\n"
     ]
    }
   ],
   "source": [
    "def user_define_formatting(country):\n",
    "    \n",
    "    if country ==\"brazil\":\n",
    "        # BRASIL\n",
    "\n",
    "        # [meta_file_name, key_pos_in_meta, name_pos_in_meta, key_name_in_data, keep_code, new_col_name, key_as_int]\n",
    "        merging_brazil=[[\"NCM.csv\", 0, 1, \"CO_NCM\", False, \"custom_description\", True],\n",
    "                          [\"URF.csv\", 0, 1, \"CO_URF\", False, \"custom\", True],\n",
    "                          [\"PAIS.csv\", 0, 4, \"CO_PAIS\",False, \"country_of_arrival\", True],\n",
    "                          [\"VIA.csv\", 0, 1, \"CO_VIA\", False, \"mean_of_transport\", True],\n",
    "                          [\"UF.csv\", 1, 2, \"SG_UF_NCM\", False, \"arrival_place\", False]]\n",
    "        # [source_col_names, dest_col_name, string, operation]\n",
    "        combining_brazil = [ [[\"VL_FOB\", \"VL_FRETE\"], \"price_transport_net\", False, \"+\"],\n",
    "                           [[\"price_transport_net\",  \"VL_SEGURO\"], \"price_transport_net_insurance\", False, \"+\"],\n",
    "                           [[\"VL_FOB\", \"KG_LIQUIDO\"], \"net_price_per_unit\", False, \"/\"],\n",
    "                           [[\"CO_MES\", \"CO_ANO\"], \"date\", True, \"+\"]]\n",
    "        #[old_names, new_names]\n",
    "        changing_brazil = [[\"VL_FOB\",\"net_price\"],\n",
    "                           [\"custom_description\",\"commercial_description\" ]]\n",
    "        path_to_avro_brazil = \"/Users/pietro/Desktop/BDM/Project/DataImporta/P2/development/test_data/test_brazil.avro\"\n",
    "        path_to_metadata_brazil = '/Users/pietro/Desktop/BDM/Project/Data.nosync/brazil/metadata copia/'\n",
    "\n",
    "        return general_main(\"brazil\", \"IMP\", merging_brazil, combining_brazil, changing_brazil, path_to_avro_brazil, path_to_metadata_brazil)\n",
    "\n",
    "\n",
    "        \n",
    "    if country ==\"chile\":\n",
    "        # CHILE\n",
    "\n",
    "        merging_chile=[]\n",
    "        combining_chile = [ [[\"FOB\", \"FLETE\"],\"price_transport_net\", False, \"+\"],\n",
    "                            [[\"price_transport_net\",  \"SEGURO\"], \"price_transport_net_insurance\", False, \"+\"],\n",
    "                            [['DNOMBRE', 'DMARCA', 'DVARIEDAD', 'DOTRO1', 'DOTRO2', 'ATR_5',  'ATR_6'],\"commercial_description\", True, \"+\"] ]\n",
    "        #[old_names, new_names]\n",
    "        changing_chile = [[\"ADU\", \"custom\"],\n",
    "                           [\"FECVENCI\", \"date\"],\n",
    "                           [\"FOB\", \"net_price\"],\n",
    "                           [\"PRE_UNIT\", \"net_price_per_unit\"],\n",
    "                           [\"CODPAISCON\", \"country_of_arrival\"],\n",
    "                           [\"VIA_TRAN\", \"mean_of_transport\"]]\n",
    "\n",
    "        path_to_avro_chile = \"/Users/pietro/Desktop/BDM/Project/DataImporta/P2/development/test_data/test_chile.avro\"\n",
    "        path_to_metadata_chile= ''\n",
    "\n",
    "        return general_main(\"chile\", \"IMP\", merging_chile, combining_chile, changing_chile, path_to_avro_chile, path_to_metadata_chile)\n",
    "    \n",
    "    if country==\"peru\": # NOT WORKING!!!!!!\n",
    "\n",
    "        # [meta_file_name, key_pos_in_meta, name_pos_in_meta, key_name_in_data, keep_code, new_col_name, key_as_int]\n",
    "        merging_peru=[[\"Partidas.txt\", 0, 1, \"PART_NANDI\", True, \"custom_description\", True],\n",
    "                      [\"Paises.txt\", 0, 1,  \"PAIS_ORIGE\", False, \"country_of_origin\", False],\n",
    "                      [ \"Paises.txt\", 0, 1, \"PAIS_ADQUI\", False, \"country_of_arrival\", False],\n",
    "                      [\"Puertos.txt\", [0,2], 3, \"PUER_EMBAR\", False, \"port_of_boarding\", False],\n",
    "                      [\"MedTransporte.txt\", 0, 1, \"VIA_TRANSP\", True, \"mean_of_transport\", True],\n",
    "                      [\"Agente.txt\", 0, 1, \"CODI_AGENT\", False, \"agente\", True],\n",
    "                      [ \"Bancos.txt\", 0, 1, \"BANC_CANCE\", False, \"bank\", True],\n",
    "                      [\"RecintAduaner.txt\", 0, 1, \"CODI_ALMA\", False, \"warehouse\", False],\n",
    "                      [\"EstMercancia.txt\", 0, 1, \"SEST_MERCA\", False, \"state\", False]\n",
    "                     ]\n",
    "        # [source_col_names, dest_col_name, string, operation]\n",
    "        combining_peru = [ [[\"FOB_DOLPOL\",  \"FLE_DOLAR\"], \"price_transport_net\", False, \"+\"],      \n",
    "                          [[\"price_transport_net\",  \"SEG_DOLAR\"], \"price_transport_net_insurance\", False, \"+\"], \n",
    "                          [[\"FOB_DOLPOL\", \"UNID_FIQTY\"], \"net_price_per_unit\", False, \"/\"],   \n",
    "                          [[\"DESC_COMER\", \"DESC_MATCO\", \"DESC_USOAP\", \"DESC_FOPRE\", \"DESC_OTROS\"], \"commercial_description\", True, \"+\"]]\n",
    "        #[old_names, new_names]\n",
    "        changing_peru = [[\"CODI_ADUAN\",\"custom\"],\n",
    "                        [\"FECH_RECEP\", \"date\"],\n",
    "                        [\"FOB_DOLPOL\", \"net_price\"]]\n",
    "        path_to_avro_peru =  \"/Users/pietro/Desktop/BDM/Project/DataImporta/P2/development/test_data/version0.avro\"\n",
    "        path_to_metadata_peru = '/Users/pietro/Desktop/BDM/Project/Data.nosync/peru/metadata_copia/'\n",
    "        \n",
    "        return general_main(\"peru\", \"IMP\", merging_peru, combining_peru, changing_peru, path_to_avro_peru, path_to_metadata_peru)\n",
    "\n",
    "\n",
    "user_define_formatting(\"peru\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ee33f39d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inizio il secondo\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(country_of_arrival='UNITED STATES', mean_of_transport='AVION', price_transport_net='2294', price_transport_net_insurance='2316', net_price_per_unit='2199.9978000022', commercial_description='BOMBA DE COMBSUTIBLE, S/M, S/M EN UNIDAD/ INCLUYE ACCESORIOS PARA SU NORMAL FUNCIONAMIENTO PARA PROPORCIONAR COMBUSTIBLE AL MOTOR /ES PARA USO DE AERONAVE BOMBA DE COMBSUTIBLE METAL,P/N: 897400-7', custom='235', date='20220209', net_price='2200.0', country='peru', type='IMP')] \n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# LOAD TO THE DB\n",
    "def main():\n",
    "    data_chile=user_define_formatting(\"chile\")\n",
    "    chileDF = data_chile.toDF()\n",
    "\n",
    "    data_brazil=user_define_formatting(\"brazil\")\n",
    "    brazilDF = data_brazil.toDF()\n",
    "\n",
    "    data_peru=main_peru()\n",
    "    # data_peru = user_define_formatting(\"peru\")\n",
    "    peruDF = data_peru.toDF()\n",
    "\n",
    "    properties = {\"user\": \"pietro\", \"password\": \"password\", \"driver\": 'org.postgresql.Driver'}\n",
    "    url = \"jdbc:postgresql://localhost:5432/dataimporta\"\n",
    "\n",
    "    chileDF.write.format(\"jdbc\").mode(\"append\").jdbc(url,\"all_countries\",\n",
    "              properties = properties)\n",
    "    brazilDF.write.format(\"jdbc\").mode(\"append\").jdbc(url,\"all_countries\",\n",
    "              properties = properties)\n",
    "    peruDF.write.format(\"jdbc\").mode(\"append\").jdbc(url,\"all_countries\",\n",
    "              properties = properties)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.8.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
