{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **DATA FORMATTER**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this stage we will create **sandboxes** with integrated portions of the datasets. Each sandbox will help us to *answer specific business questions*. So, instead of having a Data Warehouse with all the data, we will have multiple small sandboxes which will provide specific and different answers. Since this data comes from different sources (a.k.a Peru, Chile and Brazil), we must consolidate it into a specific format. This script will be used for this purpose."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first check the data we have stored in HDFS persistent zone. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.getOrCreate()\n",
    "df = spark.read.format(\"org.apache.spark.sql.avro\").load(\"hdfs://localhost:9000/persistent/peru/imp/2022/version5.avro\")\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "fd040cd6d2f6c03fa7abba4128c8b33c22dfb22c58d45ceefc9f5658edabb3e8"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('bdm_project')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
