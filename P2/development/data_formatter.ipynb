{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "afa202d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Row\n",
    "import re\n",
    "\n",
    "spark = SparkSession.builder.config(\"spark.jars\", \"/Users/pietro-avro_2.12-3.1.3\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ab8ce7f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST THIS HAS TO BE DELETED!!!!!!!!!!!!!!\n",
    "test=spark.read.csv(\"hdfs://localhost:9000/persistent/chile/metadata/imp/2022/21463104042022/EXPheaders.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b8d9e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "hdfs_path = 'hdfs://localhost:9000/persistent/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a6d5bdd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "# How it works\n",
    "###\n",
    "# 1) metadata is opened\n",
    "# 2) metadata is formatted as (code, name to be kept in the data)\n",
    "# 3) data is formatted as (code for the join, all row)\n",
    "# the variables that only have to have different names with the same values are kept as they are\n",
    "# the metadata is treated as coming from txt files, not avros.\n",
    "# Avro are always opened as set of Row objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1e1fd1f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FUNCTIONS\n",
    "\n",
    "def delete(row, col_name):\n",
    "    d = row.asDict()\n",
    "    d.pop(col_name)\n",
    "    return Row(**d)\n",
    "\n",
    "# function add one key to a row\n",
    "def insert(row, new_col_name, value):\n",
    "    d = row.asDict()\n",
    "    d[new_col_name] = str(value).rstrip()\n",
    "    return Row(**d)\n",
    "    \n",
    "def lenght(val):\n",
    "    if isinstance(val, int):\n",
    "        return 1\n",
    "    else:\n",
    "        return len(val)\n",
    "\n",
    "# update one field in row\n",
    "def updateRow(row, field_to_update, value):\n",
    "    d = row.asDict()\n",
    "    d[field_to_update] = value \n",
    "    return Row(**d)\n",
    "\n",
    "def changeNames(row, old_names, new_names):\n",
    "    for i in range(0, len(old_names)):\n",
    "        row = insert(row, new_names[i], row[old_names[i]])\n",
    "        row = delete(row, old_names[i])\n",
    "    return row\n",
    "\n",
    "def keepOnly(row, fields_to_be_kept):\n",
    "    new_row = Row(var=True)\n",
    "    for field in fields_to_be_kept:\n",
    "        new_row = insert(new_row, field, row[field])\n",
    "    new_row = delete(new_row, \"var\")\n",
    "    return new_row  \n",
    "\n",
    "def merging_data_meta (rdd, meta_file_name, key_pos_in_meta, name_pos_in_meta, key_name_in_data, keep_code, new_col_name, key_as_int, path_to_metadata, country):\n",
    "    \n",
    "    # METADATA\n",
    "    if country == \"peru\":\n",
    "        # if the first element starts with \"C\" or \"t\", it has to be dropped (it is the header)\n",
    "        meta_split = sc.textFile(path_to_metadata + meta_file_name).filter(lambda l: not l.startswith(tuple([\"C\",\"t\"]))).map(lambda l: tuple(l.split(\"\\t\")))\n",
    "        #print(meta_split.take(4))\n",
    "        #print(meta_split.take(2)) \n",
    "    \n",
    "    elif country==\"brasil\":\n",
    "        if meta_file_name == \"NCM.csv\":\n",
    "            delim = \";\"\n",
    "            meta_split = sc.textFile(path_to_metadata + meta_file_name).filter(lambda l: not l.startswith(\"C\")).map(lambda l: tuple(tuple(l.split(\"\\n\"))[0].split(delim)))\n",
    "        else:\n",
    "            delim=\"\\\",\\\"\"\n",
    "            meta_split = sc.textFile(path_to_metadata + meta_file_name).filter(lambda l: not l.startswith(\"\\\"C\")).map(lambda l: tuple(tuple(l.split(\"\\n\"))[0][1:-1].split(delim)))\n",
    "    else:\n",
    "        print('--- ERROR: wrong country name --')\n",
    "    # prepare metadata for the join\n",
    "    if lenght(key_pos_in_meta) == 2: # the key is the combination of 2 columns\n",
    "        meta_ready_for_join0 = meta_split.map(lambda l: ( l[key_pos_in_meta[0]].strip() + l[key_pos_in_meta[1]].strip(), l[name_pos_in_meta].rstrip())).distinct()\n",
    "    elif (key_as_int):\n",
    "        meta_ready_for_join0 = meta_split.map(lambda l: ( int(l[key_pos_in_meta].strip()), l[name_pos_in_meta].rstrip())).distinct()\n",
    "    else:\n",
    "        meta_ready_for_join0 = meta_split.map(lambda l: ( l[key_pos_in_meta].strip(), l[name_pos_in_meta].rstrip())).distinct() \n",
    "    \n",
    "    # if there are repetiotions of the key with typos on the attr. name that should instead be always the same\n",
    "    meta_ready_for_join = meta_ready_for_join0.groupByKey().mapValues(list).map(lambda l: (l[0],l[1][0]))\n",
    "\n",
    "    # DATA\n",
    "    # prepare data for the join:\n",
    "    if (key_as_int):\n",
    "        # if there are nan, we set them to 9999999999\n",
    "        data_ready_for_join = rdd.map(lambda l: updateRow(l, key_name_in_data, \"9999999999\") if l[key_name_in_data]=='nan' else l).map(lambda l: (int( l[key_name_in_data].split(\".\")[0]) , l) )\n",
    "        # print(data_ready_for_join.take(5))\n",
    "    else:\n",
    "        data_ready_for_join = rdd.map(lambda l: ( l[key_name_in_data], l))\n",
    "   \n",
    "    # JOIN\n",
    "    join = data_ready_for_join.leftOuterJoin(meta_ready_for_join)\n",
    "    \n",
    "    if (not keep_code): # remove the column with the code\n",
    "        merged = join.map(lambda l:  (delete(l[1][0], key_name_in_data), l[1][1]) )\n",
    "        #print(merged.take(1))\n",
    "\n",
    "    else:\n",
    "        merged = join.map(lambda l:  (l[1][0], l[1][1]) )\n",
    "    \n",
    "    complete = merged.map(lambda l:  insert(l[0], new_col_name, l[1]) )\n",
    "    \n",
    "    return complete\n",
    "\n",
    "\n",
    "\n",
    "def create_composed_columns(rdd, source_col_names, dest_col_name, string = False, operation = \"+\"):\n",
    "    \n",
    "    # \"+\" (addition for numbers or concatenating strings)\n",
    "    # \"/\" (division for numberes)\n",
    "    \n",
    "    if operation == \"+\":\n",
    "        if string:\n",
    "            return rdd.map(lambda l: insert(l, dest_col_name, ' '.join(l[s].strip() for s in source_col_names if l[s])))\n",
    "        return rdd.map(lambda l: insert(l, dest_col_name, int(float(l[source_col_names[0]].replace(\",\",\".\"))) + int(float(l[source_col_names[1]].replace(\",\",\".\"))) ))\n",
    "    if operation == \"/\":\n",
    "        return rdd.map(lambda l: insert(l, dest_col_name, int(float(l[source_col_names[0]].strip())) / (int(float(l[source_col_names[1]].strip())) + 0.000001)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2f22b836",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_peru(path_to_avro=True, path_to_metadata=True):\n",
    "    \n",
    "    if path_to_avro:\n",
    "        path_to_avro = '/Users/pietro/Desktop/version0.avro'\n",
    "    if path_to_metadata:\n",
    "        path_to_metadata = '/Users/pietro/Desktop/BDM/Project/Data.nosync/peru/metadata_copia/'\n",
    "    df = spark.read.format('avro').load(path_to_avro)\n",
    "    rdd=df.rdd\n",
    "    \n",
    "    \n",
    "    # MERGING DATA AND METADATA\n",
    "    meta_file_name = [\"Partidas.txt\", \"Paises.txt\", \"Paises.txt\", \"Puertos.txt\", \"MedTransporte.txt\", \"Agente.txt\", \"Bancos.txt\", \"RecintAduaner.txt\", \"EstMercancia.txt\"]\n",
    "    key_pos_in_meta =  [0,0,0,[0,2],0,0,0,0,0]\n",
    "    name_pos_in_meta = [1,1,1,  3,  1,1,1,1,1] # position of the columns in meta that contains the name we want to keep \n",
    "    key_name_in_data = [\"PART_NANDI\", \"PAIS_ORIGE\", \"PAIS_ADQUI\", \"PUER_EMBAR\", \"VIA_TRANSP\", \"CODI_AGENT\", \"BANC_CANCE\", \"CODI_ALMA\", \"SEST_MERCA\"]\n",
    "    keep_code = [True, False, False, False, True, False, False, False, False] # true if we want to keep the code of the variable in the final data\n",
    "    new_col_name = [\"custom_description\", \"country_of_origin\", \"country_of_arrival\", \"port_of_boarding\", \"mean_of_transport\", \"agente\", \"bank\", \"warehouse\", \"state\"]\n",
    "    key_as_int = [True, False, False, False, True, True, True, False, False ]\n",
    "    \n",
    "    rdds = []\n",
    "    rdds.append(rdd)\n",
    "    n_merging = len(name_pos_in_meta)\n",
    "    for i in range(0,n_merging):\n",
    "        rdds.append(merging_data_meta(rdds[i], meta_file_name[i], key_pos_in_meta[i], name_pos_in_meta[i], key_name_in_data[i], keep_code[i], new_col_name[i], key_as_int[i], path_to_metadata, \"peru\")) \n",
    "        #rdds.append(rdd)\n",
    "        \n",
    "    # COMBINING COLUMNS\n",
    "    source_col_names = [[\"FOB_DOLPOL\",  \"FLE_DOLAR\"],[\"price_transport_net\",  \"SEG_DOLAR\"], [\"FOB_DOLPOL\", \"UNID_FIQTY\"], [\"DESC_COMER\", \"DESC_MATCO\", \"DESC_USOAP\", \"DESC_FOPRE\", \"DESC_OTROS\"]]\n",
    "    dest_col_name = [    \"price_transport_net\",     \"price_transport_net_insurance\",     \"net_price_per_unit\",          \"commercial_description\"]\n",
    "    string = [False, False, False, True]\n",
    "    operation = [\"+\", \"+\", \"/\", \"+\"]\n",
    "    \n",
    "    print(\"inizio il secondo\")\n",
    "    for i in range(0, len(dest_col_name)):\n",
    "        rdds.append(create_composed_columns(rdds[n_merging + i], source_col_names[i], dest_col_name[i], string[i], operation[i])) \n",
    "    \n",
    "    # CHANGING NAMES\n",
    "    n_combining = len(source_col_names)\n",
    "    \n",
    "    old_names = [\"CODI_ADUAN\", \"FECH_RECEP\", \"FOB_DOLPOL\"]\n",
    "    new_names = [\"custom\", \"date\", \"net_price\"]\n",
    "    rdds.append(rdds[n_combining + n_merging].map(lambda l: changeNames(l, old_names, new_names)))\n",
    "    \n",
    "    \n",
    "    # SELECT THE COLUMNS TO BE KEPT\n",
    "    to_be_kept = ['country_of_arrival', 'mean_of_transport', 'price_transport_net', 'price_transport_net_insurance', 'net_price_per_unit', 'commercial_description', 'custom', 'date', 'net_price']\n",
    "    rdds.append(rdds[n_combining + n_merging + 1].map(lambda l: keepOnly(l, to_be_kept)))\n",
    "\n",
    "    # ADDING COUNTRY NAME and \"IMP\" label\n",
    "    rdds.append(rdds[n_combining + n_merging + 2].map(lambda l: insert(l, \"country\", \"peru\")).map(lambda l: insert(l, \"type\", \"IMP\")))\n",
    "    \n",
    "    print(rdds[n_combining + n_merging + 3].take(1), \"\\n\\n\\n\")\n",
    "    return(rdds[n_combining + n_merging + 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6a6b7b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_brazil(path_to_avro=True, path_to_metadata=True):\n",
    "    \n",
    "    if path_to_avro:\n",
    "        path_to_avro = \"/Users/pietro/Desktop/test_brazil.avro\"\n",
    "    if path_to_metadata:\n",
    "        path_to_metadata = '/Users/pietro/Desktop/BDM/Project/Data.nosync/brazil/metadata copia/'\n",
    "    df = spark.read.format('avro').load(path_to_avro)\n",
    "    rdd=df.rdd\n",
    "    \n",
    "    # NOTE: NCM.csv must contain only the 2 columns of interest, with no \\n in the strings\n",
    "    meta_file_name = [\"NCM.csv\", \"URF.csv\", \"PAIS.csv\", \"VIA.csv\", \"UF.csv\"]\n",
    "    key_pos_in_meta =  [0, 0, 0, 0, 1]\n",
    "    name_pos_in_meta = [1, 1, 4, 1, 2] \n",
    "    key_name_in_data = [\"CO_NCM\", \"CO_URF\", \"CO_PAIS\", \"CO_VIA\", \"SG_UF_NCM\"]\n",
    "    keep_code = [False, False, False, False, False] \n",
    "    new_col_name = [\"custom_description\", \"custom\", \"country_of_arrival\", \"mean_of_transport\", \"arrival_place\"]\n",
    "    key_as_int = [True, True, True, True, False]\n",
    "    \n",
    "    rdds = []\n",
    "    rdds.append(rdd)\n",
    "    n_merging = len(name_pos_in_meta)\n",
    "    for i in range(0,n_merging):\n",
    "        rdds.append(merging_data_meta(rdds[i], meta_file_name[i], key_pos_in_meta[i], name_pos_in_meta[i], key_name_in_data[i], keep_code[i], new_col_name[i], key_as_int[i], path_to_metadata, \"brasil\")) \n",
    "        \n",
    "    # COMBINING COLUMNS\n",
    "    source_col_names = [[\"VL_FOB\", \"VL_FRETE\"],[\"price_transport_net\",  \"VL_SEGURO\"], [\"VL_FOB\", \"KG_LIQUIDO\"], [\"CO_MES\", \"CO_ANO\"]]\n",
    "    dest_col_name = [\"price_transport_net\", \"price_transport_net_insurance\", \"net_price_per_unit\", \"date\"]\n",
    "    string = [False, False, False, True]\n",
    "    operation = [\"+\", \"+\", \"/\", \"+\"]\n",
    "    \n",
    "    for i in range(0, len(dest_col_name)):\n",
    "        rdds.append(create_composed_columns(rdds[n_merging + i], source_col_names[i], dest_col_name[i], string[i], operation[i])) \n",
    "    \n",
    "    # CHANGING NAMES\n",
    "    n_combining = len(source_col_names)\n",
    "    \n",
    "    old_names = [\"VL_FOB\", \"custom_description\"]\n",
    "    new_names = [ \"net_price\", \"commercial_description\"]\n",
    "    rdds.append(rdds[n_combining + n_merging].map(lambda l: changeNames(l, old_names, new_names)))\n",
    "    \n",
    "    # SELECT THE COLUMNS TO BE KEPT\n",
    "    to_be_kept = ['country_of_arrival', 'mean_of_transport', 'price_transport_net', 'price_transport_net_insurance', 'net_price_per_unit', 'commercial_description', 'custom', 'date', 'net_price']\n",
    "    rdds.append(rdds[n_combining + n_merging + 1].map(lambda l: keepOnly(l, to_be_kept)))\n",
    "     \n",
    "    # ADDING COUNTRY NAME and \"IMP\" label\n",
    "    rdds.append(rdds[n_combining + n_merging + 2].map(lambda l: insert(l, \"country\", \"brazil\")).map(lambda l: insert(l, \"type\", \"IMP\")))\n",
    "    return(rdds[n_combining + n_merging + 3])\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "adaea2af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_chile(path_to_avro=True):\n",
    "    \n",
    "    if path_to_avro:\n",
    "        path_to_avro = \"/Users/pietro/Desktop/test_chile.avro\"\n",
    "    df = spark.read.format('avro').load(path_to_avro)\n",
    "    rdd=df.rdd\n",
    "        \n",
    "    ### COLLAPSE\n",
    "    source_col_names = [[\"FOB\", \"FLETE\"],[\"price_transport_net\",  \"SEGURO\"], ['DNOMBRE', 'DMARCA', 'DVARIEDAD', 'DOTRO1', 'DOTRO2', 'ATR_5',  'ATR_6']]\n",
    "    dest_col_name = [\"price_transport_net\", \"price_transport_net_insurance\", \"commercial_description\"]\n",
    "    string = [False, False, True]\n",
    "    operation = [\"+\", \"+\", \"+\"]\n",
    "    rdds = []\n",
    "    rdds.append(rdd)\n",
    "    for i in range(0, len(dest_col_name)):\n",
    "        rdds.append(create_composed_columns(rdds[i], source_col_names[i], dest_col_name[i], string[i], operation[i])) \n",
    "        \n",
    "    # CHANGING NAMES\n",
    "    n_combining = len(source_col_names)\n",
    "    old_names = [\"ADU\", \"FECVENCI\", \"FOB\", \"PRE_UNIT\", \"CODPAISCON\", \"VIA_TRAN\"]\n",
    "    new_names = [ \"custom\", \"date\", \"net_price\", \"net_price_per_unit\", \"country_of_arrival\", \"mean_of_transport\"]\n",
    "    rdds.append(rdds[n_combining].map(lambda l: changeNames(l, old_names, new_names)))\n",
    "    \n",
    "    # SELECT THE COLUMNS TO BE KEPT\n",
    "    to_be_kept = ['country_of_arrival', 'mean_of_transport', 'price_transport_net', 'price_transport_net_insurance', 'net_price_per_unit', 'commercial_description', 'custom', 'date', 'net_price']\n",
    "    rdds.append(rdds[n_combining + 1].map(lambda l: keepOnly(l, to_be_kept)))\n",
    "    #print(rdds[n_combining  + 2].take(1), \"\\n\\n\\n\")\n",
    "\n",
    "    # ADDING COUNTRY NAME and \"IMP\" label\n",
    "    rdds.append(rdds[n_combining + 2].map(lambda l: insert(l, \"country\", \"chile\")).map(lambda l: insert(l, \"type\", \"IMP\")))\n",
    "    print(rdds[n_combining  + 3].take(1), \"\\n\\n\\n\")\n",
    "    return(rdds[n_combining  + 3]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ee33f39d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inizio il secondo\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(country_of_arrival='UNITED STATES', mean_of_transport='AVION', price_transport_net='2294', price_transport_net_insurance='2316', net_price_per_unit='2199.9978000022', commercial_description='BOMBA DE COMBSUTIBLE, S/M, S/M EN UNIDAD/ INCLUYE ACCESORIOS PARA SU NORMAL FUNCIONAMIENTO PARA PROPORCIONAR COMBUSTIBLE AL MOTOR /ES PARA USO DE AERONAVE BOMBA DE COMBSUTIBLE METAL,P/N: 897400-7', custom='235', date='20220209', net_price='2200.0', country='peru', type='IMP')] \n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# LOAD TO THE DB\n",
    "\n",
    "data_chile=main_chile()\n",
    "chileDF = data_chile.toDF()\n",
    "\n",
    "data_brazil=main_brazil()\n",
    "brazilDF = data_brazil.toDF()\n",
    "\n",
    "data_peru=main_peru()\n",
    "peruDF = data_peru.toDF()\n",
    "\n",
    "properties = {\"user\": \"pietro\", \"password\": \"password\", \"driver\": 'org.postgresql.Driver'}\n",
    "url = \"jdbc:postgresql://localhost:5432/dataimporta\"\n",
    "\n",
    "chileDF.write.format(\"jdbc\").mode(\"append\").jdbc(url,\"all_countries\",\n",
    "          properties = properties)\n",
    "brazilDF.write.format(\"jdbc\").mode(\"append\").jdbc(url,\"all_countries\",\n",
    "          properties = properties)\n",
    "peruDF.write.format(\"jdbc\").mode(\"append\").jdbc(url,\"all_countries\",\n",
    "          properties = properties)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "50adb678",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# DATA FOR TBOX GRAPH\n",
    "def tbox_peru(path_to_avro=True, path_to_metadata=True):\n",
    "    \n",
    "    if path_to_avro:\n",
    "        path_to_avro = '/Users/pietro/Desktop/version0.avro'\n",
    "    if path_to_metadata:\n",
    "        path_to_metadata = '/Users/pietro/Desktop/BDM/Project/Data.nosync/peru/metadata_copia/'\n",
    "    df = spark.read.format('avro').load(path_to_avro)\n",
    "    rdd=df.rdd\n",
    "    \n",
    "    \n",
    "    # MERGING DATA AND METADATA\n",
    "    meta_file_name = [\"Partidas.txt\", \"Paises.txt\", \"Paises.txt\", \"MedTransporte.txt\"]\n",
    "    key_pos_in_meta =  [0,0,0,0]\n",
    "    name_pos_in_meta = [1,1,1,1] # position of the columns in meta that contains the name we want to keep \n",
    "    key_name_in_data = [\"PART_NANDI\", \"PAIS_ORIGE\", \"PAIS_ADQUI\", \"VIA_TRANSP\"]\n",
    "    keep_code = [True, False, False, False] # true if we want to keep the code of the variable in the final data\n",
    "    new_col_name = [\"custom_description\", \"country_of_origin\", \"country_of_arrival\", \"mean_of_transport\"]\n",
    "    key_as_int = [True, False, False, True]\n",
    "    \n",
    "    rdds = []\n",
    "    rdds.append(rdd)\n",
    "    n_merging = len(name_pos_in_meta)\n",
    "    \n",
    "    for i in range(0,n_merging):\n",
    "        rdds.append(merging_data_meta(rdds[i], meta_file_name[i], key_pos_in_meta[i], name_pos_in_meta[i], key_name_in_data[i], keep_code[i], new_col_name[i], key_as_int[i], path_to_metadata, \"peru\")) \n",
    "        #rdds.append(rdd)\n",
    "   \n",
    "    # CHANGING NAMES\n",
    "    \n",
    "    old_names = [\"EMPR_TRANS\", \"DNOMBRE\", \"FOB_DOLPOL\"]\n",
    "    new_names = [\"shipper\", \"trader\", \"net_price\"]\n",
    "    rdds.append(rdds[n_merging].map(lambda l: changeNames(l, old_names, new_names)))\n",
    "    \n",
    "    \n",
    "    # SELECT THE COLUMNS TO BE KEPT\n",
    "    to_be_kept = [\"custom_description\", \"country_of_origin\", \"country_of_arrival\", \"mean_of_transport\", \"shipper\", \"trader\", \"net_price\"]\n",
    "    rdds.append(rdds[n_merging + 1].map(lambda l: keepOnly(l, to_be_kept)))\n",
    "    return(rdds[n_merging + 2])\n",
    "df = tbox_peru().toDF()\n",
    "\n",
    "df.toPandas().to_csv('/Users/pietro/Desktop/peru_tbox.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2e3455c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
