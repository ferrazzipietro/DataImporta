{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **DATA PERSISTANCE LOADER**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Avro\n",
    "import avro.schema\n",
    "from avro.datafile import DataFileReader, DataFileWriter\n",
    "from avro.io import DatumReader, DatumWriter\n",
    "\n",
    "import subprocess\n",
    "import pandas as pd\n",
    "from io import StringIO\n",
    "from datetime import datetime\n",
    "import json\n",
    "import re\n",
    "import os\n",
    "import logging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set path of project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'D:/Master/BDMA/Courses/Semester_2/Big_Data_Management/Project/dataimporta/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up log file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger()\n",
    "if (logger.hasHandlers()):\n",
    "    logger.handlers.clear()\n",
    "handler = logging.FileHandler('logfile.log')\n",
    "logger.addHandler(handler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check how many files need to be uploaded into the persistence zone (the count considering all countries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of files to upload: 53\n"
     ]
    }
   ],
   "source": [
    "# HDFS command to find count of directories and files inside a directory\n",
    "count_files = subprocess.run('hadoop fs -count /temporal', capture_output=True, shell=True).stdout.decode()\n",
    "# From the answer, get the count of files (note that this is the count for ALL countries)\n",
    "count_files = re.findall('\\d*\\s*(\\d*)\\s*\\d*\\s/', count_files)\n",
    "count_files = count_files[0]\n",
    "print('# of files to upload: '+count_files)\n",
    "logger.error(datetime.now().strftime(\"%m/%d/%Y, %H:%M:%S\"))\n",
    "logger.error('# of files to upload: '+count_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Countries to upload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Countries to get data from\n",
    "countries = [\"peru\", \"chile\", \"brazil\"]\n",
    "# Countries that have different metadata for imports and exports\n",
    "countries_meta = [\"chile\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check which directories have data in the temporal landing zone and get those file names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary to store the file names\n",
    "filenames = {}\n",
    "for country in countries:\n",
    "    # For countries that have different metadata for imports and exports\n",
    "    if country in countries_meta:\n",
    "        filenames[country] = {\"imports\":[], \"exports\":[], \"metadata\":{\"imports\":[], \"exports\":[]}}\n",
    "    # For countries that have the same metadata for imports and exports\n",
    "    else:\n",
    "        filenames[country] = {\"imports\":[], \"exports\":[], \"metadata\":[]}\n",
    "\n",
    "# Iterate trough each country directory\n",
    "for country in countries:\n",
    "\n",
    "    # Get the content of the directories of the country\n",
    "    # imports\n",
    "    imports = subprocess.run('hadoop fs -ls /temporal/'+country+'/imp', capture_output=True, shell=True).stdout.decode()\n",
    "    # exports\n",
    "    exports= subprocess.run('hadoop fs -ls /temporal/'+country+'/exp', capture_output=True, shell=True).stdout.decode()\n",
    "    # metadata\n",
    "    if country in countries_meta:\n",
    "        metadata_imports = subprocess.run('hadoop fs -ls /temporal/'+country+'/metadata/imp', capture_output=True, shell=True).stdout.decode()\n",
    "        metadata_exports = subprocess.run('hadoop fs -ls /temporal/'+country+'/metadata/exp', capture_output=True, shell=True).stdout.decode()\n",
    "    else:\n",
    "        metadata = subprocess.run('hadoop fs -ls /temporal/'+country+'/metadata', capture_output=True, shell=True).stdout.decode()\n",
    "    \n",
    "    # Get the names of the files (if any) existing in those directories\n",
    "     # imports\n",
    "    imports_files = re.findall('/.*/(.*)\\r', imports)\n",
    "     # exports\n",
    "    exports_files = re.findall('/.*/(.*)\\r', exports)\n",
    "    # metadata\n",
    "    if country in countries_meta:\n",
    "        metadata_import_files =re.findall('/.*/(.*)\\r', metadata_imports)\n",
    "        metadata_export_files =re.findall('/.*/(.*)\\r', metadata_exports)\n",
    "    else:\n",
    "        metadata_files =re.findall('/.*/(.*)\\r', metadata)\n",
    "\n",
    "\n",
    "    # Save those names in the variable \"file\"\n",
    "    #   For imports\n",
    "    for import_file in imports_files:\n",
    "        filenames[country]['imports'].append(import_file)\n",
    "    #   For exports\n",
    "    for export_file in exports_files:\n",
    "        filenames[country]['exports'].append(export_file)\n",
    "    #   For metadata\n",
    "    if country in countries_meta:\n",
    "        # imports\n",
    "        for metadata_import_file in metadata_import_files:\n",
    "            filenames[country]['metadata']['imports'].append(metadata_import_file)\n",
    "        # exports\n",
    "        for metadata_export_file in metadata_export_files:\n",
    "            filenames[country]['metadata']['exports'].append(metadata_export_file)\n",
    "    else:\n",
    "        for metadata_file in metadata_files:\n",
    "            filenames[country]['metadata'].append(metadata_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the files to be uploaded, we need to know to which year do they correspond"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary with the file names and the corresponding years\n",
    "filenames_year = {}\n",
    "for country in countries:\n",
    "    # For countries that have different metadata for imports and exports\n",
    "    if country in countries_meta:\n",
    "        filenames_year[country] = {\"imports\":{}, \"exports\":{}, \"metadata\":{\"imports\":{}, \"exports\":{}}}\n",
    "    # For countries that have the same metadata for imports and exports\n",
    "    else:\n",
    "        filenames_year[country] = {\"imports\":{}, \"exports\":{}, \"metadata\":{}}\n",
    "\n",
    "    # Get years of files for Peru\n",
    "    if country == \"peru\":\n",
    "        # Imports\n",
    "        for filename in filenames[country][\"imports\"]:\n",
    "            filenames_year[country][\"imports\"][filename] = '20'+ re.search('(.{2})(?=.csv)', filename).group(1)\n",
    "        # Exports\n",
    "        for filename in filenames[country][\"exports\"]:\n",
    "            filenames_year[country][\"exports\"][filename] = '20'+ re.search('(.{2})(?=.csv)', filename).group(1)\n",
    "        # Metadata\n",
    "        for filename in filenames[country][\"metadata\"]:\n",
    "            filenames_year[country][\"metadata\"][filename] = datetime.now().date().strftime(\"%Y\")\n",
    "\n",
    "    # Get years of files for Chile\n",
    "    if country == \"chile\":\n",
    "        # Imports\n",
    "        for filename in filenames[country][\"imports\"]:\n",
    "            filenames_year[country][\"imports\"][filename] = re.search('(\\d*)(?=.txt)', filename).group(1)\n",
    "        # Exports\n",
    "        for filename in filenames[country][\"exports\"]:\n",
    "            filenames_year[country][\"exports\"][filename] = re.search('(\\d*)(?=.txt)', filename).group(1)\n",
    "        # Metadata\n",
    "        # imports\n",
    "        for filename in filenames[country][\"metadata\"][\"imports\"]:\n",
    "            filenames_year[country][\"metadata\"][\"imports\"][filename] = datetime.now().date().strftime(\"%Y\")\n",
    "        # exports\n",
    "        for filename in filenames[country][\"metadata\"][\"exports\"]:\n",
    "            filenames_year[country][\"metadata\"][\"exports\"][filename] = datetime.now().date().strftime(\"%Y\")\n",
    "    \n",
    "    # Get years of files for Brazil\n",
    "    if country == \"brazil\":\n",
    "        # Imports\n",
    "        for filename in filenames[country][\"imports\"]:\n",
    "            filenames_year[country][\"imports\"][filename] = re.search('_(\\d{4})_', filename).group(1)\n",
    "        # Exports\n",
    "        for filename in filenames[country][\"exports\"]:\n",
    "            filenames_year[country][\"exports\"][filename] = re.search('_(\\d{4})_', filename).group(1)\n",
    "         # Metadata\n",
    "        for filename in filenames[country][\"metadata\"]:\n",
    "            filenames_year[country][\"metadata\"][filename] = datetime.now().date().strftime(\"%Y\")\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'imports': {'Importaciones Enero 2022.txt': '2022'},\n",
       " 'exports': {},\n",
       " 'metadata': {'imports': {'EXPheaders.csv': '2022',\n",
       "   'descripcion-y-estructura-de-datos-dus.xlsx': '2022'},\n",
       "  'exports': {'descripcion-y-estructura-de-datos-din.xlsx': '2022'}}}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filenames_year[\"chile\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the data years by country:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'peru': ['2022'], 'chile': ['2022'], 'brazil': ['2022']}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "years = {}\n",
    "# Get all the years existing in filenames_year\n",
    "for country in countries:\n",
    "    years[country] = []\n",
    "    # imports\n",
    "    for file, year in filenames_year[country][\"imports\"].items():\n",
    "        years[country].append(year)\n",
    "    # exports\n",
    "    for file, year in filenames_year[country][\"exports\"].items():\n",
    "        years[country].append(year)\n",
    "    # metadata\n",
    "    if country in countries_meta:\n",
    "        # imports\n",
    "        for file, year in filenames_year[country][\"metadata\"][\"imports\"].items():\n",
    "            years[country].append(year)\n",
    "        # exports\n",
    "        for file, year in filenames_year[country][\"metadata\"][\"exports\"].items():\n",
    "            years[country].append(year)\n",
    "    else:\n",
    "        for file, year in filenames_year[country][\"metadata\"].items():\n",
    "            years[country].append(year)\n",
    "\n",
    "# Remove duplicated years from dict\n",
    "for country in years:   \n",
    "    years[country] = list(set(years[country]))\n",
    "years\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if the persistent zone already has the corresponding \"years\" directories to upload the files into, if not, create them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for country in countries:\n",
    "    for year in years[country]:\n",
    "\n",
    "        # imports\n",
    "        years_in_persistent = subprocess.run('hadoop fs -ls /persistent/'+country+'/imp/'+year , capture_output=True, shell=True).stdout.decode()\n",
    "        years_in_persistent = re.findall('/.*/(.*)\\r', years_in_persistent)\n",
    "        # if the year directory does not exist in the persistent folder, create it\n",
    "        if year not in years_in_persistent:\n",
    "            subprocess.run('hadoop fs -mkdir /persistent/'+country+'/imp/'+year , capture_output=True, shell=True)\n",
    "\n",
    "        # exports\n",
    "        years_in_persistent = subprocess.run('hadoop fs -ls /persistent/'+country+'/exp/'+year , capture_output=True, shell=True).stdout.decode()\n",
    "        years_in_persistent = re.findall('/.*/(.*)\\r', years_in_persistent)\n",
    "        # if the year directory does not exist in the persistent folder, create it\n",
    "        if year not in years_in_persistent:\n",
    "            subprocess.run('hadoop fs -mkdir /persistent/'+country+'/exp/'+year , capture_output=True, shell=True)\n",
    "\n",
    "        # metadata\n",
    "        if country in countries_meta:\n",
    "\n",
    "            # imports\n",
    "            years_in_persistent = subprocess.run('hadoop fs -ls /persistent/'+country+'/metadata/imp/'+year , capture_output=True, shell=True).stdout.decode()\n",
    "            years_in_persistent = re.findall('/.*/(.*)\\r', years_in_persistent)\n",
    "            # if the year directory does not exist in the persistent folder, create it\n",
    "            if year not in years_in_persistent:\n",
    "                subprocess.run('hadoop fs -mkdir /persistent/'+country+'/metadata/imp/'+year , capture_output=True, shell=True)\n",
    "\n",
    "            # exports\n",
    "            years_in_persistent = subprocess.run('hadoop fs -ls /persistent/'+country+'/metadata/exp/'+year , capture_output=True, shell=True).stdout.decode()\n",
    "            years_in_persistent = re.findall('/.*/(.*)\\r', years_in_persistent)\n",
    "            # if the year directory does not exist in the persistent folder, create it\n",
    "            if year not in years_in_persistent:\n",
    "                subprocess.run('hadoop fs -mkdir /persistent/'+country+'/metadata/exp/'+year , capture_output=True, shell=True)\n",
    "\n",
    "        else:\n",
    "            \n",
    "            years_in_persistent = subprocess.run('hadoop fs -ls /persistent/'+country+'/metadata/'+year , capture_output=True, shell=True).stdout.decode()\n",
    "            years_in_persistent = re.findall('/.*/(.*)\\r', years_in_persistent)\n",
    "            # if the year directory does not exist in the persistent folder, create it\n",
    "            if year not in years_in_persistent:\n",
    "                subprocess.run('hadoop fs -mkdir /persistent/'+country+'/metadata/'+year , capture_output=True, shell=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's start the upload. Since we will save the files in AVRO format, they must be first converted and only then uploaded to the corresponding directory. Each country will have a different upload pipeline, given the differences of the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Peru loading pipeline*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------\n",
    "# PERU (from temporal to persitent in AVRO)\n",
    "# ------------------------------------------\n",
    "def load_peru(filenames_year, path):\n",
    "    \n",
    "    country = \"peru\"\n",
    "\n",
    "    # initial file name inside a directory of metadata or imports in the persistent zone\n",
    "    initial_name = \"version0.avro\"\n",
    "\n",
    "    # paths to folders\n",
    "    folders = {\"imports\":\"imp\", \"exports\":\"exp\", \"metadata\":\"metadata\"}\n",
    "    \n",
    "    # iterate trough every folder\n",
    "    for folder, folderpath in folders.items():\n",
    "\n",
    "            # ---------------------\n",
    "            # IMPORTS & EXPORTS\n",
    "            # ---------------------\n",
    "            if folder == 'imports' or folder == 'exports':\n",
    "\n",
    "                # Iterate trough every import/export file\n",
    "                for filename, year in filenames_year[country][folder].items():\n",
    "\n",
    "                    # Print status\n",
    "                    print(\"Working on: \"+country+\" | \"+folder+\" | \"+filename+'...')\n",
    "                    logger.error(datetime.now().strftime(\"%m/%d/%Y, %H:%M:%S\"))\n",
    "                    logger.error(\"Working on: \"+country+\" | \"+folder+\" | \"+filename+'...')\n",
    "\n",
    "                    # Check if there is any persistent file to append the temporal file's data into (e.g. the most recent version). If there is no persistent file, create one (version0.avro)\n",
    "                    all_versions = subprocess.run('hadoop fs -ls /persistent/'+country+'/'+folderpath+'/'+year, capture_output=True, shell=True).stdout.decode()\n",
    "                    \n",
    "                    # If no persistent file, create one\n",
    "                    if all_versions == '':\n",
    "                        # convert the temporal file to AVRO\n",
    "                        # - first retrieve the data and convert it into a dataframe\n",
    "                        data = subprocess.run('hadoop fs -cat /temporal/'+country+'/'+folderpath+'/'+filename, capture_output=True, shell=True, encoding=\"latin-1\").stdout\n",
    "                        data = pd.read_csv(StringIO(data))\n",
    "                        # - then create the AVRO schema\n",
    "                        # -- get the column names and create the fields argument for the schema\n",
    "                        fields = []\n",
    "                        for col in list(data.columns):\n",
    "                            fields.append({\"name\":col, \"type\": \"string\"})\n",
    "                        # -- complete the schema with the desired information\n",
    "                        schema = {\n",
    "                            \"doc\": datetime.now().strftime(\"%H%M%S%m%d%Y\"),\n",
    "                            \"name\": \"trade_item\",\n",
    "                            \"namespace\": country+\"_\"+folder,\n",
    "                            \"type\": \"record\",\n",
    "                            \"fields\": fields\n",
    "                        }\n",
    "                        schema = json.dumps(schema)\n",
    "                        schema = avro.schema.parse(schema)\n",
    "                        # - mutate the records to AVRO format\n",
    "                        records = data.to_dict('records')\n",
    "                        for dicts in records:\n",
    "                            # ensure all the data are strings\n",
    "                            for keys in dicts:\n",
    "                                dicts[keys] = str(dicts[keys])\n",
    "                        # - create the AVRO file with the data in the local system\n",
    "                        writer = DataFileWriter(open(path+initial_name, \"wb\"), DatumWriter(), schema)\n",
    "                        # - append each record into the file\n",
    "                        for record in records:\n",
    "                            writer.append(record)\n",
    "                        writer.close()\n",
    "                    \n",
    "                        # after creating the AVRO file, upload it into HDFS\n",
    "                        load_hdfs = subprocess.run(['hadoop', 'fs', '-put', path+initial_name, '/persistent/'+country+'/'+folderpath+'/'+year], capture_output=True, shell=True)\n",
    "                        # print status\n",
    "                        if load_hdfs.returncode == 0 :\n",
    "                            print('First version created: version0.avro file was created in /persistent/'+country+'/'+folderpath+'/'+year+'/')\n",
    "                            logger.error(datetime.now().strftime(\"%m/%d/%Y, %H:%M:%S\"))\n",
    "                            logger.error('First version created: version0.avro file was created in /persistent/'+country+'/'+folderpath+'/'+year+'/')\n",
    "                            \n",
    "                        else: \n",
    "                            print(\"An error occured, the file could not be uploaded\")\n",
    "                            logger.error(datetime.now().strftime(\"%m/%d/%Y, %H:%M:%S\"))\n",
    "                            logger.error(\"An error occured, the file could not be uploaded\")\n",
    "                            logger.error(load_hdfs)\n",
    "                        # delete the file in local\n",
    "                        os.remove(path+initial_name)\n",
    "\n",
    "                    # if there is a persistent file (e.g. a previous version)\n",
    "                    if all_versions != '':\n",
    "                        # get a list with all the versions\n",
    "                        all_versions = re.findall('/.*/(.*)\\r', all_versions)\n",
    "                        # get the most recent version\n",
    "                        most_recent_version = initial_name\n",
    "                        for version in all_versions:\n",
    "                            if re.search('(\\d*)(?=.avro)', version).group(1) > re.search('(\\d*)(?=.avro)', most_recent_version).group(1):\n",
    "                                most_recent_version = version\n",
    "                        # retrieve that version's file from hadoop\n",
    "                        old_avro = subprocess.run(['hadoop', 'fs', '-get', '/persistent/'+country+'/'+folderpath+'/'+year+'/'+most_recent_version, path], capture_output=True, shell=True)\n",
    "                        # get old records (this step can be omited if we only append the new records into the file directly)\n",
    "                        avro_records = []\n",
    "                        reader = DataFileReader(open(path+most_recent_version, \"rb\"), DatumReader())\n",
    "                        for record in reader:\n",
    "                            avro_records.append(record)\n",
    "                        # get old schema\n",
    "                        schema = json.loads(reader.schema)\n",
    "                        reader.close()\n",
    "                        # modify schema\n",
    "                        schema[\"doc\"] = datetime.now().strftime(\"%H%M%S%m%d%Y\")\n",
    "                        schema = json.dumps(schema)\n",
    "                        schema = avro.schema.parse(schema)\n",
    "                        # delete the file in local\n",
    "                        os.remove(path+most_recent_version)\n",
    "                        # convert the temporal file to AVRO\n",
    "                        # - first retrieve the data and convert it into a dataframe\n",
    "                        data = subprocess.run('hadoop fs -cat /temporal/'+country+'/'+folderpath+'/'+filename, capture_output=True, shell=True, encoding=\"latin-1\").stdout\n",
    "                        data = pd.read_csv(StringIO(data))\n",
    "                        # - mutate the records to AVRO format\n",
    "                        new_records = data.to_dict('records')\n",
    "                        for dicts in new_records:\n",
    "                            # ensure all the data are strings\n",
    "                            for keys in dicts:\n",
    "                                dicts[keys] = str(dicts[keys])\n",
    "                        # append the new records into the old ones\n",
    "                        for record in new_records:\n",
    "                            avro_records.append(record)\n",
    "                        # define name for the AVRO file to upload\n",
    "                        final_name = \"version\"+str(int(re.search('(\\d*)(?=.avro)', most_recent_version).group(1))+1)+'.avro'\n",
    "                        # create the AVRO file with the data in the local system\n",
    "                        writer = DataFileWriter(open(path+final_name, \"wb\"), DatumWriter(), schema)\n",
    "                        # append each record into the file\n",
    "                        for record in avro_records:\n",
    "                            writer.append(record)\n",
    "                        writer.close()\n",
    "                        # after creating the AVRO file, upload it into HDFS\n",
    "                        load_hdfs = subprocess.run(['hadoop', 'fs', '-put', path+final_name, '/persistent/'+country+'/'+folderpath+'/'+year], capture_output=True, shell=True)\n",
    "                        # print status\n",
    "                        if load_hdfs.returncode == 0 :\n",
    "                            print('Update performed: '+final_name+' file was created in /persistent/'+country+'/'+folderpath+'/'+year+'/')\n",
    "                            logger.error(datetime.now().strftime(\"%m/%d/%Y, %H:%M:%S\"))\n",
    "                            logger.error('Update performed: '+final_name+' file was created in /persistent/'+country+'/'+folderpath+'/'+year+'/')\n",
    "                            # HERE TE CODE TO DELETE THE FILE IN THE TEMPORAL LANDING ZONE AFTER IT IS PASSED TO THE PERSISTENT\n",
    "                        else: \n",
    "                            print('An error occured, update not performed')\n",
    "                            logger.error(datetime.now().strftime(\"%m/%d/%Y, %H:%M:%S\"))\n",
    "                            logger.error('An error occured, update not performed')\n",
    "                            logger.error(load_hdfs)\n",
    "                        # delete the file in local\n",
    "                        os.remove(path+final_name)\n",
    "            \n",
    "            # ---------------------\n",
    "            # METADATA\n",
    "            # ---------------------\n",
    "            elif folder == \"metadata\":\n",
    "\n",
    "                # If there is medatata to be uploaded, create a directory in the metadata directory in the temporal zone to store the files. The name is %H%M%S%m%d%Y (time)\n",
    "                if filenames_year[country][folder].items():\n",
    "                    metadata_directory = datetime.now().strftime(\"%H%M%S%m%d%Y\")\n",
    "                    # get the year of the any of the files (all have the same year, we will take the first one)\n",
    "                    year = list(filenames_year[country][folder].items())[0][1]\n",
    "                    # create the folder\n",
    "                    create_directory = subprocess.run('hadoop fs -mkdir /persistent/'+country+'/'+folderpath+'/'+year+'/'+metadata_directory, capture_output=True, shell=True, encoding=\"latin-1\")\n",
    "                    if create_directory.returncode == 0 :\n",
    "                        # print success status\n",
    "                        print('A directory named '+metadata_directory+' has been created in /persistent/'+country+'/'+folderpath+'/'+year+'/')\n",
    "                        logger.error(datetime.now().strftime(\"%m/%d/%Y, %H:%M:%S\"))\n",
    "                        logger.error('A directory named '+metadata_directory+' has been created in /persistent/'+country+'/'+folderpath+'/'+year+'/')\n",
    "                        \n",
    "                        # Iterate trough every metadata file\n",
    "                        for filename, year in filenames_year[country][folder].items():\n",
    "                            # print working on\n",
    "                            print(\"Working on: \"+country+\" | \"+folder+\" | \"+filename+'...')\n",
    "                            logger.error(datetime.now().strftime(\"%m/%d/%Y, %H:%M:%S\"))\n",
    "                            logger.error(\"Working on: \"+country+\" | \"+folder+\" | \"+filename+'...')\n",
    "                            # CONVERT FILE TO AVRO\n",
    "                            # get temporal file (for Peru, the files are in txt)\n",
    "                            temporal_file = subprocess.run('hadoop fs -cat /temporal/'+country+'/'+folderpath+'/'+filename, capture_output=True, shell=True, encoding=\"latin-1\").stdout\n",
    "                            # convert it to dataframe (the headers use another separator, and thus, we can´t just directly import them, that's why we skip them)\n",
    "                            data = pd.read_table(StringIO(temporal_file), skiprows = 1, header=None)\n",
    "                            # here we process the headers in a special way\n",
    "                            headers = pd.read_table(StringIO(temporal_file), nrows=1, header=None)\n",
    "                            headers['concatenated'] = headers.apply(lambda x: ' '.join(x.dropna().values.tolist()), axis=1)\n",
    "                            headers = headers['concatenated'].to_frame()\n",
    "                            headers = headers['concatenated'].values[0].split()\n",
    "                            headers_dict = {}\n",
    "                            for i in range(len(headers)):\n",
    "                                headers_dict[i] = headers[i]\n",
    "                            # finally we add the headers to the dataframe\n",
    "                            data.rename(columns=headers_dict, inplace=True)\n",
    "                            # then create the AVRO schema\n",
    "                            # - get the column names and create the fields argument for the schema\n",
    "                            fields = []\n",
    "                            for col in list(data.columns):\n",
    "                                fields.append({\"name\":col, \"type\": \"string\"})\n",
    "                            # - complete the schema with the desired information\n",
    "                            filename_without_extension = filename[:filename.index(\".\")]\n",
    "                            schema = {\n",
    "                                \"doc\": datetime.now().strftime(\"%H%M%S%m%d%Y\"),\n",
    "                                \"name\": filename_without_extension,\n",
    "                                \"namespace\": country+\"_\"+folder,\n",
    "                                \"type\": \"record\",\n",
    "                                \"fields\": fields\n",
    "                            }\n",
    "                            schema = json.dumps(schema)\n",
    "                            schema = avro.schema.parse(schema)\n",
    "                            # mutate the records to AVRO format\n",
    "                            records = data.to_dict('records')\n",
    "                            for dicts in records:\n",
    "                                # ensure all the data are strings\n",
    "                                for keys in dicts:\n",
    "                                    dicts[keys] = str(dicts[keys])\n",
    "                            # create the AVRO file with the data in the local system\n",
    "                            writer = DataFileWriter(open(path+filename_without_extension+'.avro', \"wb\"), DatumWriter(), schema)\n",
    "                            # append each record into the file\n",
    "                            for record in records:\n",
    "                                writer.append(record)\n",
    "                            writer.close()\n",
    "                        \n",
    "                            # after creating the AVRO file, upload it into HDFS\n",
    "                            load_hdfs = subprocess.run(['hadoop', 'fs', '-put', path+filename_without_extension+'.avro', '/persistent/'+country+'/'+folderpath+'/'+year+'/'+metadata_directory], capture_output=True, shell=True)\n",
    "                            # print status\n",
    "                            if load_hdfs.returncode == 0 :\n",
    "                                print('File ' + filename_without_extension+'.avro was created in /persistent/'+country+'/'+folderpath+'/'+year+'/'+metadata_directory)\n",
    "                                logger.error(datetime.now().strftime(\"%m/%d/%Y, %H:%M:%S\"))\n",
    "                                logger.error('File ' + filename_without_extension+'.avro was created in /persistent/'+country+'/'+folderpath+'/'+year+'/'+metadata_directory)\n",
    "                                # HERE THE LINE OF CODE TO DELETE THE FILE IN THE TEMPORAL LANDING ZONE AFTER IT IS PASSED TO THE PERSISTENT\n",
    "                            else: \n",
    "                                print('An error occured, the file could not be created')\n",
    "                                logger.error(datetime.now().strftime(\"%m/%d/%Y, %H:%M:%S\"))\n",
    "                                logger.error('An error occured, the file could not be created')\n",
    "                                logger.error(load_hdfs)\n",
    "                            # delete the file in local\n",
    "                            os.remove(path+filename_without_extension+'.avro')\n",
    "\n",
    "                    else: \n",
    "                        # if there was an error creating the directory\n",
    "                        print('An error occured, the directory could not be created')\n",
    "                        logger.error(datetime.now().strftime(\"%m/%d/%Y, %H:%M:%S\"))\n",
    "                        logger.error('An error occured, the directory could not be created')\n",
    "                        logger.error(load_hdfs)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Chile loading pipeline*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------\n",
    "# CHILE (from temporal to persitent in AVRO)\n",
    "# ------------------------------------------\n",
    "def load_chile(filenames_year, path):\n",
    "\n",
    "    country = \"chile\"\n",
    "    \n",
    "    # initial file name inside a directory of metadata or imports in the persistent zone\n",
    "    initial_name = \"version0.avro\"\n",
    "\n",
    "    # paths to folders\n",
    "    folders = {\"imports\":\"imp\", \"exports\":\"exp\", \"metadata\":\"metadata\"}\n",
    "\n",
    "\n",
    "    # iterate trough every folder\n",
    "    for folder, folderpath in folders.items():\n",
    "\n",
    "        # ---------------------\n",
    "        # IMPORTS & EXPORTS\n",
    "        # ---------------------\n",
    "        if folder == 'imports' or folder == 'exports':\n",
    "\n",
    "            # Iterate trough every import/export file\n",
    "            for filename, year in filenames_year[country][folder].items():\n",
    "\n",
    "                # Print status\n",
    "                print(\"Working on: \"+country+\" | \"+folder+\" | \"+filename+'...')\n",
    "                logger.error(datetime.now().strftime(\"%m/%d/%Y, %H:%M:%S\"))\n",
    "                logger.error(\"Working on: \"+country+\" | \"+folder+\" | \"+filename+'...')\n",
    "\n",
    "                # Check if there is any persistent file to append the temporal file's data into (e.g. the most recent version). If there is no persistent file, create one (version0.avro)\n",
    "                all_versions = subprocess.run('hadoop fs -ls /persistent/'+country+'/'+folderpath+'/'+year, capture_output=True, shell=True).stdout.decode()\n",
    "                \n",
    "                # If no persistent file, create one\n",
    "                if all_versions == '':\n",
    "                    # convert the temporal file to AVRO\n",
    "                    # - first retrieve the data and convert it into a dataframe\n",
    "                    data = subprocess.run(['hadoop','fs','-cat','/temporal/'+country+'/'+folderpath+'/'+filename], capture_output=True, shell=True, encoding=\"latin-1\").stdout\n",
    "                    data = pd.read_csv(StringIO(data), delimiter=\";\", header=None)\n",
    "                    # convert the default assigned headers (0,1,2,3...) to string\n",
    "                    headers_string = {}\n",
    "                    for col in list(data.columns):\n",
    "                        headers_string[col] = str(col)\n",
    "                    data.rename(columns=headers_string, inplace=True)\n",
    "                    # - then create the AVRO schema\n",
    "                    # -- get the column names and create the fields argument for the schema\n",
    "                    fields = []\n",
    "                    for col in list(data.columns):\n",
    "                        fields.append({\"name\":col, \"type\": \"string\"})\n",
    "                    # -- complete the schema with the desired information\n",
    "                    schema = {\n",
    "                        \"doc\": datetime.now().strftime(\"%H%M%S%m%d%Y\"),\n",
    "                        \"name\": \"trade_item\",\n",
    "                        \"namespace\": country+\"_\"+folder,\n",
    "                        \"type\": \"record\",\n",
    "                        \"fields\": fields\n",
    "                    }\n",
    "                    schema = json.dumps(schema)\n",
    "                    schema = avro.schema.parse(schema)\n",
    "                    # - mutate the records to AVRO format\n",
    "                    records = data.to_dict('records')\n",
    "                    for dicts in records:\n",
    "                        # ensure all the data are strings\n",
    "                        for keys in dicts:\n",
    "                            dicts[keys] = str(dicts[keys])\n",
    "                    # - create the AVRO file with the data in the local system\n",
    "                    writer = DataFileWriter(open(path+initial_name, \"wb\"), DatumWriter(), schema)\n",
    "                    # - append each record into the file\n",
    "                    for record in records:\n",
    "                        writer.append(record)\n",
    "                    writer.close()\n",
    "                \n",
    "                    # after creating the AVRO file, upload it into HDFS\n",
    "                    load_hdfs = subprocess.run(['hadoop', 'fs', '-put', path+initial_name, '/persistent/'+country+'/'+folderpath+'/'+year], capture_output=True, shell=True)\n",
    "                    # print status\n",
    "                    if load_hdfs.returncode == 0 :\n",
    "                        print('First version created: version0.avro file was created in /persistent/'+country+'/'+folderpath+'/'+year+'/')\n",
    "                        logger.error(datetime.now().strftime(\"%m/%d/%Y, %H:%M:%S\"))\n",
    "                        logger.error('First version created: version0.avro file was created in /persistent/'+country+'/'+folderpath+'/'+year+'/')\n",
    "                    else: \n",
    "                        print('An error occured, upload not performed')\n",
    "                        logger.error(datetime.now().strftime(\"%m/%d/%Y, %H:%M:%S\"))\n",
    "                        logger.error('An error occured, upload not performed')\n",
    "                        logger.error(load_hdfs)\n",
    "                    # delete the file in local\n",
    "                    os.remove(path+initial_name)\n",
    "\n",
    "                # if there is a persistent file (e.g. a previous version)\n",
    "                if all_versions != '':\n",
    "                    # get a list with all the versions\n",
    "                    all_versions = re.findall('/.*/(.*)\\r', all_versions)\n",
    "                    # get the most recent version\n",
    "                    most_recent_version = initial_name\n",
    "                    for version in all_versions:\n",
    "                        if re.search('(\\d*)(?=.avro)', version).group(1) > re.search('(\\d*)(?=.avro)', most_recent_version).group(1):\n",
    "                            most_recent_version = version\n",
    "                    # retrieve that version's file from hadoop\n",
    "                    old_avro = subprocess.run(['hadoop', 'fs', '-get', '/persistent/'+country+'/'+folderpath+'/'+year+'/'+most_recent_version, path], capture_output=True, shell=True)\n",
    "                    # get old records (this step can be omited if we only append the new records into the file directly)\n",
    "                    avro_records = []\n",
    "                    reader = DataFileReader(open(path+most_recent_version, \"rb\"), DatumReader())\n",
    "                    for record in reader:\n",
    "                        avro_records.append(record)\n",
    "                    # get old schema\n",
    "                    schema = json.loads(reader.schema)\n",
    "                    reader.close()\n",
    "                    # modify schema\n",
    "                    schema[\"doc\"] = datetime.now().strftime(\"%H%M%S%m%d%Y\")\n",
    "                    schema = json.dumps(schema)\n",
    "                    schema = avro.schema.parse(schema)\n",
    "                    # delete the file in local\n",
    "                    os.remove(path+most_recent_version)\n",
    "                    # convert the temporal file to AVRO\n",
    "                    # - first retrieve the data and convert it into a dataframe\n",
    "                    data = subprocess.run(['hadoop','fs','-cat','/temporal/'+country+'/'+folderpath+'/'+filename], capture_output=True, shell=True, encoding=\"latin-1\").stdout\n",
    "                    data = pd.read_csv(StringIO(data), delimiter=\";\", header=None)\n",
    "                    # convert the default assigned headers (0,1,2,3...) to string\n",
    "                    headers_string = {}\n",
    "                    for col in list(data.columns):\n",
    "                        headers_string[col] = str(col)\n",
    "                    data.rename(columns=headers_string, inplace=True)\n",
    "                    # - mutate the records to AVRO format\n",
    "                    new_records = data.to_dict('records')\n",
    "                    for dicts in new_records:\n",
    "                        # ensure all the data are strings\n",
    "                        for keys in dicts:\n",
    "                            dicts[keys] = str(dicts[keys])\n",
    "                    # append the new records into the old ones\n",
    "                    for record in new_records:\n",
    "                        avro_records.append(record)\n",
    "                    # define name for the AVRO file to upload\n",
    "                    final_name = \"version\"+str(int(re.search('(\\d*)(?=.avro)', most_recent_version).group(1))+1)+'.avro'\n",
    "                    # create the AVRO file with the data in the local system\n",
    "                    writer = DataFileWriter(open(path+final_name, \"wb\"), DatumWriter(), schema)\n",
    "                    # append each record into the file\n",
    "                    for record in avro_records:\n",
    "                        writer.append(record)\n",
    "                    writer.close()\n",
    "                    # after creating the AVRO file, upload it into HDFS\n",
    "                    load_hdfs = subprocess.run(['hadoop', 'fs', '-put', path+final_name, '/persistent/'+country+'/'+folderpath+'/'+year], capture_output=True, shell=True)\n",
    "                    # print status\n",
    "                    if load_hdfs.returncode == 0 :\n",
    "                        print('Update performed: '+final_name+' file was created in /persistent/'+country+'/'+folderpath+'/'+year+'/')\n",
    "                        logger.error(datetime.now().strftime(\"%m/%d/%Y, %H:%M:%S\"))\n",
    "                        logger.error('Update performed: '+final_name+' file was created in /persistent/'+country+'/'+folderpath+'/'+year+'/')\n",
    "                        # HERE THE LINE OF CODE TO DELETE THE FILE IN THE TEMPORAL LANDING ZONE AFTER IT IS PASSED TO THE PERSISTENT\n",
    "                    else: \n",
    "                        print('An error occured, update not performed')\n",
    "                        logger.error(datetime.now().strftime(\"%m/%d/%Y, %H:%M:%S\"))\n",
    "                        logger.error('An error occured, update not performed')\n",
    "                        logger.error(load_hdfs)\n",
    "                    # delete the file in local\n",
    "                    os.remove(path+final_name)\n",
    "        \n",
    "        # ---------------------\n",
    "        # METADATA\n",
    "        # ---------------------\n",
    "        if folder == 'metadata':\n",
    "\n",
    "            folders_metadata = {\"imports\":\"imp\", \"exports\":\"exp\"}\n",
    "\n",
    "            # name of the directory to store the metadata\n",
    "            metadata_directory_name = datetime.now().strftime(\"%H%M%S%m%d%Y\")\n",
    "\n",
    "            # iterate trough every metadata folder\n",
    "            for folder_metadata, folder_metadata_path in folders_metadata.items():\n",
    "                \n",
    "                # if there is data in the metadata imports/exports to be uploaded, create the directory in the persistent to store it\n",
    "                if list(filenames_year[country][folder][folder_metadata].items()):\n",
    "                    # get year of first file in metadata imports/exports in temporal\n",
    "                    year = list(filenames_year[country][folder][folder_metadata].items())[0][1]\n",
    "                    metadata_directory = subprocess.run(['hadoop', 'fs', '-mkdir', '/persistent/'+country+'/'+folderpath+'/'+folder_metadata_path+'/'+year+'/'+metadata_directory_name], capture_output=True, shell=True)\n",
    "                    if metadata_directory.returncode == 0 :\n",
    "                        print(\"Directory named \"+metadata_directory_name+' created in '+'/persistent/'+country+'/'+folderpath+'/'+folder_metadata_path+'/'+year+'/')\n",
    "                        logger.error(datetime.now().strftime(\"%m/%d/%Y, %H:%M:%S\"))\n",
    "                        logger.error(\"Directory named \"+metadata_directory_name+' created in '+'/persistent/'+country+'/'+folderpath+'/'+folder_metadata_path+'/'+year+'/')\n",
    "                    else: \n",
    "                        print('An error occured, folder could not be created')\n",
    "                        logger.error(datetime.now().strftime(\"%m/%d/%Y, %H:%M:%S\"))\n",
    "                        logger.error('An error occured, folder could not be created')\n",
    "                        logger.error(load_hdfs)\n",
    "\n",
    "                # Iterate trough every metadata file\n",
    "                for filename, year in filenames_year[country][folder][folder_metadata].items():\n",
    "\n",
    "                    # Print status\n",
    "                    print(\"Working on: \"+country+\" | \"+folder+\" | \"+folder_metadata+\" | \"+filename+'...')\n",
    "                    logger.error(datetime.now().strftime(\"%m/%d/%Y, %H:%M:%S\"))\n",
    "                    logger.error(\"Working on: \"+country+\" | \"+folder+\" | \"+folder_metadata+\" | \"+filename+'...')\n",
    "\n",
    "                    # send data to persistent\n",
    "                    load_hdfs = subprocess.run(['hadoop', 'fs', '-cp', '/temporal/'+country+'/'+folderpath+'/'+folder_metadata_path+'/'+filename, '/persistent/'+country+'/'+folderpath+'/'+folder_metadata_path+'/'+year+'/'+metadata_directory_name], capture_output=True, shell=True)\n",
    "                    if load_hdfs.returncode == 0 :\n",
    "                        print('File '+filename+' uploaded into '+'/persistent/'+country+'/'+folderpath+'/'+folder_metadata_path+'/'+year+'/')\n",
    "                        logger.error(datetime.now().strftime(\"%m/%d/%Y, %H:%M:%S\"))\n",
    "                        logger.error('File '+filename+' uploaded into '+'/persistent/'+country+'/'+folderpath+'/'+folder_metadata_path+'/'+year+'/')\n",
    "                        # HERE THE LINE OF CODE TO DELETE THE FILE IN THE TEMPORAL LANDING ZONE AFTER IT IS PASSED TO THE PERSISTENT\n",
    "\n",
    "                    else: \n",
    "                        print('An error occured, upload not performed')\n",
    "                        logger.error(datetime.now().strftime(\"%m/%d/%Y, %H:%M:%S\"))\n",
    "                        logger.error('An error occured, upload not performed')\n",
    "                        logger.error(load_hdfs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Brazil loading pipeline*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------\n",
    "# BRAZIL (from temporal to persitent in AVRO)\n",
    "# ------------------------------------------\n",
    "def load_brazil(filenames_year, path):\n",
    "    \n",
    "    country = \"brazil\"\n",
    "\n",
    "    # initial file name inside a directory of metadata or imports in the persistent zone\n",
    "    initial_name = \"version0.avro\"\n",
    "\n",
    "    # paths to folders\n",
    "    folders = {\"imports\":\"imp\", \"exports\":\"exp\", \"metadata\":\"metadata\"}\n",
    "    \n",
    "    # iterate trough every folder\n",
    "    for folder, folderpath in folders.items():\n",
    "\n",
    "            # ---------------------\n",
    "            # IMPORTS & EXPORTS\n",
    "            # ---------------------\n",
    "            if folder == 'imports' or folder == 'exports':\n",
    "\n",
    "                # Iterate trough every import/export file\n",
    "                for filename, year in filenames_year[country][folder].items():\n",
    "\n",
    "                    # Print status\n",
    "                    print(\"Working on: \"+country+\" | \"+folder+\" | \"+filename+'...')\n",
    "                    logger.error(datetime.now().strftime(\"%m/%d/%Y, %H:%M:%S\"))\n",
    "                    logger.error(\"Working on: \"+country+\" | \"+folder+\" | \"+filename+'...')\n",
    "\n",
    "                    # Check if there is any persistent file to append the temporal file's data into (e.g. the most recent version). If there is no persistent file, create one (version0.avro)\n",
    "                    all_versions = subprocess.run('hadoop fs -ls /persistent/'+country+'/'+folderpath+'/'+year, capture_output=True, shell=True).stdout.decode()\n",
    "                    \n",
    "                    # If no persistent file, create one\n",
    "                    if all_versions == '':\n",
    "                        # convert the temporal file to AVRO\n",
    "                        # - first retrieve the data and convert it into a dataframe\n",
    "                        data = subprocess.run('hadoop fs -cat /temporal/'+country+'/'+folderpath+'/'+filename, capture_output=True, shell=True, encoding=\"latin-1\").stdout\n",
    "                        data = pd.read_csv(StringIO(data))\n",
    "                        # - then create the AVRO schema\n",
    "                        # -- get the column names and create the fields argument for the schema\n",
    "                        fields = []\n",
    "                        for col in list(data.columns):\n",
    "                            fields.append({\"name\":col, \"type\": \"string\"})\n",
    "                        # -- complete the schema with the desired information\n",
    "                        schema = {\n",
    "                            \"doc\": datetime.now().strftime(\"%H%M%S%m%d%Y\"),\n",
    "                            \"name\": \"trade_item\",\n",
    "                            \"namespace\": country+\"_\"+folder,\n",
    "                            \"type\": \"record\",\n",
    "                            \"fields\": fields\n",
    "                        }\n",
    "                        schema = json.dumps(schema)\n",
    "                        schema = avro.schema.parse(schema)\n",
    "                        # - mutate the records to AVRO format\n",
    "                        records = data.to_dict('records')\n",
    "                        for dicts in records:\n",
    "                            # ensure all the data are strings\n",
    "                            for keys in dicts:\n",
    "                                dicts[keys] = str(dicts[keys])\n",
    "                        # - create the AVRO file with the data in the local system\n",
    "                        writer = DataFileWriter(open(path+initial_name, \"wb\"), DatumWriter(), schema)\n",
    "                        # - append each record into the file\n",
    "                        for record in records:\n",
    "                            writer.append(record)\n",
    "                        writer.close()\n",
    "                    \n",
    "                        # after creating the AVRO file, upload it into HDFS\n",
    "                        load_hdfs = subprocess.run(['hadoop', 'fs', '-put', path+initial_name, '/persistent/'+country+'/'+folderpath+'/'+year], capture_output=True, shell=True)\n",
    "                        # print status\n",
    "                        if load_hdfs.returncode == 0 :\n",
    "                            print('First version created: version0.avro file was created in /persistent/'+country+'/'+folderpath+'/'+year+'/')\n",
    "                            logger.error(datetime.now().strftime(\"%m/%d/%Y, %H:%M:%S\"))\n",
    "                            logger.error('First version created: version0.avro file was created in /persistent/'+country+'/'+folderpath+'/'+year+'/')\n",
    "                        else: \n",
    "                            print('An error occured, upload not performed')\n",
    "                            logger.error(datetime.now().strftime(\"%m/%d/%Y, %H:%M:%S\"))\n",
    "                            logger.error('An error occured, upload not performed')\n",
    "                            logger.error(load_hdfs)\n",
    "\n",
    "                        # delete the file in local\n",
    "                        os.remove(path+initial_name)\n",
    "\n",
    "                    # if there is a persistent file (e.g. a previous version)\n",
    "                    if all_versions != '':\n",
    "                        # get a list with all the versions\n",
    "                        all_versions = re.findall('/.*/(.*)\\r', all_versions)\n",
    "                        # get the most recent version\n",
    "                        most_recent_version = initial_name\n",
    "                        for version in all_versions:\n",
    "                            if re.search('(\\d*)(?=.avro)', version).group(1) > re.search('(\\d*)(?=.avro)', most_recent_version).group(1):\n",
    "                                most_recent_version = version\n",
    "                        # retrieve that version's file from hadoop\n",
    "                        old_avro = subprocess.run(['hadoop', 'fs', '-get', '/persistent/'+country+'/'+folderpath+'/'+year+'/'+most_recent_version, path], capture_output=True, shell=True)\n",
    "                        # get old records (this step can be omited if we only append the new records into the file directly)\n",
    "                        avro_records = []\n",
    "                        reader = DataFileReader(open(path+most_recent_version, \"rb\"), DatumReader())\n",
    "                        for record in reader:\n",
    "                            avro_records.append(record)\n",
    "                        # get old schema\n",
    "                        schema = json.loads(reader.schema)\n",
    "                        reader.close()\n",
    "                        # modify schema\n",
    "                        schema[\"doc\"] = datetime.now().strftime(\"%H%M%S%m%d%Y\")\n",
    "                        schema = json.dumps(schema)\n",
    "                        schema = avro.schema.parse(schema)\n",
    "                        # delete the file in local\n",
    "                        os.remove(path+most_recent_version)\n",
    "                        # convert the temporal file to AVRO\n",
    "                        # - first retrieve the data and convert it into a dataframe\n",
    "                        data = subprocess.run('hadoop fs -cat /temporal/'+country+'/'+folderpath+'/'+filename, capture_output=True, shell=True, encoding=\"latin-1\").stdout\n",
    "                        data = pd.read_csv(StringIO(data))\n",
    "                        # - mutate the records to AVRO format\n",
    "                        new_records = data.to_dict('records')\n",
    "                        for dicts in new_records:\n",
    "                            # ensure all the data are strings\n",
    "                            for keys in dicts:\n",
    "                                dicts[keys] = str(dicts[keys])\n",
    "                        # append the new records into the old ones\n",
    "                        for record in new_records:\n",
    "                            avro_records.append(record)\n",
    "                        # define name for the AVRO file to upload\n",
    "                        final_name = \"version\"+str(int(re.search('(\\d*)(?=.avro)', most_recent_version).group(1))+1)+'.avro'\n",
    "                        # create the AVRO file with the data in the local system\n",
    "                        writer = DataFileWriter(open(path+final_name, \"wb\"), DatumWriter(), schema)\n",
    "                        # append each record into the file\n",
    "                        for record in avro_records:\n",
    "                            writer.append(record)\n",
    "                        writer.close()\n",
    "                        # after creating the AVRO file, upload it into HDFS\n",
    "                        load_hdfs = subprocess.run(['hadoop', 'fs', '-put', path+final_name, '/persistent/'+country+'/'+folderpath+'/'+year], capture_output=True, shell=True)\n",
    "                        # print status\n",
    "                        if load_hdfs.returncode == 0 :\n",
    "                            print('Update performed: '+final_name+' file was created in /persistent/'+country+'/'+folderpath+'/'+year+'/')\n",
    "                            logger.error(datetime.now().strftime(\"%m/%d/%Y, %H:%M:%S\"))\n",
    "                            logger.error('Update performed: '+final_name+' file was created in /persistent/'+country+'/'+folderpath+'/'+year+'/')\n",
    "                            # HERE TE CODE TO DELETE THE FILE IN THE TEMPORAL LANDING ZONE AFTER IT IS PASSED TO THE PERSISTENT\n",
    "                        else: \n",
    "                            print('An error occured, update not performed')\n",
    "                            logger.error(datetime.now().strftime(\"%m/%d/%Y, %H:%M:%S\"))\n",
    "                            logger.error('An error occured, update not performed')\n",
    "                            logger.error(load_hdfs)\n",
    "                        # delete the file in local\n",
    "                        os.remove(path+final_name)\n",
    "            \n",
    "            # ---------------------\n",
    "            # METADATA\n",
    "            # ---------------------\n",
    "            elif folder == \"metadata\":\n",
    "\n",
    "                # If there is medatata to be uploaded, create a directory in the metadata directory in the temporal zone to store the files. The name is %H%M%S%m%d%Y (time)\n",
    "                if filenames_year[country][folder].items():\n",
    "                    metadata_directory = datetime.now().strftime(\"%H%M%S%m%d%Y\")\n",
    "                    # get the year of the any of the files (all have the same year, we will take the first one)\n",
    "                    year = list(filenames_year[country][folder].items())[0][1]\n",
    "                    # create the folder\n",
    "                    create_directory = subprocess.run('hadoop fs -mkdir /persistent/'+country+'/'+folderpath+'/'+year+'/'+metadata_directory, capture_output=True, shell=True, encoding=\"latin-1\")\n",
    "                    if create_directory.returncode == 0 :\n",
    "                        # print success status\n",
    "                        print('A directory named '+metadata_directory+' has been created in /persistent/'+country+'/'+folderpath+'/'+year+'/')\n",
    "                        logger.error(datetime.now().strftime(\"%m/%d/%Y, %H:%M:%S\"))\n",
    "                        logger.error('A directory named '+metadata_directory+' has been created in /persistent/'+country+'/'+folderpath+'/'+year+'/')\n",
    "                        \n",
    "                        # Iterate trough every metadata file\n",
    "                        for filename, year in filenames_year[country][folder].items():\n",
    "                            # print working on\n",
    "                            print(\"Working on: \"+country+\" | \"+folder+\" | \"+filename+'...')\n",
    "                            logger.error(datetime.now().strftime(\"%m/%d/%Y, %H:%M:%S\"))\n",
    "                            logger.error(\"Working on: \"+country+\" | \"+folder+\" | \"+filename+'...') \n",
    "                            # CONVERT FILE TO AVRO\n",
    "                            # get temporal file\n",
    "                            data = subprocess.run(['hadoop','fs','-cat','/temporal/'+country+'/'+folderpath+'/'+filename], capture_output=True, shell=True, encoding=\"latin-1\").stdout\n",
    "                            data = pd.read_csv(StringIO(data), delimiter=\";\")\n",
    "                            # then create the AVRO schema\n",
    "                            # - get the column names and create the fields argument for the schema\n",
    "                            fields = []\n",
    "                            for col in list(data.columns):\n",
    "                                fields.append({\"name\":col, \"type\": \"string\"})\n",
    "                            # - complete the schema with the desired information\n",
    "                            filename_without_extension = filename[:filename.index(\".\")].replace(\" \", \"\")\n",
    "                            schema = {\n",
    "                                \"doc\": datetime.now().strftime(\"%H%M%S%m%d%Y\"),\n",
    "                                \"name\": filename_without_extension,\n",
    "                                \"namespace\": country+\"_\"+folder,\n",
    "                                \"type\": \"record\",\n",
    "                                \"fields\": fields\n",
    "                            }\n",
    "                            schema = json.dumps(schema)\n",
    "                            schema = avro.schema.parse(schema)\n",
    "                            # mutate the records to AVRO format\n",
    "                            records = data.to_dict('records')\n",
    "                            for dicts in records:\n",
    "                                # ensure all the data are strings\n",
    "                                for keys in dicts:\n",
    "                                    dicts[keys] = str(dicts[keys])\n",
    "                            # create the AVRO file with the data in the local system\n",
    "                            writer = DataFileWriter(open(path+filename_without_extension+'.avro', \"wb\"), DatumWriter(), schema)\n",
    "                            # append each record into the file\n",
    "                            for record in records:\n",
    "                                writer.append(record)\n",
    "                            writer.close()\n",
    "                        \n",
    "                            # after creating the AVRO file, upload it into HDFS\n",
    "                            load_hdfs = subprocess.run(['hadoop', 'fs', '-put', path+filename_without_extension+'.avro', '/persistent/'+country+'/'+folderpath+'/'+year+'/'+metadata_directory], capture_output=True, shell=True)\n",
    "                            # print status\n",
    "                            if load_hdfs.returncode == 0 :\n",
    "                                print('File ' + filename_without_extension+'.avro was created in /persistent/'+country+'/'+folderpath+'/'+year+'/'+metadata_directory)\n",
    "                                logger.error(datetime.now().strftime(\"%m/%d/%Y, %H:%M:%S\"))\n",
    "                                logger.error('File ' + filename_without_extension+'.avro was created in /persistent/'+country+'/'+folderpath+'/'+year+'/'+metadata_directory) \n",
    "                                # HERE THE LINE OF CODE TO DELETE THE FILE IN THE TEMPORAL LANDING ZONE AFTER IT IS PASSED TO THE PERSISTENT\n",
    "                            else: \n",
    "                                print('An error occured, the file could not be created')\n",
    "                                logger.error(datetime.now().strftime(\"%m/%d/%Y, %H:%M:%S\"))\n",
    "                                logger.error('An error occured, the file could not be created') \n",
    "                                logger.error(load_hdfs)\n",
    "                            # delete the file in local\n",
    "                            os.remove(path+filename_without_extension+'.avro')\n",
    "\n",
    "                    else: \n",
    "                        # if there was an error creating the directory\n",
    "                        print('An error occured, the directory could not be created')\n",
    "                        logger.error(datetime.now().strftime(\"%m/%d/%Y, %H:%M:%S\"))\n",
    "                        logger.error('An error occured, the directory could not be created') \n",
    "                        logger.error(load_hdfs)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on: peru | imports | x14200222.csv...\n",
      "First version created: version0.avro file was created in /persistent/peru/imp/2022/\n",
      "Working on: peru | imports | x17230122.csv...\n",
      "Update performed: version1.avro file was created in /persistent/peru/imp/2022/\n",
      "Working on: peru | imports | x21270222.csv...\n",
      "Update performed: version2.avro file was created in /persistent/peru/imp/2022/\n",
      "Working on: peru | imports | x24300122.csv...\n",
      "Update performed: version3.avro file was created in /persistent/peru/imp/2022/\n",
      "Working on: peru | imports | x28060322.csv...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\spost\\AppData\\Local\\Temp\\ipykernel_12976\\828821007.py:111: DtypeWarning: Columns (18,24,25) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  data = pd.read_csv(StringIO(data))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Update performed: version4.avro file was created in /persistent/peru/imp/2022/\n",
      "Working on: peru | imports | x31060222.csv...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\spost\\AppData\\Local\\Temp\\ipykernel_12976\\828821007.py:111: DtypeWarning: Columns (18,24,25) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  data = pd.read_csv(StringIO(data))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Update performed: version5.avro file was created in /persistent/peru/imp/2022/\n",
      "A directory named 19040704042022 has been created in /persistent/peru/metadata/2022/\n",
      "Working on: peru | metadata | Aduanas.txt...\n",
      "File Aduanas.avro was created in /persistent/peru/metadata/2022/19040704042022\n",
      "Working on: peru | metadata | Aerolineas.txt...\n",
      "File Aerolineas.avro was created in /persistent/peru/metadata/2022/19040704042022\n",
      "Working on: peru | metadata | AgenCar.txt...\n",
      "File AgenCar.avro was created in /persistent/peru/metadata/2022/19040704042022\n",
      "Working on: peru | metadata | AgenEmb.txt...\n",
      "File AgenEmb.avro was created in /persistent/peru/metadata/2022/19040704042022\n",
      "Working on: peru | metadata | AgenMar.txt...\n",
      "File AgenMar.avro was created in /persistent/peru/metadata/2022/19040704042022\n",
      "Working on: peru | metadata | AgenNav.txt...\n",
      "File AgenNav.avro was created in /persistent/peru/metadata/2022/19040704042022\n",
      "Working on: peru | metadata | Agente.txt...\n",
      "File Agente.avro was created in /persistent/peru/metadata/2022/19040704042022\n",
      "Working on: peru | metadata | Bancos.txt...\n",
      "File Bancos.avro was created in /persistent/peru/metadata/2022/19040704042022\n",
      "Working on: peru | metadata | CodProvS.txt...\n",
      "File CodProvS.avro was created in /persistent/peru/metadata/2022/19040704042022\n",
      "Working on: peru | metadata | Documento.txt...\n",
      "File Documento.avro was created in /persistent/peru/metadata/2022/19040704042022\n",
      "Working on: peru | metadata | EmpInternac.txt...\n",
      "File EmpInternac.avro was created in /persistent/peru/metadata/2022/19040704042022\n",
      "Working on: peru | metadata | EmpNacional.txt...\n",
      "File EmpNacional.avro was created in /persistent/peru/metadata/2022/19040704042022\n",
      "Working on: peru | metadata | EntAutMerRest.txt...\n",
      "File EntAutMerRest.avro was created in /persistent/peru/metadata/2022/19040704042022\n",
      "Working on: peru | metadata | EstMercancia.txt...\n",
      "File EstMercancia.avro was created in /persistent/peru/metadata/2022/19040704042022\n",
      "Working on: peru | metadata | LinAereasManif.txt...\n",
      "File LinAereasManif.avro was created in /persistent/peru/metadata/2022/19040704042022\n",
      "Working on: peru | metadata | MedTransporte.txt...\n",
      "File MedTransporte.avro was created in /persistent/peru/metadata/2022/19040704042022\n",
      "Working on: peru | metadata | MerExpTemp.txt...\n",
      "File MerExpTemp.avro was created in /persistent/peru/metadata/2022/19040704042022\n",
      "Working on: peru | metadata | Monedas.txt...\n",
      "File Monedas.avro was created in /persistent/peru/metadata/2022/19040704042022\n",
      "Working on: peru | metadata | MotValorAjust.txt...\n",
      "File MotValorAjust.avro was created in /persistent/peru/metadata/2022/19040704042022\n",
      "Working on: peru | metadata | Paises.txt...\n",
      "File Paises.avro was created in /persistent/peru/metadata/2022/19040704042022\n",
      "Working on: peru | metadata | Puertos.txt...\n",
      "File Puertos.avro was created in /persistent/peru/metadata/2022/19040704042022\n",
      "Working on: peru | metadata | RecintAduaner.txt...\n",
      "File RecintAduaner.avro was created in /persistent/peru/metadata/2022/19040704042022\n",
      "Working on: peru | metadata | RegOpeAduaneras.txt...\n",
      "File RegOpeAduaneras.avro was created in /persistent/peru/metadata/2022/19040704042022\n",
      "Working on: peru | metadata | Sanciones.txt...\n",
      "File Sanciones.avro was created in /persistent/peru/metadata/2022/19040704042022\n",
      "Working on: peru | metadata | Tablibe.txt...\n",
      "File Tablibe.avro was created in /persistent/peru/metadata/2022/19040704042022\n",
      "Working on: peru | metadata | TipDocumAutMerRest.txt...\n",
      "File TipDocumAutMerRest.avro was created in /persistent/peru/metadata/2022/19040704042022\n",
      "Working on: peru | metadata | TipTratamiento.txt...\n",
      "File TipTratamiento.avro was created in /persistent/peru/metadata/2022/19040704042022\n",
      "Working on: peru | metadata | TipUltract.txt...\n",
      "File TipUltract.avro was created in /persistent/peru/metadata/2022/19040704042022\n",
      "Working on: peru | metadata | Tributos.txt...\n",
      "File Tributos.avro was created in /persistent/peru/metadata/2022/19040704042022\n",
      "Working on: peru | metadata | UnidComerExp.txt...\n",
      "File UnidComerExp.avro was created in /persistent/peru/metadata/2022/19040704042022\n",
      "Working on: peru | metadata | UnidMedida.txt...\n",
      "File UnidMedida.avro was created in /persistent/peru/metadata/2022/19040704042022\n",
      "Working on: peru | metadata | UnidMercancia.txt...\n",
      "File UnidMercancia.avro was created in /persistent/peru/metadata/2022/19040704042022\n",
      "Working on: peru | metadata | codprov.txt...\n",
      "File codprov.avro was created in /persistent/peru/metadata/2022/19040704042022\n"
     ]
    }
   ],
   "source": [
    "load_peru(filenames_year, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on: chile | imports | Importaciones Enero 2022.txt...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\spost\\AppData\\Local\\Temp\\ipykernel_12976\\242883574.py:39: DtypeWarning: Columns (44,45,46,52,98,100,107,108,139,149,151,153,155,159,162,166,168,169,170,172,173) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  data = pd.read_csv(StringIO(data), delimiter=\";\", header=None)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First version created: version0.avro file was created in /persistent/chile/imp/2022/\n",
      "Directory named 19122304042022 created in /persistent/chile/metadata/imp/2022/\n",
      "Working on: chile | metadata | imports | EXPheaders.csv...\n",
      "File EXPheaders.csv uploaded into /persistent/chile/metadata/imp/2022/\n",
      "Working on: chile | metadata | imports | descripcion-y-estructura-de-datos-dus.xlsx...\n",
      "File descripcion-y-estructura-de-datos-dus.xlsx uploaded into /persistent/chile/metadata/imp/2022/\n",
      "Directory named 19122304042022 created in /persistent/chile/metadata/exp/2022/\n",
      "Working on: chile | metadata | exports | descripcion-y-estructura-de-datos-din.xlsx...\n",
      "File descripcion-y-estructura-de-datos-din.xlsx uploaded into /persistent/chile/metadata/exp/2022/\n"
     ]
    }
   ],
   "source": [
    "load_chile(filenames_year, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on: brazil | exports | EXP_2022_NCM.csv...\n",
      "First version created: version0.avro file was created in /persistent/brazil/exp/2022/\n",
      "A directory named 19145404042022 has been created in /persistent/brazil/metadata/2022/\n",
      "Working on: brazil | metadata | CUCI.csv...\n",
      "File CUCI.avro was created in /persistent/brazil/metadata/2022/19145404042022\n",
      "Working on: brazil | metadata | ISIC .csv...\n",
      "File ISIC.avro was created in /persistent/brazil/metadata/2022/19145404042022\n",
      "Working on: brazil | metadata | MUN.csv...\n",
      "File MUN.avro was created in /persistent/brazil/metadata/2022/19145404042022\n",
      "Working on: brazil | metadata | NCM.csv...\n",
      "File NCM.avro was created in /persistent/brazil/metadata/2022/19145404042022\n",
      "Working on: brazil | metadata | NCM_FAT_AGREG.csv...\n",
      "File NCM_FAT_AGREG.avro was created in /persistent/brazil/metadata/2022/19145404042022\n",
      "Working on: brazil | metadata | NCM_PPE.csv...\n",
      "File NCM_PPE.avro was created in /persistent/brazil/metadata/2022/19145404042022\n",
      "Working on: brazil | metadata | PAIS.csv...\n",
      "File PAIS.avro was created in /persistent/brazil/metadata/2022/19145404042022\n",
      "Working on: brazil | metadata | SH.csv...\n",
      "File SH.avro was created in /persistent/brazil/metadata/2022/19145404042022\n",
      "Working on: brazil | metadata | UF.csv...\n",
      "File UF.avro was created in /persistent/brazil/metadata/2022/19145404042022\n"
     ]
    }
   ],
   "source": [
    "load_brazil(filenames_year, path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Call loading functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for country in countries:\n",
    "    # PERU\n",
    "    if country == 'peru':\n",
    "        load_peru(filenames_year, path)\n",
    "    # CHILE\n",
    "    if country == 'chile':\n",
    "        load_chile(filenames_year, path)\n",
    "    # BRAZIL\n",
    "    if country == 'brazil':\n",
    "        load_brazil(filenames_year, path)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "fd040cd6d2f6c03fa7abba4128c8b33c22dfb22c58d45ceefc9f5658edabb3e8"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('bdm_project')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
